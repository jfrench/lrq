[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "A Progressive Introduction to Linear Models",
    "section": "",
    "text": "Preliminaries",
    "crumbs": [
      "Preliminaries"
    ]
  },
  {
    "objectID": "index.html#background",
    "href": "index.html#background",
    "title": "A Progressive Introduction to Linear Models",
    "section": "Background",
    "text": "Background\nI designed this book to progressively introduce you to the analysis of data using linear models. My goal is to provide you with the skills needed to perform a linear regression analysis sooner rather than later. Most of the detailed derivations have been placed in Going Deeper sections or in their own chapter, which can be skipped over to more quickly progress through the material at the expense of less exposure to theory.",
    "crumbs": [
      "Preliminaries"
    ]
  },
  {
    "objectID": "index.html#acknowledgments",
    "href": "index.html#acknowledgments",
    "title": "A Progressive Introduction to Linear Models",
    "section": "Acknowledgments",
    "text": "Acknowledgments\nThe writing of the book was partially supported by the Colorado Department of Higher Education as part of the proposal “OER for the Creation of Interactive Computational Notebooks and a Computational Pathway in Mathematics and Statistics”.\nThe computational examples utilize the R programming language and environment (R Core Team 2024).\nWe will also make use of the following packages:\n\napi2lm (French 2023).\ndplyr (Wickham et al. 2023).\nggplot2 (Wickham et al. 2024)\nknitr (Xie 2024)\npalmerpenguins (Horst, Hill, and Gorman 2022)\nplotly (Sievert et al. 2024)\ntidyverse (Wickham 2023).",
    "crumbs": [
      "Preliminaries"
    ]
  },
  {
    "objectID": "index.html#creative-commons-license-information",
    "href": "index.html#creative-commons-license-information",
    "title": "A Progressive Introduction to Linear Models",
    "section": "Creative Commons License Information",
    "text": "Creative Commons License Information\n A Progressive Introduction to Linear Models by Joshua French is licensed under a Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License.\n\n\n\n\nFrench, Joshua P. 2023. Api2lm: Functions and Data Sets for the Book \"a Progressive Introduction to Linear Models\". https://CRAN.R-project.org/package=api2lm.\n\n\nHorst, Allison, Alison Hill, and Kristen Gorman. 2022. Palmerpenguins: Palmer Archipelago (Antarctica) Penguin Data. https://allisonhorst.github.io/palmerpenguins/.\n\n\nR Core Team. 2024. R: A Language and Environment for Statistical Computing. Vienna, Austria: R Foundation for Statistical Computing. https://www.R-project.org/.\n\n\nSievert, Carson, Chris Parmer, Toby Hocking, Scott Chamberlain, Karthik Ram, Marianne Corvellec, and Pedro Despouy. 2024. Plotly: Create Interactive Web Graphics via Plotly.js. https://plotly-r.com.\n\n\nWickham, Hadley. 2023. Tidyverse: Easily Install and Load the Tidyverse. https://tidyverse.tidyverse.org.\n\n\nWickham, Hadley, Winston Chang, Lionel Henry, Thomas Lin Pedersen, Kohske Takahashi, Claus Wilke, Kara Woo, Hiroaki Yutani, Dewey Dunnington, and Teun van den Brand. 2024. Ggplot2: Create Elegant Data Visualisations Using the Grammar of Graphics. https://ggplot2.tidyverse.org.\n\n\nWickham, Hadley, Romain François, Lionel Henry, Kirill Müller, and Davis Vaughan. 2023. Dplyr: A Grammar of Data Manipulation. https://dplyr.tidyverse.org.\n\n\nXie, Yihui. 2024. Knitr: A General-Purpose Package for Dynamic Report Generation in r. https://yihui.org/knitr/.",
    "crumbs": [
      "Preliminaries"
    ]
  },
  {
    "objectID": "r-foundations.html",
    "href": "r-foundations.html",
    "title": "1  R Foundations",
    "section": "",
    "text": "1.1 Setting up R and RStudio Desktop",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>R Foundations</span>"
    ]
  },
  {
    "objectID": "r-foundations.html#setting-up-r-and-rstudio-desktop",
    "href": "r-foundations.html#setting-up-r-and-rstudio-desktop",
    "title": "1  R Foundations",
    "section": "",
    "text": "1.1.1 What is R?\nR is a programming language and environment designed for statistical computing. It was introduced by Robert Gentleman and Robert Ihaka in 1993 as a free implementation of the S programming language developed at Bell Laboratories (https://www.r-project.org/about.html)\nSome important facts about R are that:\n\nR is free, open source, and runs on many different types of computers (Windows, Mac, Linux, and others).\nR is an interactive programming language.\n\nWe can type and run a command in the Console for immediate feedback, in contrast to a compiled programming language, which compiles a program that is then executed.\n\nR is highly extendable.\n\nWe can extend the functionality of R beyond what is available by default by installing user-created packages.\nWe can write our own functions to add additional capabilities to R.\n\n\n\n\n1.1.2 Installing R\nTo install R on our personal computer, we will need to download an installer program from the R Project’s website (https://www.r-project.org/). Links to download the installer program for a specific operating system are found at https://cloud.r-project.org/.\nWe should click on the download link appropriate for our computer’s operating system and then go through the process of installing R. A stable link for the most current installer program for the Windows operating system is available at https://cloud.r-project.org/bin/windows/base/release.html. (Similar links are not currently available for Mac and Linux computers.)\n\n\n1.1.3 Installing RStudio\nRStudio Desktop is a free “front end” for R provided by Posit Software (https://posit.co/). RStudio Desktop makes doing data analysis with R much easier by adding an Integrated Development Environment (IDE) and providing many other features. Currently, we can download RStudio at https://posit.co/download/rstudio-desktop/. We should download the Free version of RStudio Desktop appropriate for our computer and install it.\nHaving installed both R and RStudio Desktop, we will want to open RStudio Desktop as we continue to learn about R.\n\n\n1.1.4 RStudio Layout\nRStudio Desktop has four panes:\n\nConsole: the pane where commands are run.\nSource: the pane where we prepare commands to run.\nEnvironment/History: the pane where we can see all the objects in our workspace, our command history, and other information.\nFiles/Plot/Packages/Help: the pane where we navigate between directories, view plots, see the packages available to be loaded, or get help.\n\nTo see all RStudio panes, press the keys Ctrl + Alt + Shift + 0 on a PC or Cmd + Option + Shift + 0 on a Mac.\nFigure 1.1 displays a labeled graphic of the panes. The position of the panes can change depending on how we set the display preferences, but the look of each pane will be similar.\n\n\n\n\n\n\n\n\nFigure 1.1: The RStudio panes labeled for convenience.\n\n\n\n\n\n\n\n1.1.5 Customizing the RStudio workspace\nAt this point, we should customize our workspace to prevent us from experiencing future frustration. R provides a “feature” that allows us to “save a workspace”. This allows us to easily pick up our analysis where we last left off. Unfortunately, over time, we will accumulate many environmental artifacts that can conflict with each other. This can lead to errors and incorrect results that we will need to deal with. Additionally, this “feature” hinders the ability of others to reproduce our analysis because other users will not have the same workspace.\nTo turn off this feature, in the RStudio menu bar, we click Tools → Global Options and then make sure the “General” option is selected. Then we make the following changes (if necessary):\n\nUncheck the box for “Restore .RData into workspace at startup”.\nChange the toggle box for “Save workspace to .RData on exit” to “Never”.\nClick “Apply” then “OK” to save the changes.\n\nFigure 1.2 displays what these options should look like.\n\n\n\n\n\n\n\n\nFigure 1.2: The General options window.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>R Foundations</span>"
    ]
  },
  {
    "objectID": "r-foundations.html#running-code-scripts-and-comments",
    "href": "r-foundations.html#running-code-scripts-and-comments",
    "title": "1  R Foundations",
    "section": "1.2 Running code, scripts, and comments",
    "text": "1.2 Running code, scripts, and comments\nWe can run code in R by typing it in the Console next to the &gt; symbol and pressing the Enter key.\nIf we need to successively run multiple commands, it’s better to write our commands in a “script” file and then save the file. The commands in a Script file are often generically referred to as “code”.\nScript files make it easy to:\n\nReproduce our data analysis without retyping all our commands.\nShare our code with others.\n\nA new Script file can be obtained by:\n\nClicking File → New File → R Script in the RStudio menu bar.\nPressing Ctrl + Shift + n on a PC or Cmd + Shift + n on a Mac.\n\nThere are various ways to run code from a Script file. The most common ones are:\n\nHighlight the code we want to run and click the “Run” button at the top of the Script pane. Figure 1.3 displays the Run button.\nHighlight the code we want to run and press Ctrl + Enter on our keyboard. If we don’t highlight anything, by default, RStudio runs the command the cursor currently lies on.\n\n\n\n\n\n\n\n\n\nFigure 1.3: The Run icon can be clicked to run a selection of commands.\n\n\n\n\n\nTo save a Script file:\n\nClick File → Save in the RStudio menu bar.\nPress Ctrl + s on a PC or Cmd + s on a Mac.\n\nA comment is a set of text ignored by R when submitted to the Console. The # symbol indicates the start of a comment. Nothing to the right of the # is executed by the Console. We can comment (or uncomment) multiple lines of code in the Source pane of RStudio by highlighting the code we want to comment and pressing Ctrl + Shift + c on a PC or Cmd + Shift + c on a Mac.\n\n\nHands-on Practice\nPerform the following tasks:\n\nType 1+1 in the Console and press the Enter key.\nOpen a new Script in RStudio.\nType mean(1:3) in the Script file.\nType # mean(1:3) in the Script file.\nRun the commands from the Script using an approach mentioned above.\nSave the Script file.\nUse the keyboard shortcut to “comment out” some of the lines of the Script file.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>R Foundations</span>"
    ]
  },
  {
    "objectID": "r-foundations.html#sec-assignment",
    "href": "r-foundations.html#sec-assignment",
    "title": "1  R Foundations",
    "section": "1.3 Assignment",
    "text": "1.3 Assignment\nR works on various types of objects that we’ll learn more about later.\nTo store an object in the computer’s memory, we must assign it a name using the assignment operator (&lt;-) or the equal sign (=).\nSome comments:\n\nIn general, both &lt;- and = can be used for assignment.\nPressing Alt + - on a PC or Option + - on a Mac will insert &lt;- into the R Console and Script files.\n\nIf we are creating an R Markdown (rmd) or Quarto Markdown (qmd) file, then this shortcut will only insert &lt;- if we are in an R code block.\n\n&lt;- and = are NOT synonyms, but can be used identically most of the time.\n\nIt is best to use &lt;- for assigning a name to an object and reserving = for specifying function arguments. See Section Section 1.14.1 for a deeper explanation.\nOnce an object has been assigned a name, it can be printed by running the name of the object in the Console or using the print function.\n\n\nHands-on Practice\nRun the following commands in the Console:\n\nm &lt;- mean(1:10)\n\nIn the code above, we compute the sample mean of the values \\(1, 2, \\ldots, 10\\), then assign it the name m. However, nothing is printed by the R Console.\n\nm\n\nWhen we type the name of an object in the R Console and run the command, R will print at least some of the information in the object. In this case, the value stored in m, the value 5.5, will be returned.\n\nprint(m)\n\nThe command above is a formal way of printing an object in R and can be especially useful when an object has a defined print method with additional arguments. We will discuss this later when it is relevant.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>R Foundations</span>"
    ]
  },
  {
    "objectID": "r-foundations.html#functions",
    "href": "r-foundations.html#functions",
    "title": "1  R Foundations",
    "section": "1.4 Functions",
    "text": "1.4 Functions\nA function is an object that performs a certain action or set of actions based on the objects it receives from its arguments. We use a sequence of function calls to perform data analysis.\nWe use a function by typing the function’s name in the Console (or Script), supplying the function’s arguments between parentheses, (), and then pressing the Enter key.\nThe arguments of a function are pieces of data or information that the function needs to perform the requested task (i.e., the function “inputs”). Each argument we supply is separated by a comma, ,. Some functions have default values for certain arguments and do not need to specified unless something besides the default behavior is desired.\nThe mean function computes the sample mean of an R object x. (We can look at the documentation for the function by running ?mean in the Console. We’ll talk more about getting help with R shortly.) The mean function also has a trim argument that indicates the, “… fraction … of observations to be trimmed from each end of x before the mean is computed” (R Core Team 2024, ?mean).\nConsider the command below, in which we compute the mean of the set of values 1, 5, 3, 2, 10.\n\nmean(c(1, 5, 3, 4, 10))\n\n[1] 4.6\n\n\nIn the next command, we compute a trimmed mean of the numeric vector. Since the trim argument is 0.2, we “trim” the smallest 20% of values and the largest 20% of values from the numeric vector prior to computing the mean.\n\nmean(c(1, 5, 3, 4, 10), trim = 0.2)\n\n[1] 4\n\n\nIn the first function call, we compute (1 + 5 + 3 + 4 + 10)/5 = 23/5 = 4.6. In the second function call, we remove the smallest 20% and largest 20% of the values (i.e., dropping 1 and 10) and compute (5 + 3 + 4)/3 = 12/3 = 4.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>R Foundations</span>"
    ]
  },
  {
    "objectID": "r-foundations.html#packages",
    "href": "r-foundations.html#packages",
    "title": "1  R Foundations",
    "section": "1.5 Packages",
    "text": "1.5 Packages\nPackages are collections of functions, data, and other objects that extend the functionality available in R by default.\nR packages can be installed using the install.packages function and loaded using the library function.\n\n\nHands-on Practice\nThe tidyverse (Wickham 2023c) is a popular ecosystem of R packages used for manipulating, tidying, and plotting data. Currently, the tidyverse is comprised of the following packages:\n\nggplot2: A package for plotting based on the “Grammar of Graphics” (Wickham et al. 2024).\npurrr: A package for functional programming (Wickham and Henry 2023).\ntibble: A package providing a more advanced data frame (Müller and Wickham 2023).\ndplyr: A package for manipulating data (Wickham et al. 2023). More specifically, it provides ” a grammar of data manipulation”.\ntidyr: A package to help create “tidy” data (Wickham, Vaughan, and Girlich 2024). Tidy data is a data organization style often convenient for data analysis.\nstringr: A package for working with character/string data (Wickham 2023b).\nreadr: A package for importing data (Wickham, Hester, and Bryan 2024).\nforcats: A package for working with categorical data (Wickham 2023a).\n\nInstall the set of tidyverse R packages by running the command below in the Console.\n\ninstall.packages(\"tidyverse\")\n\nAfter we install the tidyverse, we load the collection of packages by running the command below.\n\nlibrary(tidyverse)\n\nWe should see something like the output below.\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n\nWhen we loaded the tidyverse package above, notice that dplyr::lag() masks stats::lag(). “Masking” means that functions or objects from different packages have the same name. In this case, both the dplyr and stats packages have a function called lag. Different packages may use the same function name to provide certain functionality. The functions will likely be used for different tasks or require different arguments.\nTo refer to a function in a specific package, we should add the syntax package:: prior to the function name. In the code below, we run stats::lag and dplyr::lag on two different objects using the :: syntax.\n\n# run stats::lag on a numeric vector\nstats::lag(1:10, 2)\n\n [1]  1  2  3  4  5  6  7  8  9 10\nattr(,\"tsp\")\n[1] -1  8  1\n\n\n\n# run dplyr::lag on a numeric vector\ndplyr::lag(1:10, 2)\n\n [1] NA NA  1  2  3  4  5  6  7  8\n\n\nThe output returned by the two functions is different because the functions are intended to do different things. The stats::lag function call shifts the time base of the provided time series object back 2 units, while the call to dplyr::lag provides the values 2 positions earlier in the object.\nNote: this example is not intended to help us understand the lag function. This example highlights how to use the :: syntax to call a function in a specific package when the function name is used in multiple packages.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>R Foundations</span>"
    ]
  },
  {
    "objectID": "r-foundations.html#getting-help",
    "href": "r-foundations.html#getting-help",
    "title": "1  R Foundations",
    "section": "1.6 Getting help",
    "text": "1.6 Getting help\nThere are many ways to get help in R.\nIf we know the command for which we want help, then we run ?command in the Console to access the documentation for the object (where command is replaced by the name of the relevant command). This approach will also work with data sets, package names, object classes, etc. If we need to refer to a function in a specific package, we can use ?package::function to get help on a specific function, e.g., ?dplyr::filter.\nThe documentation will provide:\n\nA Description section with general information about the function or object.\nA Usage section with a generic template for using the function or object.\nAn Arguments section summarizing the inputs the function needs.\nA Details section may be provided with additional information about the function or object.\nA Value section that describes what is returned by the function.\nAn Examples section providing examples of how to use the function. Usually, these can be copied and pasted into the Console to better understand the function arguments and what it produces.\n\nIf we need to find a command related to a certain topic, then ??topic will search for the topic through all installed documentation and bring up any vignettes, code demonstrations, or help pages that include the topic for which we searched.\nIf we are trying to figure out why an error is being produced, what packages can be used to perform a certain analysis, how to perform a complex task that we can’t seem to figure out, etc., then we can simply do a web search for what we’re trying to figure out! Because R is such a popular programming language, it is likely we will find a stackoverflow response, a helpful blog post, an R users forum response, etc., that addresses our question.\nLastly, we can query artificial intelligence chatbots with questions about how to use R to perform a certain analysis or how to fix bugs in our code.\n\n\nHands-on Practice\nDo the following:\n\nRun?lm in the Console to get help on the lm function, which is one of the main functions used for fitting linear models.\nRun ??logarithms in the Console to search the R documentation for information about logarithms. It is likely that we will see multiple help pages that mention “logarithm”, so we may end up needing to find the desired entry via trial and error.\nRun a web search for something along the lines of “How do I change the size of the axis labels in an R plot?”.\nQuery an artificial intelligence chatbot regarding how to create a scatter plot using base R. Were the results useful and correct?",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>R Foundations</span>"
    ]
  },
  {
    "objectID": "r-foundations.html#data-types-and-structures",
    "href": "r-foundations.html#data-types-and-structures",
    "title": "1  R Foundations",
    "section": "1.7 Data types and structures",
    "text": "1.7 Data types and structures\n\n1.7.1 Basic data types\nR has 6 basic (“atomic”) vector types (https://cran.r-project.org/doc/manuals/r-release/R-lang.html#Basic-types) (R Core Team 2024):\n\ncharacter: collections of characters. E.g., \"a\", \"hello world!\".\ndouble: decimal numbers. e.g., 1.2, 1.0.\ninteger: positive and negative whole numbers. In R, we must add L to the end of a number to specify it as an integer. E.g., 1L is an integer but 1 is a double.\nlogical: Boolean values, TRUE and FALSE.\ncomplex: complex numbers. E.g., 1+3i.\nraw: a type to hold raw bytes.\n\nBoth double and integer values are specific types of numeric values.\nThe typeof function returns the R internal type or storage mode of any object.\nConsider the output of the commands below. How does the output differ?\n\ntypeof(1)\n\n[1] \"double\"\n\n\n\ntypeof(1L)\n\n[1] \"integer\"\n\n\n\ntypeof(\"hello world!\")\n\n[1] \"character\"\n\n\n\n\n1.7.2 Other important object types\nThere are other important types of objects in R that are not basic. We will discuss a few. The R Project manual provides additional information about available types (https://cran.r-project.org/doc/manuals/r-release/R-lang.html#Basic-types).\n\n1.7.2.1 Numeric\nAn object is numeric if it is of type integer or double. In that case, it’s mode is said to be numeric.\nThe is.numeric function tests whether an object can be interpreted as a set of numbers. In the code below, we use the is.numeric function to query whether certain objects are numeric.\n\nis.numeric(\"1\")\n\n[1] FALSE\n\n\n\nis.numeric(1)\n\n[1] TRUE\n\n\n\nis.numeric(1L)\n\n[1] TRUE\n\n\n\n\n1.7.2.2 NULL\nNULL is a special object to indicate an object is absent. An object having a length of zero is not the same thing as an object being absent.\n\n\n1.7.2.3 NA\nA “missing value” occurs when the value of an object isn’t known. R uses the special object NA to represent a missing value.\nIf we have a missing value, we should represent that value as NA. \"NA\" is not the same object as NA.\n\n\n1.7.2.4 Functions\nFrom R’s perspective, a function is simply another data type.\n\n\n1.7.2.5 A comment about classes\nEvery R object has a class that may be distinct from its type. Many functions will operate differently depending on an object’s class.\n\n\n\n1.7.3 Data structures\nR operates on data structures. A data structure is a “container” that holds certain kinds of information.\nR has 5 basic data structures:\n\nvector.\nmatrix.\narray.\ndata frame.\nlist.\n\nVectors, matrices, and arrays are homogeneous objects that can only store a single data type at a time. Data frames and lists can store multiple data types.\nVectors and lists are considered one-dimensional objects. A list is technically a vector. Vectors of a single type are atomic vectors (https://cran.r-project.org/doc/manuals/r-release/R-lang.html#List-objects). Matrices and data frames are considered two-dimensional objects. Arrays can have one or more dimensions.\nTable 1.1 summarizes the relationship between dimensionality and data type for the basic data structures. It is based on a table in the first edition of Hadley Wickham’s Advanced R (Wickham 2019).\n\n\n\n\nTable 1.1: A table summarizing the relationship between dimensionality and data type homogeneity for the 5 basic data structures.\n\n\n\n\n\n\nnumber of dimensions\nhomogeneous data type\nheterogeneous data types\n\n\n\n\n1\natomic vector\nlist\n\n\n2\nmatrix\ndata frame\n\n\n1 or more\narray",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>R Foundations</span>"
    ]
  },
  {
    "objectID": "r-foundations.html#vectors",
    "href": "r-foundations.html#vectors",
    "title": "1  R Foundations",
    "section": "1.8 Vectors",
    "text": "1.8 Vectors\nA vector is a one-dimensional set of data of the same type.\n\n1.8.1 Direct creation\nThe most basic way to create a vector is the c (combine) function. The c function combines values into an atomic vector or list.\nThe following commands create vectors of type numeric, character, and logical, respectively.\n\nc(1, 2, 5.3, 6, -2, 4)\nc(\"one\", \"two\", \"three\")\nc(TRUE, TRUE, FALSE, TRUE)\n\n\n\n\n1.8.2 Hands-on Practice\nRun the commands below in the Console.\n\nc(1, 2, 5.3, 6, -2, 4)\n\n\nc(\"one\", \"two\", \"three\")\n\n\nc(TRUE, TRUE, FALSE, TRUE)\n\n\nR provides two main functions for creating vectors with specific patterns: seq and rep.\n\n\n1.8.3 The seq function\nThe seq (sequence) function is used to create an equidistant series of numeric values. We provide some examples of using the seq function below:\n\nseq(1, 10) creates a sequence of numbers from 1 to 10 in increments of 1.\n1:10 creates a sequence of numbers from 1 to 10 in increments of 1.\nseq(1, 20, by = 2) creates a sequence of numbers from 1 to 20 in increments of 2.\nseq(10, 20, len = 10) creates a sequence of numbers from 10 to 20 of length 10.\n\n\n\n\nHands-on Practice\nRun the commands below in the Console and try to answer the questions below.\n\nWhat does the by argument of the seq function control?\nWhat does the len argument of the seq function control?\n\n\nseq(1, 10)\n\n\n1:10\n\n\nseq(1, 20, by = 2)\n\n\nseq(10, 20, len = 10)\n\n\n\n\n1.8.4 The rep function\nThe rep (replicate) function can be used to create a vector by replicating values. We provide some examples of using the rep function below:\n\nrep(1:3, times = 3) replicates the sequence 1, 2, 3 three times in a row.\nrep(c(\"trt1\", \"trt2\", \"trt3\"), times = 1:3) replicates \"trt1\" once, \"trt2\" twice, and \"trt3\" three times.\nrep(1:3, each = 3) replicates each element of the sequence 1, 2, 3 three times.\n\n\n\n\nHands-on Practice\nRun the commands below in the Console and try to answer the questions below.\n\nWhat does the times argument of the rep function control?\nWhat does the each argument of the rep function control?\n\n\nrep(1:3, times = 3)\n\n\nrep(c(\"trt1\", \"trt2\", \"trt3\"), times = 1:3)\n\n\nrep(1:3, each = 3)\n\n\n\n\n1.8.5 Combining vectors\nMultiple vectors of the same type can be combined into a new vector object using the c function. E.g., c(v1, v2, v3) will combine vectors v1, v2, and v3.\n\n\n\nHands-on Practice\nRun the commands below in the Console. Determine what action each command performs.\n\nv1 &lt;- 1:5\n\n\nv2 &lt;- c(1, 10, 11)\n\n\nv3 &lt;- rep(1:2, each = 3)\n\n\nnew &lt;- c(v1, v2, v3)\n\n\nnew\n\n\n\n\n1.8.6 Categorical vectors\nCategorical data should be stored as a factor in R. Sometimes, we might naively store categorical data as character or numeric objects for simplicity. In that situation, our code might still work because a cautious developer planned for our laziness, but it is best to use good coding practices that minimize potential issues. For that reason, represent categorical data as a factor.\n\n1.8.6.1 Creating a factor object\nThe factor function takes a vector of values that can be coerced to type character and converts them to an object of class factor. In the code chunks below, we create two factor objects from vectors.\n\nf1 &lt;- factor(rep(1:6, times = 3))\nf1\n\n [1] 1 2 3 4 5 6 1 2 3 4 5 6 1 2 3 4 5 6\nLevels: 1 2 3 4 5 6\n\n\n\nf2 &lt;- factor(c(\"a\", 7, \"blue\", \"blue\", FALSE))\nf2\n\n[1] a     7     blue  blue  FALSE\nLevels: 7 a blue FALSE\n\n\nA printed factor object lists the Levels (i.e., unique categories) of the object.\nThe is.factor function can be used to determine whether an object is a factor.\nfactor objects aren’t technically vectors (e.g., running is.factor(f2) based on the above code will return FALSE) though they essentially behave like vectors, which is why discuss them here.\n\n\n\nHands-on Practice\nComplete the following tasks:\n\nCreate a vector named grp that has two levels: a and b, where the first 7 values are a and the second 4 values are b.\nRun is.factor(grp) in the Console.\nRun is.vector(grp) in the Console.\nRun typeof(grp) in the Console.\n\nRelated to the last task, a factor object is technically a collection of integers that have labels associated with each unique integer value.\n\n\n\n1.8.6.2 Creating an ordered factor object\nCategorical data sometimes has a natural ordering that we want to acknowledge in our analysis. E.g., we may have categorical data with the levels small, medium, and large. The levels have a natural ordering (either smallest to largest or vice versa, depending on the context).\nSuppose we create the size vector with character values \"small\", \"medium\", and \"large\".\n\nsize &lt;- c(\"small\", \"medium\", \"small\", \"large\", \"medium\", \"medium\", \"large\")\n\nIf we convert size to a factor object, R will automatically order the levels of size alphabetically, as seen from the code chunk below.\n\nfactor(size)\n\n[1] small  medium small  large  medium medium large \nLevels: large medium small\n\n\nThis is not technically a problem, but can result in undesirable side effects such as plots with levels in an undesirable order.\nTo create an ordered factor object, we specify the desired order of the levels using the levels argument and set the ordered argument to TRUE, as in the code below.\n\nfactor(size, levels = c(\"small\", \"medium\", \"large\"), ordered = TRUE)\n\n[1] small  medium small  large  medium medium large \nLevels: small &lt; medium &lt; large\n\n\n\n\n\n1.8.7 Extracting parts of a vector\nParts a vector can be extracted by appending an index vector in square brackets [] to the name of the vector, where the index vector indicates which parts of the vector to retain or exclude. We can include either numbers or logical values in our index vector. We discuss both approaches below.\n\n1.8.7.1 Selection using a numeric index vector\nLet’s create a numeric vector a with the values 2, 4, 6, 8, 10, 12, 14, 16.\n\na &lt;- seq(2, 16, by = 2)\na\n\n[1]  2  4  6  8 10 12 14 16\n\n\nTo extract the 2nd, 4th, and 6th elements of a, we can use the code below. The code indicates that the 2nd, 4th, and 6th elements of a should be extracted.\n\na[c(2, 4, 6)]\n\n[1]  4  8 12\n\n\nWe can also use “negative” indexing to indicate the elements of the vector we want to exclude. Specifically, supplying a negative index vector indicates the values we want to exclude from our selection.\nIn the example below, we use the minus (-) sign in front of the index vector c(2, 4, 6) to indicate we want all elements of a EXCEPT the 2nd, 4th, and 6th.\n\na[-c(2, 4, 6)]\n\n[1]  2  6 10 14 16\n\n\nRunning the code chunk below excludes the 3rd through 6th elements of a.\n\na[-(3:6)]\n\n[1]  2  4 14 16\n\n\n\n\n1.8.7.2 Logical expressions\nA logical expression uses one or more logical operators to determine which elements of an object satisfy the specified statement. The basic logical operators are:\n\n&lt;, &lt;=: less than, less than or equal to.\n&gt;, &gt;=: greater than, greater than or equal to.\n==: equal to.\n!=: not equal to.\n\nCreating a logical expression with a vector will result in a logical vector indicating whether each element satisfies the logical expression.\n\n\n\nHands-on Practice\nRun the following commands in R and see what is printed. What task is each statement performing?\n\na &gt; 10\n\n\na &lt;= 4\n\n\na == 10\n\n\na != 10\n\n\n\n\n1.8.7.3 The “and”, “or”, and “not” operators\nWe can create more complicated logical expressions using the “and”, “or”, and “not” operators.\n\n&: and.\n|: or.\n!: not, i.e., not true.\n\nThe & operator returns TRUE if all logical values connected by the & are TRUE, otherwise it returns FALSE.\nThe | operator returns TRUE if any logical values connected by the | are TRUE, otherwise it returns FALSE.\nThe ! operator returns the complement of a logical value or expression.\n\n\n\nHands-on Practice\nRun the following commands below in the Console.\n\nWhat role does & serve in a sequence of logical values?\nWhat role does | serve in a sequence of logical values?\nWhat role does ! serve in a sequence of logical values?\n\n\nTRUE & TRUE & TRUE\n\n\nTRUE & TRUE & FALSE\n\n\nFALSE | TRUE | FALSE\n\n\nFALSE | FALSE | FALSE\n\n\n!TRUE\n\n\n!FALSE\n\n\n\n\n1.8.7.4 Connecting logical expressions\nLogical expressions can be connected via & and | (and impacted via !), in which case the operators are applied elementwise (i.e., to all of the first elements in the expressions, then all the second elements in the expressions, etc).\n\n\n\nHands-on Practice\nRun the following commands in R and see what is printed. What task is each statement performing?\nNote that the parentheses () are used to group logical expressions to more easily understand what is being done. This is a good coding style to follow.\n\n(a &gt; 6) & (a &lt;= 10)\n\n\n(a &lt;= 4) | (a &gt;= 12)\n\n\n!((a &lt;= 4) | (a &gt;= 12))\n\n\n\n1.8.7.5 Selection using logical expressions\nLogical expressions can be used to return parts of an object satisfying the appropriate criteria. Specifically, we pass logical expressions within the square brackets to access part of a data structure. This syntax will return each element of the object for which the expression is TRUE.\n\n\n\n\nHands-on Practice\nRun the following commands in R and see what is printed. What task is each statement performing?\n\na[a &lt; 6]\n\n\na[a == 10]\n\n\na[(a &lt; 6)|(a == 10)]",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>R Foundations</span>"
    ]
  },
  {
    "objectID": "r-foundations.html#helpful-functions",
    "href": "r-foundations.html#helpful-functions",
    "title": "1  R Foundations",
    "section": "1.9 Helpful functions",
    "text": "1.9 Helpful functions\nWe provide a brief overview of R functions we often use in our data analysis.\n\n1.9.1 General functions\nFor brevity, Table 1.2 provides a table of functions commonly useful for basic data analysis along with a description of their purpose.\n\n\n\n\nTable 1.2: Functions frequently useful for data analysis.\n\n\n\n\n\n\n\n\n\n\nfunction.\npurpose\n\n\n\n\nlength\nDetermines the length/number of elements in an object.\n\n\nsum\nSums the elements in the object.\n\n\nmean\nComputes the sample mean of the elements in an object.\n\n\nvar\nComputes the sample variance of the elements in an object.\n\n\nsd\nComputes the sample standard deviation the elements of an object.\n\n\nrange\nDetermines the range (minimum and maximum) of the elements of an object.\n\n\nlog\nComputes the (natural) logarithm of elements in an object.\n\n\nsummary\nReturns a summary of an object. The output changes depending on the class type of the object.\n\n\nstr\nProvides information about the structure of an object. Usually, the class of the object and some information about its size.\n\n\n\n\n\n\n\n\n\n\n\nHands-on Practice\nRun the following commands in the Console. Determine what task each command is performing.\n\nx &lt;- rexp(100) # sample 100 iid values from an Exponential(1) distribution\n\n\nlength(x)\n\n\nsum(x)\n\n\nmean(x)\n\n\nvar(x)\n\n\nsd(x)\n\n\nrange(x)\n\n\nlog(x)\n\n\nsummary(x)\n\n\nstr(x) # structure of x\n\n\n\n\n1.9.2 Functions related to statistical distributions\nR is designed specifically for statistical analysis, so it natively includes functionality for determining properties of statistical distributions. R makes it easy to evaluate the cumulative distribution function (CDF) of a distribution, the quantiles of a distribution, the density or mass of a distribution, and to sample random values from a distribution.\nSuppose that a random variable \\(X\\) has the dist distribution. The function templates in the list below describe how to obtain certain properties of \\(X\\).\n\np[dist](q, ...): returns the cdf of \\(X\\) evaluated at q, i.e., \\(p=P(X\\leq q)\\).\nq[dist](p, ...): returns the inverse cdf (or quantile function) of \\(X\\) evaluated at \\(p\\), i.e., \\(q = \\inf\\{x: P(X\\leq x) \\geq p\\}\\).\nd[dist](x, ...): returns the mass or density of \\(X\\) evaluated at \\(x\\) (depending on whether \\(X\\) is a discrete or continuous random variable).\nr[dist](n, ...): returns an independent and identically distributed random sample of size n having the same distribution as \\(X\\).\nThe ... indicates that additional arguments describing the parameters of the distribution may be required.\n\nTo determine the distributions available by default in R, run ?Distributions in the R Console. We demonstrate some of this functionality in the practice below.\nNote: If we are using functions related to statistical distributions in R, then it is imperative that we look at the associated documentation to determine the parameterization of the distribution, as this dramatically impacts the results. Some distributions have multiple common parameterizations.\n\n\n\nHands-on Practice\nRun the following commands in R to see the output. Before each command is a description of the action performed by the command.\npnorm(1.96, mean = 0, sd = 1) returns the probability that a standard normal random variable is less than or equal to 1.96, i.e., \\(P(X \\leq 1.96)\\).\n\npnorm(1.96, mean = 0, sd = 1)\n\nqunif(0.6, min = 0, max = 1) returns the value \\(x\\) such that \\(P(X\\leq x) = 0.6\\) for a uniform random variable on the interval \\([0, 1]\\).\n\nqunif(0.6, min = 0, max = 1)\n\ndbinom(2, size = 20, prob = .2) returns the probability that \\(X\\) equals 2 when \\(X\\) has a Binomial distribution with \\(n=20\\) trials and the probability of a successful trial is \\(0.2\\).\n\ndbinom(2, size = 20, prob = .2)\n\ndexp(1, rate = 2) evaluates the density of an exponential random variable with mean = 1/2 (i.e., the reciprocal of the rate) at \\(x=1\\).\n\ndexp(1, rate = 2)\n\nrchisq(10, df = 5) draws a sample of 10 observations from a chi-squared random variable with 5 degrees of freedom.\n\nrchisq(10, df = 5)",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>R Foundations</span>"
    ]
  },
  {
    "objectID": "r-foundations.html#data-frames",
    "href": "r-foundations.html#data-frames",
    "title": "1  R Foundations",
    "section": "1.10 Data Frames",
    "text": "1.10 Data Frames\nData frames are a fundamental data structure used by most of R’s modeling functions. Data frames are two-dimensional data objects where each column of a data frame is a vector (or variable) of possibly different data types. The class of a base R data frame is data.frame, which is technically a specially structured list.\nIn general, we prefer our data to be tidy, which means that each variable forms a column of the data frame, and each observation forms a row.\n\n1.10.1 Direct creation\nData frames are directly created by passing vectors into the data.frame function. The names of the columns in the data frame are the names of the vectors we give the data.frame function. Consider the following simple example.\n\n# simple data frame creation\nd &lt;- c(1, 2, 3, 4)\ne &lt;- c(\"red\", \"white\", \"blue\", NA)\nf &lt;- c(TRUE, TRUE, TRUE, FALSE)\ndf &lt;- data.frame(d,e,f)\ndf\n\n  d     e     f\n1 1   red  TRUE\n2 2 white  TRUE\n3 3  blue  TRUE\n4 4  &lt;NA&gt; FALSE\n\n\nThe columns of a data frame can be renamed using the names function on the data frame and assigning a vector of names to the data frame.\n\n# name columns of data frame\nnames(df) &lt;- c(\"ID\", \"Color\", \"Passed\")\ndf\n\n  ID Color Passed\n1  1   red   TRUE\n2  2 white   TRUE\n3  3  blue   TRUE\n4  4  &lt;NA&gt;  FALSE\n\n\nThe columns of a data frame can be named when we create the data frame by using the syntax name = for each vector of data.\n\n# create data frame with better column names\ndf2 &lt;- data.frame(ID = d, Color = e, Passed = f)\ndf2\n\n  ID Color Passed\n1  1   red   TRUE\n2  2 white   TRUE\n3  3  blue   TRUE\n4  4  &lt;NA&gt;  FALSE\n\n\n\n\n1.10.2 Importing Data\nDirect creation of data frames is only appropriate for very small data sets. In practice, we will import data from a file into R.\nThe read.table function imports data in table format from file into R as a data frame.\nThe basic usage of this function is: read.table(file, header = TRUE, sep = \",\").\n\nfile is the file path and name of the file we want to import into R.\n\nIf we don’t know the file path, setting file = file.choose() will bring up a dialog box asking us to locate the file we want to import.\n\nheader specifies whether the data file has a header (variable labels for each column of data in the first row of the data file).\n\nIf we don’t specify this option in R or specify header = FALSE, then R will assume the file doesn’t have any headings.\nheader = TRUE tells R to read in the data as a data frame with column names taken from the first row of the data file.\n\nsep specifies the delimiter separating elements in the file.\n\nIf each column of data in the file is separated by a space, then use sep = \" \".\nIf each column of data in the file is separated by a comma, then use sep = \",\".\nIf each column of data in the file is separated by a tab, then use sep = \"\\t\".\n\n\n\n\n\nHands-on Practice\nConsider reading in a csv (comma separated file) with a header.\nThe file in question is available in the api2lm package (French 2023) and contains crime-related information for U.S. states for the year 2009. However, we will import the data directly from a location where it is stored on GitHub.\nIn the code below, we specify the path of the file prior to specifying the file name (crime2009.csv). Since the file has a header, we specify header = TRUE. Since the data values are separated by commas, we specify sep = \",\".\n\n# specify file path\npath &lt;- \"https://raw.githubusercontent.com/jfrench/api2lm/main/inst/extdata/crime2009.csv\"\n# import data as data frame\ncrime2009 &lt;- read.table(file = path, header = TRUE, sep = \",\")\n# view data structure\nstr(crime2009)\n\n'data.frame':   51 obs. of  8 variables:\n $ state  : chr  \"Alabama\" \"Alaska\" \"Arizona\" \"Arkansas\" ...\n $ violent: num  460 633 423 530 473 ...\n $ murder : num  7.1 3.2 5.5 6.3 5.4 3.2 3 4.6 24.2 5.5 ...\n $ hs_grad: num  82.1 91.4 84.2 82.4 80.6 89.3 88.6 87.4 87.1 85.3 ...\n $ poverty: num  17.5 9 16.5 18.8 14.2 12.9 9.4 10.8 18.4 14.9 ...\n $ single : num  29 25.5 25.7 26.3 27.8 21.4 25 27.6 48 26.6 ...\n $ white  : num  70 68.3 80 78.4 62.7 84.6 79.1 71.9 38.7 76.9 ...\n $ urban  : num  48.6 44.5 80.1 39.5 89.7 ...\n\n\nRunning str on the data frame gives us a general picture of the values stored in the data frame, the variable names, and the default variable types.\nThe read_table function in the readr package (Wickham, Hester, and Bryan 2024) is arguably a better way of importing tabular data from file and uses similar syntax. We can import data contained in a Microsoft Excel file using functions available in the readxl package (Wickham and Bryan 2023).\n\n\n\n1.10.3 Extracting parts of a data frame\nR provides many ways to extract parts of a data frame. We will provide several examples using the mtcars data frame in the datasets package.\nThe mtcars data frame has 32 observations for 11 variables. The variables are:\n\nmpg: miles per gallon.\ncyl: number of cylinders.\ndisp: engine displacement (cubic inches).\nhp: horsepower.\ndrat: rear axle ratio.\nwt: weight in 1000s of pounds.\nqsec: time in seconds to travel 0.25 of a mile.\nvs: engine shape (0 = V-shaped, 1 = straight).\nam: transmission type (0 = automatic, 1 = manual).\ngear: number of forward gears.\ncarb: number of carburetors.\n\nWe load the data set and examine the basic structure by running the commands below.\n\ndata(mtcars) # load data set\nstr(mtcars)  # examine data structure\n\n'data.frame':   32 obs. of  11 variables:\n $ mpg : num  21 21 22.8 21.4 18.7 18.1 14.3 24.4 22.8 19.2 ...\n $ cyl : num  6 6 4 6 8 6 8 4 4 6 ...\n $ disp: num  160 160 108 258 360 ...\n $ hp  : num  110 110 93 110 175 105 245 62 95 123 ...\n $ drat: num  3.9 3.9 3.85 3.08 3.15 2.76 3.21 3.69 3.92 3.92 ...\n $ wt  : num  2.62 2.88 2.32 3.21 3.44 ...\n $ qsec: num  16.5 17 18.6 19.4 17 ...\n $ vs  : num  0 0 1 1 0 1 0 1 1 1 ...\n $ am  : num  1 1 1 0 0 0 0 0 0 0 ...\n $ gear: num  4 4 4 3 3 3 3 4 4 4 ...\n $ carb: num  4 4 1 1 2 1 4 2 2 4 ...\n\n\nWe should do some data cleaning on this data set (see ?sec-ch-eda), but we will refrain from this for simplicity.\n\n\n1.10.3.1 Direct extraction\nThe column variables of a data frame may be extracted from a data frame by specifying the data frame’s name, then $, and then specifying the name of the desired variable. This pulls the actual variable vector out of the data frame, so the thing extracted is a vector, not a data frame.\nBelow, we extract the mpg variable from the mtcars data frame.\n\nmtcars$mpg\n\n [1] 21.0 21.0 22.8 21.4 18.7 18.1 14.3 24.4 22.8 19.2 17.8 16.4 17.3 15.2 10.4\n[16] 10.4 14.7 32.4 30.4 33.9 21.5 15.5 15.2 13.3 19.2 27.3 26.0 30.4 15.8 19.7\n[31] 15.0 21.4\n\n\nAnother way to extract a variable from a data frame as a vector is df[, \"var\"], where df is the name of our data frame and var is the desired variable name. We extract the mpg vector from mtcars below.\n\nmtcars[,\"mpg\"]\n\n [1] 21.0 21.0 22.8 21.4 18.7 18.1 14.3 24.4 22.8 19.2 17.8 16.4 17.3 15.2 10.4\n[16] 10.4 14.7 32.4 30.4 33.9 21.5 15.5 15.2 13.3 19.2 27.3 26.0 30.4 15.8 19.7\n[31] 15.0 21.4\n\n\nThe syntax above is a special case of extracting information from a data frame using df[rows, columns] style syntax, where rows and columns indicate the desired rows or columns to extract. If either the rows or columns are left blank, then all rows or columns, respectively, are extracted.\nThe df[rows, columns] syntax has confusing behavior when we only specify a single column. Specifically, the [ operator has a drop argument that is set to TRUE by default. The drop argument controls whether the result is coerced to the lowest possible dimension. If we we only extract a single column of our data frame using [, then R will coerce the result to a vector by default.\nTo get around the default drop behavior we can change the drop argument to FALSE, as shown below. We only retain the first 5 rows for brevity.\n\n# extract mpg variable, keep as data frame\nmtcars[1:5, \"mpg\", drop = FALSE]\n\n                   mpg\nMazda RX4         21.0\nMazda RX4 Wag     21.0\nDatsun 710        22.8\nHornet 4 Drive    21.4\nHornet Sportabout 18.7\n\n\nAn easier approach to avoiding the default drop behavior is to use the slightly different syntax df[\"var\"] (notice we no longer have the comma to separate rows and columns). We use this syntax below for the mpg variable in mtcars. For brevity, we use the print method for a data.frame object and specify the max argument to limit the number of data frame entries we print.\n\n# extract mpg variable, keep as data frame, show only 5 rows\nprint(mtcars[\"mpg\"], max = 5)\n\n                   mpg\nMazda RX4         21.0\nMazda RX4 Wag     21.0\nDatsun 710        22.8\nHornet 4 Drive    21.4\nHornet Sportabout 18.7\n [ reached 'max' / getOption(\"max.print\") -- omitted 27 rows ]\n\n\nTo select multiple variables in a data frame, we can provide a character vector with multiple variable names between []. In the example below, we extract both the mpg and cyl variables from mtcars.\n\nprint(mtcars[c(\"mpg\", \"cyl\")], max = 10)\n\n                   mpg cyl\nMazda RX4         21.0   6\nMazda RX4 Wag     21.0   6\nDatsun 710        22.8   4\nHornet 4 Drive    21.4   6\nHornet Sportabout 18.7   8\n [ reached 'max' / getOption(\"max.print\") -- omitted 27 rows ]\n\n\nWe can also use numeric indices to directly indicate the rows or columns of the data frame that we would like to extract. We can also combine this syntax style with the syntax styles previously discussed.\n\ndf[1,] accesses the first row of df.\ndf[1:2,] accesses the first two rows of df.\ndf[, 2] accesses the second column of df.\ndf[1:2, 2:3] accesses the information in rows 1 and 2 of columns 2 and 3 of df.\ndf[c(1, 3, 5), c(\"var1\", \"var2\")] accesses the information in rows 1, 3, and 5 of the var1 and var2 variables.\n\nWe practice these techniques below.\n\n\n\nHands-on Practice\nRun the following commands in the Console. Determine what task each command is performing.\n\ndf3 &lt;- data.frame(numbers = 1:5,\n                  characters = letters[1:5],\n                  logicals = c(TRUE, TRUE, FALSE, TRUE, FALSE))\n\n\ndf3\n\n\ndf3$logicals\n\n\ndf3[1, ]\n\n\ndf3[, 3]\n\n\ndf3[, 2:3]\n\n\ndf3[, c(\"numbers\", \"logicals\")]\n\n\ndf3[c(\"numbers\", \"logicals\")]\n\n\n\n\n1.10.3.2 Extraction using logical expressions\nLogical expressions can be used to subset a data frame.\nTo select specific rows of a data frame, we use the syntax df[logical vector, ], where logical vector is a valid logical vector whose length matches the number of rows in the data frame. Usually, the logical vector is created using a logical expression involving one or more data frame variables. In the code below, we extract the rows of the mtcars data frame for which the hp variable is more than 250.\n\nmtcars[mtcars$hp &gt; 250,]\n\n                mpg cyl disp  hp drat   wt qsec vs am gear carb\nFord Pantera L 15.8   8  351 264 4.22 3.17 14.5  0  1    5    4\nMaserati Bora  15.0   8  301 335 3.54 3.57 14.6  0  1    5    8\n\n\nWe can make the logical expression more complicated and also select specific variables using the syntax discussed in Section 1.10.3.1. Below, we extract the rows of mtcars with 8 cylinders and mpg &gt; 17, while extracting only the mpg, cyl, disp, and hp variables.\n\n# return rows with `cyl == 8` and `mpg &gt; 17`\n# return columns mpg, cyl, disp, hp\nmtcars[mtcars$cyl == 8 & mtcars$mpg &gt; 17, c(\"mpg\", \"cyl\", \"disp\", \"hp\")]\n\n                   mpg cyl  disp  hp\nHornet Sportabout 18.7   8 360.0 175\nMerc 450SL        17.3   8 275.8 180\nPontiac Firebird  19.2   8 400.0 175\n\n\n\n\n1.10.3.3 Extraction using the subset function\nThe techniques for extracting parts of a data frame discussed in -Section 1.10.3.1 and -Section 1.10.3.2 are the fundamental approaches for selecting desired parts of a data frame. However, these techniques can seem complex and difficult to interpret, particularly when looking back at code we have written in the past. A sleeker approach to extracting part of a data frame is to use the subset function.\nThe subset function returns the elements of a data frame that meets the specified conditions. The basic usage of this function is: subset(x, subset, select, drop = FALSE)\n\nx is the object we want to subset.\n\nx can be a vector, matrix, or data frame.\n\nsubset is a logical expression that indicates the elements or rows of x to keep (TRUE means keep).\nselect is a vector that indicates the columns to keep.\ndrop is a logical value indicating whether the data frame should “drop” into a vector if only a single row or column is kept. The default is FALSE, meaning that a data frame will always be returned by the subset function by default.\n\nThere are many clever ways of using subset to select specific parts of a data frame. We encourage the reader to run ?base::subset in the Console for more details.\n\n\n\nHands-on Practice\nRun the following commands in the Console to use the subset function to extract parts of the mtcars data frame.\nThe command below subsets the rows of mtcars that have more than 4 gears. Note that any variables referred to in the subset function are assumed to be part of the supplied data frame or are available in memory.\n\nsubset(mtcars, subset = gear &gt; 4)\n\nThe command below will select the disp, hp, and gear variables of mtcars but will exclude the other columns.\n\nsubset(mtcars, select = c(disp, hp, gear))\n\nLastly, we can use the following command to perform the two previous subsetting actions in one step.\n\nsubset(mtcars, subset = gear &gt; 4, select = c(disp, hp, gear))\n\nAn advantage of the subset function is that it makes code easily readable. Using conventional base R syntax, the final code example above would be: mtcars[mtcars$gear&gt;4, c(\"disp\", \"hp\", \"gear\")].\nIt can be difficult to look at base R code and immediately tell what it happening, so the subset function adds clarity. This is important for collaborating with others, including our future selves!\n\n\n\n\n1.10.4 Modifying a Data Frame\nColumns can be added to a data frame using $ and the assignment operator. In the example below, we add a new column, kpg, to the mtcars data set based on a transformatino of the mpg column.\n\nmtcars$kpg &lt;- mtcars$mpg*1.6\nhead(mtcars)\n\n                   mpg cyl disp  hp drat    wt  qsec vs am gear carb   kpg\nMazda RX4         21.0   6  160 110 3.90 2.620 16.46  0  1    4    4 33.60\nMazda RX4 Wag     21.0   6  160 110 3.90 2.875 17.02  0  1    4    4 33.60\nDatsun 710        22.8   4  108  93 3.85 2.320 18.61  1  1    4    1 36.48\nHornet 4 Drive    21.4   6  258 110 3.08 3.215 19.44  1  0    3    1 34.24\nHornet Sportabout 18.7   8  360 175 3.15 3.440 17.02  0  0    3    2 29.92\nValiant           18.1   6  225 105 2.76 3.460 20.22  1  0    3    1 28.96",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>R Foundations</span>"
    ]
  },
  {
    "objectID": "r-foundations.html#using-the-pipe-operator",
    "href": "r-foundations.html#using-the-pipe-operator",
    "title": "1  R Foundations",
    "section": "1.11 Using the pipe operator",
    "text": "1.11 Using the pipe operator\nR’s native pipe operator (|&gt;) allows us to “pipe” the object on the left side of the operator into the first argument of the function on the right side of the operator. There are ways to modify this default behavior, but we will not discuss them.\nThe pipe operator is a convenient way to string together numerous steps in a string of commands. This coding style is generally considered more readable than other approaches because we can incrementally modify the object through each pipe, and each step of the pipe is easy to understand. Ultimately, it’s a stylistic choice that we can decide to adopt or ignore.\nConsider the following approaches to extracting part of mtcars. We choose the rows for which engine displacement is more than 400 and only keep the mpg, disp, and hp columns. We first do this in a single function call using the subset function.\n\n# two styles for selecting certain rows and columns of mtcars\nsubset(mtcars,\n       subset = disp &gt; 400,\n       select = c(mpg, disp, hp))\n\n                     mpg disp  hp\nCadillac Fleetwood  10.4  472 205\nLincoln Continental 10.4  460 215\nChrysler Imperial   14.7  440 230\n\n\nNext, we use the piping approach to break the action into smaller parts.\n\nmtcars |&gt;\n  subset(subset = disp &gt; 400) |&gt;\n  subset(select = c(mpg, disp, hp))\n\n                     mpg disp  hp\nCadillac Fleetwood  10.4  472 205\nLincoln Continental 10.4  460 215\nChrysler Imperial   14.7  440 230\n\n\nWhen reading code with pipes, the pipe can be thought of as the word “then”. In the code above, we take mtcars then subset it based on disp and then select some columns.\nMost parts of the world do not use miles per gallon to measure fuel economy because they don’t measure distance in miles nor volume in gallons. A common measure of fuel economy is the liters of fuel required to travel 100 kilometers. Noting that 3.8 liters is approximately equivalent to 1 U.S. gallon and 1.6 kilometers is approximately equivalent to 1 mile, we can convert fuel economy of \\(x\\) miles per gallon to liters per 100 kilometers by noting:\n\\[\\frac{1}{x}\\frac{\\mathrm{gal}}{\\mathrm{mi}}\\times\\frac{3.8}{1}\\frac{\\mathrm{L}}{\\mathrm{gal}}\\times\\frac{1}{1.6}\\frac{\\mathrm{mi}}{\\mathrm{km}}\\times\\frac{100\\;\\mathrm{km}}{100\\;\\mathrm{km}} = \\frac{237.5}{x}\\frac{\\mathrm{L}}{100\\;\\mathrm{km}}.\\]\nThus, to convert from miles per gallon to liters per 100 kilometers, we take 237.5 and divide by the number of miles per gallon.\nWe consider two approaches for converting the units of a variable in miles per gallon to liters per 100 km. In the first approach, we use the base::transform function create a new variable, lp100km, in the mtcars data frame that describes the liters of fuel each car requires to travel 100 kilometers and assign it the name mtcars2. Then we select only the columns mpg and lp100km from mtcars and assign it the name mtcars3. We then use the head function to print only the first 5 observations. Run ?base::transform in the Console for more details and examples.\n\n# create new variable\nmtcars2 &lt;- transform(mtcars, lp100km = 237.5/mpg)\n# select certain columns\nmtcars3 &lt;- subset(mtcars2, select = c(mpg, lp100km))\n# print first 5 rows\nhead(mtcars3, n = 5)\n\n                   mpg  lp100km\nMazda RX4         21.0 11.30952\nMazda RX4 Wag     21.0 11.30952\nDatsun 710        22.8 10.41667\nHornet 4 Drive    21.4 11.09813\nHornet Sportabout 18.7 12.70053\n\n\nNext, we perform the actions above with pipes.\n\n# create new variable, select columns, extract first 5 rows\nmtcars |&gt;\n  transform(lp100km = 237.5/mpg) |&gt;\n  subset(select = c(mpg, lp100km)) |&gt;\n  head(n = 5)\n\n                   mpg  lp100km\nMazda RX4         21.0 11.30952\nMazda RX4 Wag     21.0 11.30952\nDatsun 710        22.8 10.41667\nHornet 4 Drive    21.4 11.09813\nHornet Sportabout 18.7 12.70053",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>R Foundations</span>"
    ]
  },
  {
    "objectID": "r-foundations.html#dealing-with-common-problems",
    "href": "r-foundations.html#dealing-with-common-problems",
    "title": "1  R Foundations",
    "section": "1.12 Dealing with common problems",
    "text": "1.12 Dealing with common problems\nWe are going to have to deal with many errors and problems as we use R. It happens even to the best programmers.\nEvery problem is unique, but there are common mistakes that we try to provide insight for below.\n\n1.12.1 Error in ...: could not find function \"...\"\nWe probably forgot to load the package needed to use the function. We may also have misspelled the function name.\n\n\n1.12.2 Error: object '...' not found\nThe object doesn’t exist in loaded memory. Perhaps we forget to assign that name to an object or misspelled the name of the object we are trying to access.\n\n\n1.12.3 Error in plot.new() : figure margins too large\nThis typically happens because our Plots pane is too small. We should increase the size of the Plots pane and try again.\n\n\n1.12.4 Code was working, but isn’t anymore\nWe may have run code out of order. It may work if we run it in order. Or we may have run something in the Console that we don’t have in our Script file. It is good practice to clear our environment (the objects R has loaded in memory) using the broom icon in the Environment pane and rerunning our entire Script file to ensure it behaves as expected. The broom icon is shown in Figure 1.4.\n\n\n\n\n\n\n\n\nFigure 1.4: The broom icon can be clicked to clear the objects loaded in the Environment.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>R Foundations</span>"
    ]
  },
  {
    "objectID": "r-foundations.html#ecosystem-debate",
    "href": "r-foundations.html#ecosystem-debate",
    "title": "1  R Foundations",
    "section": "1.13 Ecosystem debate",
    "text": "1.13 Ecosystem debate\nIn general, we favor performing analysis using base R functionality, which means we try to perform our analysis with features R offers by default. Base R code is easier to maintain since base R features change very slowly and are nearly always backwards compatible. Additionally, base R functions are often faster than alternatives. However, more complicated aspects of regression analysis are not easy to perform using base R, so we will introduce new packages and functions as we progress.\nMany readers may have previous experience working with the tidyverse (https://www.tidyverse.org) and wonder how frequently we use tidyverse functionality. The tidyverse offers a unified framework for data manipulation and visualization that tends to be more consistent than base R. However, there are many situations where a base R solution is more straightforward than a tidyverse solution, not to mention the fact that there are many aspects of R programming (e.g., S3 and S4 objects, method dispatch) that require knowledge of base R features. Because the R universe is vast and there are many competing coding styles, we will prioritize analysis approaches using base R, which gives users a stronger programming foundation. However, we will use parts of the tidyverse when it greatly simplifies analysis, data manipulation, or visualization because it provides an extremely useful feature set.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>R Foundations</span>"
    ]
  },
  {
    "objectID": "r-foundations.html#going-deeper",
    "href": "r-foundations.html#going-deeper",
    "title": "1  R Foundations",
    "section": "1.14 Going deeper",
    "text": "1.14 Going deeper\n\n1.14.1 Comparing assignment operators\nAs previously mentioned in Section Section 1.3, both &lt;- and = can mostly be used interchangeably for assignment. But there are times when using = for assignment can be problematic. Consider the examples below where we want to use system.time to time how long it takes to draw 100 values from a standard normal distribution and assign it the name result.\nThis code works:\n\nsystem.time(result &lt;- rnorm(100))\n\n   user  system elapsed \n      0       0       0 \n\n\nThis code doesn’t work:\n\nsystem.time(result = rnorm(100))\n\nError in system.time(result = rnorm(100)): unused argument (result = rnorm(100))\n\n\nWhat’s the difference? In the second case, R thinks we are setting the result argument of the system.time function (which doesn’t exist) to the value produced by rnorm(100).\nThus, it is best to use &lt;- for assigning a name to an object and reserving = for specifying function arguments.\n\n\n\n\nFrench, Joshua P. 2023. Api2lm: Functions and Data Sets for the Book \"a Progressive Introduction to Linear Models\". https://CRAN.R-project.org/package=api2lm.\n\n\nMüller, Kirill, and Hadley Wickham. 2023. Tibble: Simple Data Frames. https://tibble.tidyverse.org/.\n\n\nR Core Team. 2024. R: A Language and Environment for Statistical Computing. Vienna, Austria: R Foundation for Statistical Computing. https://www.R-project.org/.\n\n\nWickham, Hadley. 2019. Advanced R. CRC press. http://adv-r.had.co.nz/.\n\n\n———. 2023a. Forcats: Tools for Working with Categorical Variables (Factors). https://forcats.tidyverse.org/.\n\n\n———. 2023b. Stringr: Simple, Consistent Wrappers for Common String Operations. https://stringr.tidyverse.org.\n\n\n———. 2023c. Tidyverse: Easily Install and Load the Tidyverse. https://tidyverse.tidyverse.org.\n\n\nWickham, Hadley, and Jennifer Bryan. 2023. Readxl: Read Excel Files. https://readxl.tidyverse.org.\n\n\nWickham, Hadley, Winston Chang, Lionel Henry, Thomas Lin Pedersen, Kohske Takahashi, Claus Wilke, Kara Woo, Hiroaki Yutani, Dewey Dunnington, and Teun van den Brand. 2024. Ggplot2: Create Elegant Data Visualisations Using the Grammar of Graphics. https://ggplot2.tidyverse.org.\n\n\nWickham, Hadley, Romain François, Lionel Henry, Kirill Müller, and Davis Vaughan. 2023. Dplyr: A Grammar of Data Manipulation. https://dplyr.tidyverse.org.\n\n\nWickham, Hadley, and Lionel Henry. 2023. Purrr: Functional Programming Tools. https://purrr.tidyverse.org/.\n\n\nWickham, Hadley, Jim Hester, and Jennifer Bryan. 2024. Readr: Read Rectangular Text Data. https://readr.tidyverse.org.\n\n\nWickham, Hadley, Davis Vaughan, and Maximilian Girlich. 2024. Tidyr: Tidy Messy Data. https://tidyr.tidyverse.org.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>R Foundations</span>"
    ]
  },
  {
    "objectID": "data-exploration.html",
    "href": "data-exploration.html",
    "title": "2  Data Cleaning and Exploration",
    "section": "",
    "text": "2.1 Raw Palmer penguins data –&gt;\nTo attach a data set from an R package into our working environment, we run the data function while specifying the name of the data set to be loaded and the package that contains the data. We do this for the penguins_raw data below.\ndata(penguins, package = \"palmerpenguins\")\nThis command actually loads two data sets: penguins_raw, the data set we will be looking at, and penguins, a simplified version of penguins_raw. Note that this particular data set loads in a nonstandard way; we would normally load the penguins_raw data using data(penguins_raw, package = \"palmerpenguins\").\nWe could have also loaded the data set by running the following commands in the Console.\nlibrary(palmerpenguins)\ndata(penguins)\nThis second approach loads and attaches everything the package includes into memory (functions, data, etc). If we are going to be using many functions or objects from a package, then the second approach is sensible. Otherwise, the first approach is more precise and is better coding practice to prevent masking (as discussed in Chapter 1) and namespace pollution.\nThe penguins_raw data set provides data related to various penguin species measured in the Palmer Archipelago (Antarctica), originally provided by Gorman, Williams, and Fraser (2014).\nThe data set includes 344 observations of 17 variables. The variables are:",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Data Cleaning and Exploration</span>"
    ]
  },
  {
    "objectID": "data-exploration.html#raw-palmer-penguins-data",
    "href": "data-exploration.html#raw-palmer-penguins-data",
    "title": "2  Data Cleaning and Exploration",
    "section": "",
    "text": "studyName: the expedition from which the data were collected (character).\nSample Number: the continuous number sequence for each sample (numeric).\nSpecies: the penguin’s species (character).\nRegion: the region of the Palmer LTER sampling grid the sample was obtained (character).\nIsland: the island on which the penguin was observed (character).\nStage: the reproductive stage of the observation (character).\nIndividual ID: the unique identification number of the observations (character).\nClutch Completion: whether the study nest was observed with a “full clutch” of 2 eggs (character).\nDate Egg: the date that the study nest was observed with 1 egg (Date).\nCulman Length (mm): the length of the dorsal ridge of the penguin’s bill in millimeters (numeric).\nCulmen Depth (mm): the depth of the dorsal ridge of the penguin’s bill in millimeters (numeric).\nFlipper Length (mm): the penguin’s flipper length in millimeters (numeric).\nBody Mass (g): the penguin’s body mass in grams (numeric).\nSex: the penguin’s sex (character).\nDelta 15 N (o/oo): the ratio of stable isotopes 15N:14N (numeric).\nDelta 13 C (o/oo): the ratio of stable isotopes 15C:12C (numeric).\nComments: additional information about the observation (character).",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Data Cleaning and Exploration</span>"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "French, Joshua P. 2023. Api2lm: Functions and Data Sets for the Book\n\"a Progressive Introduction to Linear Models\". https://CRAN.R-project.org/package=api2lm.\n\n\nHorst, Allison, Alison Hill, and Kristen Gorman. 2022.\nPalmerpenguins: Palmer Archipelago (Antarctica) Penguin Data.\nhttps://allisonhorst.github.io/palmerpenguins/.\n\n\nMüller, Kirill, and Hadley Wickham. 2023. Tibble: Simple Data\nFrames. https://tibble.tidyverse.org/.\n\n\nR Core Team. 2024. R: A Language and Environment for Statistical\nComputing. Vienna, Austria: R Foundation for Statistical Computing.\nhttps://www.R-project.org/.\n\n\nSievert, Carson, Chris Parmer, Toby Hocking, Scott Chamberlain, Karthik\nRam, Marianne Corvellec, and Pedro Despouy. 2024. Plotly: Create\nInteractive Web Graphics via Plotly.js. https://plotly-r.com.\n\n\nWickham, Hadley. 2019. Advanced R. CRC press. http://adv-r.had.co.nz/.\n\n\n———. 2023a. Forcats: Tools for Working with Categorical Variables\n(Factors). https://forcats.tidyverse.org/.\n\n\n———. 2023b. Stringr: Simple, Consistent Wrappers for Common String\nOperations. https://stringr.tidyverse.org.\n\n\n———. 2023c. Tidyverse: Easily Install and Load the Tidyverse.\nhttps://tidyverse.tidyverse.org.\n\n\nWickham, Hadley, and Jennifer Bryan. 2023. Readxl: Read Excel\nFiles. https://readxl.tidyverse.org.\n\n\nWickham, Hadley, Winston Chang, Lionel Henry, Thomas Lin Pedersen,\nKohske Takahashi, Claus Wilke, Kara Woo, Hiroaki Yutani, Dewey\nDunnington, and Teun van den Brand. 2024. Ggplot2: Create Elegant\nData Visualisations Using the Grammar of Graphics. https://ggplot2.tidyverse.org.\n\n\nWickham, Hadley, Romain François, Lionel Henry, Kirill Müller, and Davis\nVaughan. 2023. Dplyr: A Grammar of Data Manipulation. https://dplyr.tidyverse.org.\n\n\nWickham, Hadley, and Lionel Henry. 2023. Purrr: Functional\nProgramming Tools. https://purrr.tidyverse.org/.\n\n\nWickham, Hadley, Jim Hester, and Jennifer Bryan. 2024. Readr: Read\nRectangular Text Data. https://readr.tidyverse.org.\n\n\nWickham, Hadley, Davis Vaughan, and Maximilian Girlich. 2024. Tidyr:\nTidy Messy Data. https://tidyr.tidyverse.org.",
    "crumbs": [
      "References"
    ]
  },
  {
    "objectID": "data-exploration.html#initial-data-cleaning",
    "href": "data-exploration.html#initial-data-cleaning",
    "title": "2  Data Cleaning and Exploration",
    "section": "2.2 Initial data cleaning",
    "text": "2.2 Initial data cleaning\nThe str function is a great first function to apply on a newly loaded data set because it provides a general overview of the data’s structure.\n\nstr(penguins_raw, give.attr = FALSE)\n\ntibble [344 × 17] (S3: tbl_df/tbl/data.frame)\n $ studyName          : chr [1:344] \"PAL0708\" \"PAL0708\" \"PAL0708\" \"PAL0708\" ...\n $ Sample Number      : num [1:344] 1 2 3 4 5 6 7 8 9 10 ...\n $ Species            : chr [1:344] \"Adelie Penguin (Pygoscelis adeliae)\" \"Adelie Penguin (Pygoscelis adeliae)\" \"Adelie Penguin (Pygoscelis adeliae)\" \"Adelie Penguin (Pygoscelis adeliae)\" ...\n $ Region             : chr [1:344] \"Anvers\" \"Anvers\" \"Anvers\" \"Anvers\" ...\n $ Island             : chr [1:344] \"Torgersen\" \"Torgersen\" \"Torgersen\" \"Torgersen\" ...\n $ Stage              : chr [1:344] \"Adult, 1 Egg Stage\" \"Adult, 1 Egg Stage\" \"Adult, 1 Egg Stage\" \"Adult, 1 Egg Stage\" ...\n $ Individual ID      : chr [1:344] \"N1A1\" \"N1A2\" \"N2A1\" \"N2A2\" ...\n $ Clutch Completion  : chr [1:344] \"Yes\" \"Yes\" \"Yes\" \"Yes\" ...\n $ Date Egg           : Date[1:344], format: \"2007-11-11\" \"2007-11-11\" ...\n $ Culmen Length (mm) : num [1:344] 39.1 39.5 40.3 NA 36.7 39.3 38.9 39.2 34.1 42 ...\n $ Culmen Depth (mm)  : num [1:344] 18.7 17.4 18 NA 19.3 20.6 17.8 19.6 18.1 20.2 ...\n $ Flipper Length (mm): num [1:344] 181 186 195 NA 193 190 181 195 193 190 ...\n $ Body Mass (g)      : num [1:344] 3750 3800 3250 NA 3450 ...\n $ Sex                : chr [1:344] \"MALE\" \"FEMALE\" \"FEMALE\" NA ...\n $ Delta 15 N (o/oo)  : num [1:344] NA 8.95 8.37 NA 8.77 ...\n $ Delta 13 C (o/oo)  : num [1:344] NA -24.7 -25.3 NA -25.3 ...\n $ Comments           : chr [1:344] \"Not enough blood for isotopes.\" NA NA \"Adult not sampled.\" ...\n\n\nWe see that the penguins_raw object is a tibble, a special kind of data frame provided by the tibble package (Müller and Wickham 2023) as part of the broader tidyverse. It has 344 rows and 17 columns. In general, a tibble will function like a standard data frame, though its behavior may change when tidyverse packages are loaded. In the code below, we confirm that penguins_raw qualifies as a base R data.frame.\n\nis.data.frame(penguins_raw)\n\n[1] TRUE\n\n\nAn alternative to str is the glimpse function from the dplyr package. dplyr::glimpse also summarizes the structure of an object, but also automatically formats the printed output to the size of the Console to make it more readable. An example is provided below.\n\ndplyr::glimpse(penguins_raw)\n\nRows: 344\nColumns: 17\n$ studyName             &lt;chr&gt; \"PAL0708\", \"PAL0708\", \"PAL0708\", \"PAL0708\", \"PAL…\n$ `Sample Number`       &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 1…\n$ Species               &lt;chr&gt; \"Adelie Penguin (Pygoscelis adeliae)\", \"Adelie P…\n$ Region                &lt;chr&gt; \"Anvers\", \"Anvers\", \"Anvers\", \"Anvers\", \"Anvers\"…\n$ Island                &lt;chr&gt; \"Torgersen\", \"Torgersen\", \"Torgersen\", \"Torgerse…\n$ Stage                 &lt;chr&gt; \"Adult, 1 Egg Stage\", \"Adult, 1 Egg Stage\", \"Adu…\n$ `Individual ID`       &lt;chr&gt; \"N1A1\", \"N1A2\", \"N2A1\", \"N2A2\", \"N3A1\", \"N3A2\", …\n$ `Clutch Completion`   &lt;chr&gt; \"Yes\", \"Yes\", \"Yes\", \"Yes\", \"Yes\", \"Yes\", \"No\", …\n$ `Date Egg`            &lt;date&gt; 2007-11-11, 2007-11-11, 2007-11-16, 2007-11-16,…\n$ `Culmen Length (mm)`  &lt;dbl&gt; 39.1, 39.5, 40.3, NA, 36.7, 39.3, 38.9, 39.2, 34…\n$ `Culmen Depth (mm)`   &lt;dbl&gt; 18.7, 17.4, 18.0, NA, 19.3, 20.6, 17.8, 19.6, 18…\n$ `Flipper Length (mm)` &lt;dbl&gt; 181, 186, 195, NA, 193, 190, 181, 195, 193, 190,…\n$ `Body Mass (g)`       &lt;dbl&gt; 3750, 3800, 3250, NA, 3450, 3650, 3625, 4675, 34…\n$ Sex                   &lt;chr&gt; \"MALE\", \"FEMALE\", \"FEMALE\", NA, \"FEMALE\", \"MALE\"…\n$ `Delta 15 N (o/oo)`   &lt;dbl&gt; NA, 8.94956, 8.36821, NA, 8.76651, 8.66496, 9.18…\n$ `Delta 13 C (o/oo)`   &lt;dbl&gt; NA, -24.69454, -25.33302, NA, -25.32426, -25.298…\n$ Comments              &lt;chr&gt; \"Not enough blood for isotopes.\", NA, NA, \"Adult…\n\n\nAnother thing that we notice about penguins_raw is that it has terrible variable names. The variable names have a mixture of lowercase and uppercase letters, parentheses, and even spaces! This makes it complicated to access variables in the data frame. To access the flipper length variable, we would have to use something like the command below. Note the ` ` around Flipper Length (mm) because of the spaces in the variable name.\n\noptions(max.print = 18)\n\n\npenguins_raw$`Flipper Length (mm)`\n\n [1] 181 186 195  NA 193 190 181 195 193 190 186 180 182 191 198 185 195 197\n [ reached getOption(\"max.print\") -- omitted 326 entries ]\n\n\n\noptions(max.print = 99999)\n\nIn The tidyverse style guide (Wickham 2022), Hadley Wickham recommends:\n\nVariable and function names should use only lowercase letters, numbers, and _. Use underscores (_) (so called snake case) to separate words within a name.\n\nWe will apply this recommendation to the penguins_raw data below.\nAdditionally, many variables will be extraneous for our future analyses, so we will select only the ones that we will use in the future. We the subset function to select the Species, Island, Culmen Length (mm), Culmen Depth (mm), Flipper Length (mm), Body Mass (g), and Sex variables of penguins_raw and assign the subsetted data frame the name penguins_clean.\n\npenguins_clean &lt;-\n  penguins_raw |&gt;\n  subset(select = c(\"Species\", \"Island\", \"Culmen Length (mm)\", \"Culmen Depth (mm)\",\n                    \"Flipper Length (mm)\", \"Body Mass (g)\", \"Sex\"))\n\nTo rename the columns of penguins_clean, we use the names function to extract the variable names of the data frame and replace them with a vector containing the desired variable names. A second usage of names confirms that the data frame now has improved column names.\n\n# access variable names and replace with new names\nnames(penguins_clean) &lt;- c(\"species\", \"island\", \"bill_length\", \"bill_depth\",\n                           \"flipper_length\", \"body_mass\", \"sex\")\n# look at new variable names\nnames(penguins_clean)\n\n[1] \"species\"        \"island\"         \"bill_length\"    \"bill_depth\"    \n[5] \"flipper_length\" \"body_mass\"      \"sex\"           \n\n\nThere are still some issues with penguins_clean. Notably, the species, island, and sex variables are categorical but are represented as character vectors. These variables should each be converted to a factor. We use the transform function to convert each variable to a factor. Notice that we must replace the original penguins_clean object with the transformed object using the assignment operator. We then run the str function to confirm the changes.\n\n# convert sex variable to factor, replace original object\npenguins_clean &lt;-\n  penguins_clean |&gt;\n  transform(species = factor(species), island = factor(island), sex = factor(sex))\n# view structure\nstr(penguins_clean)\n\n'data.frame':   344 obs. of  7 variables:\n $ species       : Factor w/ 3 levels \"Adelie Penguin (Pygoscelis adeliae)\",..: 1 1 1 1 1 1 1 1 1 1 ...\n $ island        : Factor w/ 3 levels \"Biscoe\",\"Dream\",..: 3 3 3 3 3 3 3 3 3 3 ...\n $ bill_length   : num  39.1 39.5 40.3 NA 36.7 39.3 38.9 39.2 34.1 42 ...\n $ bill_depth    : num  18.7 17.4 18 NA 19.3 20.6 17.8 19.6 18.1 20.2 ...\n $ flipper_length: num  181 186 195 NA 193 190 181 195 193 190 ...\n $ body_mass     : num  3750 3800 3250 NA 3450 ...\n $ sex           : Factor w/ 2 levels \"FEMALE\",\"MALE\": 2 1 1 NA 1 2 1 2 NA NA ...\n\n\nOur conversion of species, island, and sex to factor variables was successful. However, we notice that the levels of sex are MALE and FEMALE, which is visually unappealing. Also, the levels of species are extremely long, which can create formatting challenges. We simplify both below. First, we confirm the order of the factor levels of the two variables.\n\n# determine levels of species and sex\nlevels(penguins_clean$species)\n\n[1] \"Adelie Penguin (Pygoscelis adeliae)\"      \n[2] \"Chinstrap penguin (Pygoscelis antarctica)\"\n[3] \"Gentoo penguin (Pygoscelis papua)\"        \n\nlevels(penguins_clean$sex)\n\n[1] \"FEMALE\" \"MALE\"  \n\n\nWe now change the levels of each variable in the same order they are printed above and confirm that the changes were successful.\n\n# update factor levels of species and sex\nlevels(penguins_clean$species) &lt;- c(\"adelie\", \"chinstrap\", \"gentoo\")\nlevels(penguins_clean$sex) &lt;- c(\"female\", \"male\")\n# confirm that changes took effect\nstr(penguins_clean)\n\n'data.frame':   344 obs. of  7 variables:\n $ species       : Factor w/ 3 levels \"adelie\",\"chinstrap\",..: 1 1 1 1 1 1 1 1 1 1 ...\n $ island        : Factor w/ 3 levels \"Biscoe\",\"Dream\",..: 3 3 3 3 3 3 3 3 3 3 ...\n $ bill_length   : num  39.1 39.5 40.3 NA 36.7 39.3 38.9 39.2 34.1 42 ...\n $ bill_depth    : num  18.7 17.4 18 NA 19.3 20.6 17.8 19.6 18.1 20.2 ...\n $ flipper_length: num  181 186 195 NA 193 190 181 195 193 190 ...\n $ body_mass     : num  3750 3800 3250 NA 3450 ...\n $ sex           : Factor w/ 2 levels \"female\",\"male\": 2 1 1 NA 1 2 1 2 NA NA ...\n\n\nOur initial data cleaning process is now completed. As we explore our data further, it may become clear that additional data cleaning is needed.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Data Cleaning and Exploration</span>"
    ]
  },
  {
    "objectID": "data-exploration.html#numerical-summarization-of-data",
    "href": "data-exploration.html#numerical-summarization-of-data",
    "title": "2  Data Cleaning and Exploration",
    "section": "2.3 Numerical summarization of data",
    "text": "2.3 Numerical summarization of data\nNumerical exploration of a data set generally consists of computing various relevant statistics for each of the variables in a data set in order to summarize the data. The data type determines which statistics are relevant.\nTable 2.1 provides an overview of common numerical summaries used to explore data, the type of data that can be summarized, the characteristic summarized, and the function used to compute the statistic.\n\n\n\n\nTable 2.1: A summary of statistics frequently used to numerically summarize data, the variable type that can be summarized, the characteristic summarized, and the R function used to compute the statistic.\n\n\n\n\n\n\n\n\n\n\n\n\nnumeric summary\nvariable type\nsummarizes\nR function\n\n\n\n\nmean\nnumeric\ncenter\nmean\n\n\nmedian\nnumeric\ncenter\nmedian\n\n\nvariance\nnumeric\nspread\nvar\n\n\nstandard deviation\nnumeric\nspread\nsd\n\n\ninterquartile range\nnumeric\nspread\nquantile (modified)\n\n\nquantiles\nnumeric\ncenter and spread\nquantile\n\n\ncorrelation\nnumeric\nsimilarity\ncor\n\n\nfrequency distribution\nfactor\ncounts\ntable\n\n\nrelative frequency distribution\nfactor\nproportions\ntable (modified)\n\n\n\n\n\n\n\n\nWe provide additional explanation about the numeric summaries in what follows.\n\n2.3.1 Numeric data\nNumerical exploration of a set of numeric values usually focuses on determining the:\n\ncenter\nspread\nquantiles (less common).\n\nComputing the correlation between two numeric variables can also be useful.\n\n2.3.1.1 Measures of center\nThe sample mean and median are the most common statistics used to represent the “center” of a set of numeric values.\nThe sample mean or average is obtained by adding all values in the sample and dividing by the number of observations. The sample mean is the most commonly used measure of center. A weakness of the sample mean is that it is easily affected by outliers (values that are very large or small compared to the rest of the data values). Formally, if \\(x_1, x_2, \\ldots, x_n\\) are a sample of \\(n\\) numeric values, then the sample mean is computed as \\[\n\\bar{x} = \\frac{1}{n}\\sum_{i=1}^n x_i.\n\\] The mean function computes the sample mean of a set of numeric values.\nThe sample median is the middle value of an ordered set of values (the actual middle value when the number of values is odd and the average of the two middle values when the number of values is even). Alternatively, the median is identical to the 0.5 quantile of the data. The median is considered more “resistant” because it is not so greatly affected by outliers. The median of the values 1, 8, 7, 6, 100 is 7. The median of the values 1, 8, 7, 6, 100, 4 is 6.5. The median function computes the sample median of a set of a numeric values.\nWe compute the sample mean of the body_mass variable of the penguins_clean data in the code below.\n\nmean(penguins_clean$body_mass)\n\n[1] NA\n\n\nWhy is the result NA instead of a number? In general, an NA value “poisons” any calculation it is part of, with the function returning NA. Even a single NA value will cause a calculation to return NA, even if there are thousands or millions of non-NA values. If you want to compute the sample mean of the non-NA values, then you must change the na.rm argument of mean to TRUE, which makes the mean function to temporarily remove NAs prior to calculation. The na.rm argument is provided in many other functions, as we’ll see in subsequent examples. We now compute the sample mean and median of the body_mass variable in penguins_clean, ignoring NA values.\n\n# compute sample mean and median body_mass, ignoring NAs\nmean(penguins_clean$body_mass, na.rm = TRUE)\n\n[1] 4201.754\n\nmedian(penguins_clean$body_mass, na.rm = TRUE)\n\n[1] 4050\n\n\nWe see that the average penguin body_mass is approximately 4,201 grams, while the median penguin body_mass is 4,050 grams. Since the median is less than the mean (i.e., large values are pulling the mean in the positive direction) the data may be positively skewed, but we will need to look at a histogram or density plot of the data to be sure (these plots are discussed in Section 2.4.2.3 and Section 2.4.2.4).\n\n\n2.3.1.2 Quantiles\nInformally, the pth quantile (where \\(0\\leq p \\leq 1\\)) of a set of values is the value that separates the smallest \\(100 p\\)% of the values from the upper \\(100(1-p)\\)% of the values. E.g., the 0.25 sample quantile (often called Q1) of a set of values is the value that separates the smallest 25% of the values from the largest 75% of the values. Similarly, the 0.75 sample quantile (often called Q3) of a set of values is the value that separates the smallest 75% of the values from the largest 25% of the values.\nThe quantile function is used to compute sample quantiles. There are many competing approaches to computing sample quantiles (which we won’t discuss or worry about). Run ?quantile in the Console to learn more about the approaches R can use to compute sample quantiles.\nQuantiles are useful quantifying both the center (median) and spread (minimum and maxmimum or interquartile range) of a set of values.\nWe use the quantile function to compute the minimum (0 quantile), Q1 (0.25 quantile), median (0.5 quantile), Q3 (0.75 quantile), and maximum (1 quantile) of body_mass in the code below. The desired quantiles are provided as a numeric vector to the probs argument.\n\nquantile(penguins_clean$body_mass, probs = c(0, 0.25, 0.5, 0.75, 1), na.rm = TRUE)\n\n  0%  25%  50%  75% 100% \n2700 3550 4050 4750 6300 \n\n\nWe see that the smallest and largest body masses are 2,700 grams and 6,300 grams, so the range of the data is 6,300 - 2,700 = 3,600 grams. Q1 is 3,550 grams, while Q3 is 4,750 grams. Since Q3 and the maximum are further from the median than Q1 and the minimum, respectively, this is additional evidence that this variable may be positively skewed (stretched out in the positive direction), but we must visualize the distribution of this variable to confirm this.\n\n\n2.3.1.3 Measures of spread\nIn addition to identifying the center of a set of values, it is important to measure their spread, i.e., how much the values vary.\nThe sample variance and standard deviation are the most common measures of spread for numeric values. The sample variance of a set of values is the (approximate) average of the squared deviation of each observation from the sample mean. Formally, the equation for the sample variance is \\[\ns^2 = \\frac{1}{n-1}\\sum_{i=1}^n (x_i - \\bar{x})^2.\n\\] The sample standard deviation is the square root of the sample variance and is generally a more useful measure of spread because it is has the same units as the original data. The equation for the sample standard deviation is \\[\ns = \\sqrt{\\frac{1}{n-1}\\sum_{i=1}^n (x_i - \\bar{x})^2}.\n\\] The larger the standard deviation or variance of a set of values, the more the values vary from their sample mean. The sample standard deviation and variance can be greatly affected by outliers. The var function computes the sample variance while sd computes the sample standard deviation.\nThe interquartile range is a more resistant measure of spread based on quantiles. The interquartile range is the difference between the 0.75 and 0.25 quantiles of a data set.\nThe minimum and maximum of a set of values (in relation to their sample mean or median) can also be used to ascertain the spread of a data set. The min and max functions compute the minimum and maximum values of a set of values, respectively.\nWe compute these measures of spread for the body_mass variable below.\n\nvar(penguins_clean$body_mass, na.rm = TRUE) # sample variance\n\n[1] 643131.1\n\nsd(penguins_clean$body_mass, na.rm = TRUE) # sample standard deviation\n\n[1] 801.9545\n\n# interquartile range (names = FALSE removes text above the results)\nquantile(penguins_clean$body_mass, probs = 0.75, na.rm = TRUE, names = FALSE) -\n  quantile(penguins_clean$body_mass, probs = 0.25, na.rm = TRUE, names = FALSE)\n\n[1] 1200\n\nmin(penguins_clean$body_mass, na.rm = TRUE) # minimum\n\n[1] 2700\n\nmax(penguins_clean$body_mass, na.rm = TRUE) # maximum\n\n[1] 6300\n\n\nThe sample variance of body_mass is 643,131.1 grams2, which isn’t easy to interpret. The sample standard deviation is almost 802 grams. So the “typical” deviation of a body_mass value from the sample mean is about 800 grams. The interquartile range is 1,200 grams. The minimum and maximum values of penguin body_mass match what we computed in Section 2.3.1.2.\n\n\n2.3.1.4 Correlation\nThe correlation between two numeric variables quantifies the strength and direction of their linear relationship. The most common correlation statistic is Pearson’s correlation statistic. If \\(x_1, x_2, x_n\\) and \\(y_1, y_2, \\ldots, y_n\\) are two sets of numeric values, then the sample correlation statistic is computed as \\[\nr = \\frac{1}{n-1}\\sum_{i=1}^n\\left(\\frac{x_i - \\bar{x}}{s_x}\\right)\\left(\\frac{y_i - \\bar{y}}{s_y}\\right),\n\\] where \\(\\bar{x}\\) and \\(s_x\\) denote the sample mean and standard deviation of the \\(x\\)’s while \\(\\bar{y}\\) and \\(s_y\\) denote the sample mean and standard deviation of the \\(y\\)’s. \\(r\\) must be between -1 and 1. The cor function computes the sample correlation between two numeric variables.\nThe closer \\(r\\) is to -1 or 1, the closer the data values fall to a straight line when we plot \\((x_i, y_i)\\), \\(i=1,2,\\ldots,n\\) in a scatter plot (discussed in Section 2.4.3.1). Sample correlation values close to 0 indicate that there is no linear relationship between the two variables. Negative \\(r\\) values indicate a negative relationship between the two variables (as values of one variable increase, the values of the other variable tend to decrease). Positive \\(r\\) values indicate a positive relationship between the two variables (as values of one variable increase, the values of the other variable also tend to increase).\nIn the code below, we compute the sample correlation between all numeric variables in penguins_clean. We specify use = \"pairwise.complete.obs\" so that all non-NA pairs of values are used in the calculation. We first determine which variables are numeric using the code below.\n\n# determine whether each variable is numeric\nnum_col &lt;- unlist(lapply(penguins_clean, is.numeric))\n# observe results\nnum_col\n\n       species         island    bill_length     bill_depth flipper_length \n         FALSE          FALSE           TRUE           TRUE           TRUE \n     body_mass            sex \n          TRUE          FALSE \n\n\nWe then compute the sample correlation between all pairs of numeric variables.\n\n# compute correlation of numeric variables\ncor(penguins_clean[, num_col], use = \"pairwise.complete.obs\")\n\n               bill_length bill_depth flipper_length  body_mass\nbill_length      1.0000000 -0.2350529      0.6561813  0.5951098\nbill_depth      -0.2350529  1.0000000     -0.5838512 -0.4719156\nflipper_length   0.6561813 -0.5838512      1.0000000  0.8712018\nbody_mass        0.5951098 -0.4719156      0.8712018  1.0000000\n\n\nCommenting on the output, we see that the values of each variable are perfectly correlated with themselves (this is always true since the values in each pair are identical). The correlation between bill_length and body_mass is 0.87, so heavier penguins tend to have longer bills. Perhaps surprisingly, the correlation between bill_length and bill_depth is -0.24, so penguins with longer bills tend to have shallower (narrower) bills. Similarly, the correlation between bill_depth and body_mass is -0.47, so heavier penguins tend to have narrower bills.\nWe briefly explain why we didn’t simply use the cor function on penguins_clean directly. If we try to use cor on penguins_clean naively, then R will return an error because not all variables in penguins_clean are numeric. To account for this, we create a vector that determines whether a variable in penguins_clean is numeric. Recall that a data.frame object is a specially-structured list object, with each variable being an element of the list. The lapply function applies a function (is.numeric in this case) to each element of the supplied list. In our case, we use this to determine whether each variable is numeric. The result is returned as a list, so we use the unlist function to simplify the list to a vector.\n\n\n\n2.3.2 Categorical data\nThe statistics mentioned in the previous section are generally not appropriate for a categorical variable. Instead, frequency distributions and relative frequency distributions are useful numeric summaries of categorical data.\nA frequency distribution summarizes the number of observations having each level of a categorical variable. The table function produces a frequency distribution (contingency table) summarizing the number of observations for each level of a categorical variable. Note that by default, the table function ignores NA values. We provide an example below.\n\ntable(penguins_clean$sex)\n\n\nfemale   male \n   165    168 \n\n\nWe see that for the sex variable, there are 165 female penguins and 168 male penguins.\nTo count the NA values (if present), we can set the useNA argument of table to \"ifany\", as in the example below.\n\ntable(penguins_clean$sex, useNA = \"ifany\")\n\n\nfemale   male   &lt;NA&gt; \n   165    168     11 \n\n\nWe see that 11 of the observations had no available information on sex.\nA relative frequency distribution summarizes the proportion or percentage of observations with each level of a categorical variable. To compute the relative frequency distribution of a variable, we must divide the frequency distribution by the number of observations. If we want to ignore the NAs, then we can use the following code, which takes the frequency distribution of sex and divides by the number of non-NA sex values.\n\n# divide the frequence distribution of sex by the number of non-NA values\ntable(penguins_clean$sex)/sum(!is.na(penguins_clean$sex))\n\n\n   female      male \n0.4954955 0.5045045 \n\n\nWe see that slightly under 50% of the sex values are female (not accounting for NA values), while slightly more than 50% are male.\nWhat is the command sum(!is.na(penguins_clean$sex)) doing in the code above? The is.na function returns a TRUE for each value that is NA but otherwise returns FALSE. The ! in front of is.na inverts the logical expression so that we are determining whether each value is NOT an NA (and returns TRUE if the value is not NA). It is common in programming to associate TRUE with 1 and FALSE with 0. So, if we sum the values that are not NA, that is equivalent to counting the number of non-NA observations.\nIf we want to include the NA values in our relative frequency distribution, we can use the code below.\n\ntable(penguins_clean$sex, useNA = \"ifany\")/length(penguins_clean$sex)\n\n\n    female       male       &lt;NA&gt; \n0.47965116 0.48837209 0.03197674 \n\n\nWe do not know the sex of approximately 3% of the observed penguins.\n\n\n2.3.3 The summary function\nThe summary function provides a simple approach for quickly quantifying the center and spread of each numeric variable in a data frame or determining the frequency distribution of a factor variable. More specifically, the summary function will compute the minimum, 0.25 quantile, mean, median, 0.75 quantile, and maximum of a numeric variable and will return the frequency distribution of a factor variable. This summary will also count the number of NA values in each variable when they are present.\nA summary method is available for a data.frame object, which means that we can apply the summary function directly to our penguins_clean data frame, which we do below.\n\nsummary(penguins_clean)\n\n      species          island     bill_length      bill_depth   \n adelie   :152   Biscoe   :168   Min.   :32.10   Min.   :13.10  \n chinstrap: 68   Dream    :124   1st Qu.:39.23   1st Qu.:15.60  \n gentoo   :124   Torgersen: 52   Median :44.45   Median :17.30  \n                                 Mean   :43.92   Mean   :17.15  \n                                 3rd Qu.:48.50   3rd Qu.:18.70  \n                                 Max.   :59.60   Max.   :21.50  \n                                 NA's   :2       NA's   :2      \n flipper_length    body_mass        sex     \n Min.   :172.0   Min.   :2700   female:165  \n 1st Qu.:190.0   1st Qu.:3550   male  :168  \n Median :197.0   Median :4050   NA's  : 11  \n Mean   :200.9   Mean   :4202               \n 3rd Qu.:213.0   3rd Qu.:4750               \n Max.   :231.0   Max.   :6300               \n NA's   :2       NA's   :2                  \n\n\nWe conveniently get a numeric summary of all of the variables in our data set (we will see different results for variables that are not factor or numeric type). The summary function makes it easy to identify the presence of any NAs in a variable.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Data Cleaning and Exploration</span>"
    ]
  },
  {
    "objectID": "data-exploration.html#visual-summaries-of-data",
    "href": "data-exploration.html#visual-summaries-of-data",
    "title": "2  Data Cleaning and Exploration",
    "section": "2.4 Visual summaries of data",
    "text": "2.4 Visual summaries of data\nVisual summaries (i.e., plots) of data are vital to understanding our data prior to modeling. They help us spot data errors, unusual observations, and simple patterns. They also help us communicate the results of our analysis.\nWe will introduce basic visualization approaches using base R functions as well as the popular ggplot2 package (Wickham et al. 2024). It is important to know the basic plotting capabilities of base R (particularly the plot function, which has been extended by many packages to provide standard plots for complex objects produced by those packages). However, ggplot2 is able to produce complex graphics with automated legends in a consistent, systematic way, which provides it advantages over base graphics in many contexts.\nTable 2.2 provides an overview of different plots types that can be used to summarize data, the type of data being summarized, whether the plot is for univariate (one variable), bivariate (two variable), or multivariate (3 or more variables) data, the base R functions used to create the plot, and the main ggplot2 functions needed to create the plot. The table is not intended to be an exhaustive list of useful graphics we should use for data exploration.\n\n\n\n\nTable 2.2: A summary of common plot types used to explore data, the type of variable(s) they summarize, the number of variables summarized, and the base R and ggplot2 functions used to create the plot.\n\n\n\n\n\n\n\n\n\n\n\n\n\nplot type\nvariable types\nnumber of variables\nbase R\nggplot2\n\n\n\n\nbox plot\nnumeric\nunivariate\nboxplot\ngeom_boxplot\n\n\nhistogram\nnumeric\nunivariate\nhist\ngeom_histogram\n\n\ndensity plot\nnumeric\nunivariate\nplot, density\ngeom_density\n\n\nbar plot\nfactor\nunivariate\nplot or barplot, table\ngeom_bar\n\n\nscatter plot\n2 numeric\nbivariate\nplot\ngeom_point\n\n\nparallel box plot\n1 numeric, 1 factor\nbivariate\nplot or boxplot\ngeom_boxplot\n\n\ngrouped scatter plot\n2 numeric, 1 factor\nmultivariate\nplot\ngeom_point\n\n\nfacetted plots\nmixed\nmultivariate\nnone\nfacet_wrap or facet_grid\n\n\ninteractive plots\nmixed\nmultivariate\nnone\nplotly::ggplotly\n\n\n\n\n\n\n\n\n\n2.4.1 The ggplot recipe\nThere are 4 main components needed to produce a graphic using ggplot2.\n\nA data frame containing our data.\n\nEach column should be a variable and each row should be an observation of data.\n\nA ggplot object.\n\nThis is initialized using the ggplot function.\n\nA geometric object.\n\nThese are called “geoms” for short.\ngeoms indicate the geometric object used to visualize the data. E.g., points, lines, polygons etc. More generally, geoms indicate the type of plot that is desired, e.g., histogram, density, or box plot, which are complex geometric objects.\n\nAn aesthetic.\n\nAn aesthetic mapping indicates what role a variable plays in the plot.\ne.g., which variable will play the “x” variable in the plot, the “y” variable in the plot, control the “color” of the observations, etc.\n\n\nWe add “layers” of information to a ggplot, such as geoms, scales, or other customizations, using +.\n\n\n2.4.2 Univariate plots\nA univariate plot is a plot that only involves a single variable. Examples include bar plots, box plots, histograms, density plots, dot plots, pie charts, etc. (the last two are are generally poor choices.)\n\n2.4.2.1 Bar plots\nA bar plot (or bar chart) displays the number or proportion of observations in each category of a categorical variable (or using R terminology, each level of a factor variable).\nWhat are we looking for when we create a bar plot? Generally, we are interested in categories that have substantially more or fewer observations than the other categories.\nThe simplest way to create a bar plot in base R is using the plot function on a factor variable. In the code below, we create a bar plot for the island variable of penguins_clean. We use the main argument to add a title to the plot.\n\nplot(penguins_clean$island, main = \"distribution of island\")\n\n\n\n\n\n\n\n\nWe see that there are approximately 170 penguins from Biscoe island, 125 from Dream island, and 50 from Torgersen island.\nAlternatively, we can combine barplot with the table function. We do so below for the sex variable. To account for NAs in the sex variable, we specify useNA = \"ifany\" in the table function. Also, we specify names.arg = ... to specify the bar names, otherwise the bar for NA will be blank.\n\nbarplot(table(penguins_clean$sex, useNA = \"ifany\"), names.arg = c(\"female\", \"male\", \"NA\"))\n\n\n\n\n\n\n\n\nWe see that approximately 48% of the penguins are female, 49% are male, and 3% have an unknown sex.\nTo create a relative frequency bar plot, we divide the results of table by the number of relevant observations. For this particular example, we could use the code below. We use the length function to determine the number of observations to divide the counts by.\n\nbarplot(table(penguins_clean$sex, useNA = \"ifany\") / length(penguins_clean$sex),\n        names.arg = c(\"female\", \"male\", \"NA\"))\n\n\n\n\n\n\n\n\nTo create a bar plot with ggplot2, we first create a basic ggplot object containing our data frame. We must make sure to load the ggplot2 package prior to creating the plot, otherwise we’ll get errors!\n\n# load ggplot2 package\nlibrary(ggplot2)\n# create generic ggplot object with our data frame\ngg_penguin &lt;- ggplot(data = penguins_clean)\n\ngg_penguin is a minimal ggplot object with the raw information needed to produce future graphics. To create a bar plot, we add the geom geom_bar and map the species variable (in this example) to the x aesthetic using the aes function.\n\n# create bar plot for species variable\ngg_penguin + geom_bar(aes(x = species))\n\n\n\n\n\n\n\n\nWe see that there are about 150 Adelie penguins, 70 Chinstrap penguins, and 125 Gentoo penguins.\n\n\n2.4.2.2 Box plots\nA box plot is a simple graphic showing critical quantiles of a numeric variable, as well as outliers. A box plot indicates the median, 0.25 quantile (Q1), and 0.75 quantile (Q3) of the sample data and extends bars to the largest and smallest observations that are not outliers. Outliers are usually marked with stars or dots. The standard definition of an outlier in the context of box plots is any value that is more than Q3 + 1.5 (Q3 - Q1) and less than Q1 - 1.5 (Q3 - Q1). The box of a box plot extends from Q1 to Q3, with a line in the box indicating the median.\nBox plots are useful for identifying outliers and skewness in the variable. However, box plots throw away a lot of information, so we must be cautious in making conclusions about skewness and modality without creating a histogram or density plot of the data.\nThe boxplot function is the easiest approach for producing a box plot using base R. We do so for the body_mass variable below, using the main argument to customize the title of the plot.\n\nboxplot(penguins_clean$body_mass, main = \"distribution of body mass\")\n\n\n\n\n\n\n\n\nThe body_mass variable doesn’t have any outliers. It has perhaps a slight positive skew since the upper tail and upper part of the box are longer than the lower tail and lower part of the box. The median body_mass value is a bit more than 4,000 grams, while 50% of the body_mass values are approximately between 3,500 and 4,750 grams (i.e., between Q1 and Q3). The lightest penguin is less than 3,000 grams, while the heaviest penguin is more than 6,000 grams.\nTo create a box plot using ggplot2, we use geom_boxplot. We create a box plot for the bill_length variable below. We map bill_length to the y aesthetic so that we get a vertically-oriented box plot (mapping it to x will produce a horizontal box plot).\n\ngg_penguin + geom_boxplot(aes(y = bill_length))\n\n\n\n\n\n\n\n\nThere are no bill_length outliers. The minimum bill length is approximately 32 mm and the maximum is almost 60 mm. Q1 is approximately 39 mm, the median is approximately 44 mm, and Q3 is approximately 48 mm. It is difficult to assess the skewness of this data. The upper tail is longer than the shorter tail, but the upper box is shorter than the lower box, so the evidence is inconsistent.\n\n\n2.4.2.3 Histograms\nA histogram displays the distribution of a numeric variable. A histogram counts the number of values falling into (usually) equal-sized “bins” running from the smallest value to the largest value. The number of bins and width of the bins affect the shape of the histogram.\nHistograms are used to assess skewness, modality (the number of clear “peaks” in the plot), and to some extent, outliers.\nThe hist function creates a histogram of a numeric variable. We create a histogram of bill_length in the code below.\n\nhist(penguins_clean$bill_length)\n\n\n\n\n\n\n\n\nThe title and x-axis label are visually unappealing, so we use the main and xlab arguments to change them to a blank title and bill length (mm), respectively. We also increase the number of bins using the breaks argument.\n\nhist(penguins_clean$bill_length, main = \"\", xlab = \"bill length (mm)\", breaks = 20)\n\n\n\n\n\n\n\n\nThis distribution of bill_length is bimodal (has two prominent peaks or modes). That is why the box plot of bill_length provided inconsistent evidence of skewness. This also demonstrates why we should not draw conclusions about modality or skewness from numeric summaries alone.\nWe use geom_histogram to create a histogram using ggplot2, mapping the variable of interest to the x aesthetic. We do so for the flipper_length variable below.\n\ngg_penguin + geom_histogram(aes(x = flipper_length))\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\nWarning: Removed 2 rows containing non-finite outside the scale range\n(`stat_bin()`).\n\n\n\n\n\n\n\n\n\nFlipper length has a bimodal distribution, with prominent peaks centered around 190 and 220 mm.\n\n\n2.4.2.4 Density plots\nA density plot is similar to a smoothed histogram. The area under a density curve must equal 1. In general, density plots are more visually appealing than histograms, but both communicate similar information. However, density plots can sometimes have problems near the edges of a variable with a fixed upper or lower bound because it is difficult to know how to smooth the data in that case.\nWe combine the plot and density functions to construct a density plot using base R. We do that for the bill_depth variable below. Note the use of na.rm to remove NA values that would otherwise poison the density calculation, and use the main argument to create a blank title.\n\nplot(density(penguins_clean$bill_depth, na.rm = TRUE), main = \"\")\n\n\n\n\n\n\n\n\nThe bill_depth variable is bimodal with peaks around 14 mm and 18 mm. The graphic also indicates that 342 observations were used to estimate the density and the bandwidth parameter was 0.5533. The bandwidth parameter controls the amount of smoothing and can be changed. Run ?stats::density in the Console for more details.\nWe create a density plot with ggplot2 using geom_density. We do so for the body_mass variable, mapping it to the x aesthetic.\n\ngg_penguin + geom_density(aes(x = body_mass))\n\n\n\n\n\n\n\n\nThe body_mass variable is unimodal, with a peak around 3,700 grams. It is also positively skewed.\n\n\n\n2.4.3 Bivariate plots\nA bivariate plot is a plot involving two variables. A bivariate plot can involve more than one data type.\n\n2.4.3.1 Scatter plots\nScatter plots can be used to examine the relationship between two numeric variables.\nWe use the plot function to create a scatter plot of bill_length versus body_mass (the y variable versus the x variable) using base R below. The plot function is very flexible and can be used in multiple ways to produce a scatter plot, but we will use the formula method that takes a formula describing the variables (y ~ x) and the data frame from which the variables come.\n\n# xlab and ylab are used to customize the x-axis and y-axis labels\nplot(bill_length ~ body_mass, data = penguins_clean,\n     xlab = \"body mass (g)\", ylab = \"bill length (mm)\")\n\n\n\n\n\n\n\n\nOverall, there is a positive linear relationship between body_mass and bill_length. As body_mass increases, bill_length tends to increase. However, there is a group of points in the upper left part of the scatter plot that don’t fall into the linear pattern quite as well.\nThe geom_point function is used to create a scatter plot with ggplot2. We map the variables to be plotted to the x and y aesthetics. We create a scatter plot of bill_length versus bill_depth using ggplot2 below.\n\ngg_penguin + geom_point(aes(x = bill_depth, y = bill_length))\n\nWarning: Removed 2 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\n\nThe plot of bill_length versus bill_depth doesn’t reveal a clear pattern about their association.\n\n\n2.4.3.2 Parallel box plots\nA parallel box plot displays the distribution of a numeric variable split by the level of a factor variable. Specifically, the values of the numeric variable are grouped based on the associated level of a factor variable, and then a box plot is drawn for each group of numeric values. Parallel box plots are useful for determining if the distribution of a numeric variable substantially changes based on whether an observation has a certain level of a factor.\nWe use the plot function to draw a parallel box plot by associating a numeric variable with a factor variable. Specifically, we use the formula argument of the plot function using the syntax numeric variable ~ factor variable to create a set of vertically-oriented box plots for each level of the factor variable. We can reverse the roles of the variables in the formula to get a horizontal box plots. We create parallel box plots of body_mass versus sex below.\n\nplot(body_mass ~ sex, data = penguins_clean)\n\n\n\n\n\n\n\n\nWe can see that the body_mass values tend to be larger for the male penguins compared to the female penguins.\nWe can produce something similar with ggplot2 by mapping the numeric variable to the y aesthetic and the factor variable to the x aesthetic inside geom_boxplot. We do so below to compare bill_length for the different penguin species.\n\ngg_penguin + geom_boxplot(aes(x = species, y = bill_length))\n\n\n\n\n\n\n\n\nBased on the parallel box plot of bill_length, we see that the Chinstrap penguins tend to have slightly larger bill lengths than the Gentoo penguins, which typically have larger bill lengths than the Adelie penguins.\n\n\n\n2.4.4 Multivariate plots\nA multivariate plot displays relationships between two or more variables (so bivariate plots are technically multivariate plots). We focus on creating multivariate plots with ggplot2. While the same graphics can be created with base R, it is easier to create informative multivariate graphics with ggplot2.\n\n2.4.4.1 Grouped scatter plot\nA grouped scatter plot is a scatter plot that uses colors or symbols (or both) to indicate the level of a factor variable that each point corresponds to. We can actually use more than one factor variable, but interpreting the plot becomes more difficult. The most common way to create a grouped scatter plot with ggplot2 is to map a factor variable to the color or shape aesthetic of geom_point. ggplot2 will automatically map the factor variable to unique colors or shapes and then describe the mapping in a legend (this process is known as “scaling”). In the example below, we create a scatter plot of flipper_length versus body_mass that distinguishes the different species using color.\n\ngg_penguin + geom_point(aes(x = body_mass, y = flipper_length, color = species))\n\n\n\n\n\n\n\n\nThe flipper length and body mass of Gentoo penguins tend to be noticeably larger than the other two species. Chinstrap and Adelie penguins tend to have similar flipper length and body mass, with Chinstrap penguins tending to have slightly longer flipper length.\nWe can improve the graphic above can be made better in two ways. 1. Using better colors, and 2. Using more than one visual approach to distinguish the levels of the factor variable. The grouped scatter plot uses both red and green colors, which may be difficult to distinguish for individuals with certain forms of colorblindness. We should use a more friendly color palette. An excellent resource for choosing a color palette is https://colorbrewer2.org (Brewer (2022)). The Color Brewer website simplifies choosing a color palette based on certain desired characteristics such as whether the palette is colorblind-friendly, printer friendly, etc. The recommend palettes can be accessed using the scale_color_brewer function. We use a colorblind-friendly palette below. We also changes the x-axis label, y-axis label, and title using the xlab, ylab, and ggtitle functions respectively.\n\ngg_penguin +\n  geom_point(aes(x = body_mass, y = flipper_length, color = species, shape = species)) +\n  scale_color_brewer(type = \"qual\", palette = \"Dark2\") +\n  xlab(\"body mass (g)\") + ylab(\"flipper length (mm)\") +\n  ggtitle(\"body mass versus flipper length by species\")\n\n\n\n\n\n\n\n\n\n\n2.4.4.2 Facetted plots (and alternatives)\nFacetting creates separate panels (facets) of plots based on one or more facetting variables. The key functions to do this with ggplot2 are the facet_grid and facet_wrap functions. facet_grid is used to create a grid of plots based on one or two factor variables, while facet_wrap wraps facets of panels around the plot. Below, we facet scatter plots of bill_length versus bill_depth by species.\n\ngg_penguin + geom_point(aes(x = bill_depth, y = bill_length)) + facet_grid(~ species)\n\n\n\n\n\n\n\n\nWhereas we previously couldn’t discern a clear relationship between bill length and depth based on a single scatter plot, facetting by species makes it clear there is a positive relationship between bill_length and bill_depth after accounting for species. We could have used a grouped scatter plot for the same purpose.\nA simpler facetting example would be to facet density plots of body_mass by sex as shown below.\n\ngg_penguin + geom_density(aes(x = body_mass)) + facet_grid(~sex)\n\n\n\n\n\n\n\n\nThis plot is a bit difficult to interpret. We see that body mass is bimodal for the males and females. Perhaps this is related to species. Since the density plots are in different panels, its a bit tricky to see how they relate to each other. Also, the NA panel is probably not needed.\nTo get rid of the NA panel, we need to remove all of the observations with NA values. We do this below, using subset to select the desired columns and then using na.omit to remove any rows that have NA values for body_mass, sex, or species. Note that order matters here because na.omit removes any observation of the data frame that has an NA value. We save the filtered object as penguins_temp.\n\npenguins_temp &lt;-\n  penguins_clean |&gt;\n  subset(select = c(body_mass, sex, species)) |&gt;\n  na.omit()\n\nIn the next graphic, we create density plots of the body_mass variable. However, we use the fill aesthetic to scale the sex variable so that the we distinguish the densities of male and female penguins with different colors. We set the alpha argument to 0.5 OUTSIDE the aes function (because it is being manually specified) so that the colors are translucent and blend. We also facet by species to see what the patterns look like for the different species.\n\nggplot(data = penguins_temp) +\n  geom_density(aes(x = body_mass, fill = sex), alpha = 0.5) +\n  facet_grid(~ species)\n\n\n\n\n\n\n\n\nWe see that for all species, the body mass of the males tends to be larger than the females.\nThe examples above provide a small taste of the complex graphics we can create with ggplot2 using only a few lines of code.\n\n\n2.4.4.3 Interactive graphics\nThere are many tools for creating interactive graphics in R. We have found the ggiraph package (Gohel and Skintzos 2024) useful for creating interactive graphics based on ggplot2. However, it is a bit too complex to discuss here.\nThe plotly package (Sievert et al. 2024) provides the capabilities of plotly (Plotly Technologies Inc. 2015, https://plotly.com/), a well-known tool for creating interactive scientific plots, through R. The ggplotly function will instantly make a ggplot interactive (though we may need to customize it for our needs). We provide two examples below.\nFirst, we load the plotly package to have access to the ggplotly function. We then take our previous grouped scatter plot that plotted flipper_length versus body_mass while distinguishing by species and assign it the name ggi. We then use the ggplotly function to make the graphic interactive. When we hover over a point, the plot interactively provides the exact body_mass value, flipper_length value, and species of the observation.\n\n# load plotly package\nlibrary(plotly)\n# assign grouped scatter plot name\nggi &lt;-\n  gg_penguin +\n  geom_point(aes(x = body_mass, y = flipper_length, color = species, shape = species)) +\n  scale_color_brewer(type = \"qual\", palette = \"Dark2\") +\n  xlab(\"body mass (g)\") + ylab(\"flipper length (mm)\") +\n  ggtitle(\"body mass versus flipper length by species\")\n# make plot interactive\nggplotly(ggi)\n\n\n\n\n\nIn the next example, we make interactive parallel box plots of bill_length that distinguish between species.\n\n# assign parallel box plot name\nggi2 &lt;- gg_penguin + geom_boxplot(aes(x = species, y = bill_length))\n# make plot interactive\nggplotly(ggi2)\n\n\n\n\n\nThe interactive parallel box plot provides information about the box plot characteristics of each species (such as the minimum bill_length, Q1, median, Q3, etc.)",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Data Cleaning and Exploration</span>"
    ]
  },
  {
    "objectID": "data-exploration.html#final-notes-on-missing-or-erroneous-data",
    "href": "data-exploration.html#final-notes-on-missing-or-erroneous-data",
    "title": "2  Data Cleaning and Exploration",
    "section": "2.6 Final notes on missing or erroneous data",
    "text": "2.6 Final notes on missing or erroneous data\nWhat should we do with our data when observations are missing information or the information is clearly erroneous?\nIf the data are clearly erroneous, attempt to get the correct value. If the values cannot be corrected, replace them with NA since we don’t have that information.\nWhat should we do about NAs? There are many approaches for dealing with NAs. The proper approach depends a lot on why the data are missing. Speaking informally, if there is no systematic reason causing the data to be missing, then ignoring the observations with missing data isn’t a terrible approach. However, if there is a systematic reason behind why the data are missing (such as individuals not wanting to answer a sensitive question, subjects dying for a specific reason, etc.) then ignoring that data can lead to erroneous conclusions.\nIn what follows, we will assume our missing data problem is not systematic and ignore missing values.\n\n\n\n\nBrewer, Cynthia A. 2022. “ColorBrewer2.org.” https://colorbrewer2.org.\n\n\nGohel, David, and Panagiotis Skintzos. 2024. Ggiraph: Make Ggplot2 Graphics Interactive. https://davidgohel.github.io/ggiraph/.\n\n\nGorman, Kristen B., Tony D. Williams, and William R. Fraser. 2014. “Ecological Sexual Dimorphism and Environmental Variability Within a Community of Antarctic Penguins (Genus Pygoscelis).” PLOS ONE 9 (3): 1–14. https://doi.org/10.1371/journal.pone.0090081.\n\n\nHorst, Allison, Alison Hill, and Kristen Gorman. 2022. Palmerpenguins: Palmer Archipelago (Antarctica) Penguin Data. https://allisonhorst.github.io/palmerpenguins/.\n\n\nMüller, Kirill, and Hadley Wickham. 2023. Tibble: Simple Data Frames. https://tibble.tidyverse.org/.\n\n\nPlotly Technologies Inc. 2015. “Collaborative Data Science.” Montreal, QC: Plotly Technologies Inc. 2015. https://plot.ly.\n\n\nSievert, Carson, Chris Parmer, Toby Hocking, Scott Chamberlain, Karthik Ram, Marianne Corvellec, and Pedro Despouy. 2024. Plotly: Create Interactive Web Graphics via Plotly.js. https://plotly-r.com.\n\n\nWickham, Hadley. 2022. The Tidyverse Style Guide. https://style.tidyverse.org/.\n\n\nWickham, Hadley, Winston Chang, Lionel Henry, Thomas Lin Pedersen, Kohske Takahashi, Claus Wilke, Kara Woo, Hiroaki Yutani, Dewey Dunnington, and Teun van den Brand. 2024. Ggplot2: Create Elegant Data Visualisations Using the Grammar of Graphics. https://ggplot2.tidyverse.org.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Data Cleaning and Exploration</span>"
    ]
  },
  {
    "objectID": "data-exploration.html#a-plan-for-data-cleaning-and-exploration",
    "href": "data-exploration.html#a-plan-for-data-cleaning-and-exploration",
    "title": "2  Data Cleaning and Exploration",
    "section": "2.5 A plan for data cleaning and exploration",
    "text": "2.5 A plan for data cleaning and exploration\nWe have provided many examples of data cleaning and exploration using the penguins_raw data. The analysis above is NOT exhaustive, and there are many additional numeric and visual summaries we could consider. In what follows, we summarize a basic plan below for initial data cleaning and exploration that we have found useful. We assume that we are working with a data frame, which is the most common data structure used for data analysis in R.\n\nImport or create the data set.\nUse the str function to get an idea of the initial structure. This can help to identify clear issues we may have had in importing the data, problems with variable names and types, etc.\nClean the variable names based on our preferences.\nConvert the variables to the appropriate type (e.g., categorical variables to factor).\nRun the summary function on the data frame. Take note of NAs, impossible values that are data entry errors, etc. We may need to perform additional cleaning based on the information learned in this step.\nCompute any additional numeric summaries of the different variables, as desired.\nCreate univariate plots of all variables being considered. Use histograms for discrete numeric variables, density plots for continuous numeric variables, and bar plots for factor variables. Take note of any interesting patterns such as modality, skewness, overall shape, outliers, etc.\nCreate bivariate plots for any pairs of variables. Use scatter plots for two numeric variables. Use parallel box plots for numeric and factor variables, or perhaps create histogram plots of the numeric variable facetted by the factor variable, or density plots of the numeric variables filled with different colors by the factor variable. Once again, notice any patterns.\nCreate multivariate and interactive graphics based on what we learn in the previous steps.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Data Cleaning and Exploration</span>"
    ]
  }
]