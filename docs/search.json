[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "A Progressive Introduction to Linear Models",
    "section": "",
    "text": "Preliminaries",
    "crumbs": [
      "Preliminaries"
    ]
  },
  {
    "objectID": "index.html#background",
    "href": "index.html#background",
    "title": "A Progressive Introduction to Linear Models",
    "section": "Background",
    "text": "Background\nI designed this book to progressively introduce you to the analysis of data using linear models. My goal is to provide you with the skills needed to perform a linear regression analysis sooner rather than later. Most of the detailed derivations have been placed in Going Deeper sections or in their own chapter, which can be skipped over to more quickly progress through the material at the expense of less exposure to theory.",
    "crumbs": [
      "Preliminaries"
    ]
  },
  {
    "objectID": "index.html#acknowledgments",
    "href": "index.html#acknowledgments",
    "title": "A Progressive Introduction to Linear Models",
    "section": "Acknowledgments",
    "text": "Acknowledgments\nThe writing of the book was partially supported by the Colorado Department of Higher Education as part of the proposal “OER for the Creation of Interactive Computational Notebooks and a Computational Pathway in Mathematics and Statistics”.\nThe computational examples utilize the R programming language and environment (R Core Team 2024).\nWe will also make use of the following packages:\n\napi2lm (French 2023).\ndplyr (Wickham et al. 2023).\nggplot2 (Wickham et al. 2024)\nknitr (Xie 2024)\npalmerpenguins (Horst, Hill, and Gorman 2022)\nplotly (Sievert et al. 2024)\ntidyverse (Wickham 2023).",
    "crumbs": [
      "Preliminaries"
    ]
  },
  {
    "objectID": "index.html#creative-commons-license-information",
    "href": "index.html#creative-commons-license-information",
    "title": "A Progressive Introduction to Linear Models",
    "section": "Creative Commons License Information",
    "text": "Creative Commons License Information\n A Progressive Introduction to Linear Models by Joshua French is licensed under a Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License.\n\n\n\n\nFrench, Joshua P. 2023. Api2lm: Functions and Data Sets for the Book \"a Progressive Introduction to Linear Models\". https://CRAN.R-project.org/package=api2lm.\n\n\nHorst, Allison, Alison Hill, and Kristen Gorman. 2022. Palmerpenguins: Palmer Archipelago (Antarctica) Penguin Data. https://allisonhorst.github.io/palmerpenguins/.\n\n\nR Core Team. 2024. R: A Language and Environment for Statistical Computing. Vienna, Austria: R Foundation for Statistical Computing. https://www.R-project.org/.\n\n\nSievert, Carson, Chris Parmer, Toby Hocking, Scott Chamberlain, Karthik Ram, Marianne Corvellec, and Pedro Despouy. 2024. Plotly: Create Interactive Web Graphics via Plotly.js. https://plotly-r.com.\n\n\nWickham, Hadley. 2023. Tidyverse: Easily Install and Load the Tidyverse. https://tidyverse.tidyverse.org.\n\n\nWickham, Hadley, Winston Chang, Lionel Henry, Thomas Lin Pedersen, Kohske Takahashi, Claus Wilke, Kara Woo, Hiroaki Yutani, Dewey Dunnington, and Teun van den Brand. 2024. Ggplot2: Create Elegant Data Visualisations Using the Grammar of Graphics. https://ggplot2.tidyverse.org.\n\n\nWickham, Hadley, Romain François, Lionel Henry, Kirill Müller, and Davis Vaughan. 2023. Dplyr: A Grammar of Data Manipulation. https://dplyr.tidyverse.org.\n\n\nXie, Yihui. 2024. Knitr: A General-Purpose Package for Dynamic Report Generation in r. https://yihui.org/knitr/.",
    "crumbs": [
      "Preliminaries"
    ]
  },
  {
    "objectID": "r-foundations.html",
    "href": "r-foundations.html",
    "title": "1  R Foundations",
    "section": "",
    "text": "1.1 Setting up R and RStudio Desktop",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>R Foundations</span>"
    ]
  },
  {
    "objectID": "r-foundations.html#setting-up-r-and-rstudio-desktop",
    "href": "r-foundations.html#setting-up-r-and-rstudio-desktop",
    "title": "1  R Foundations",
    "section": "",
    "text": "1.1.1 What is R?\nR is a programming language and environment designed for statistical computing. It was introduced by Robert Gentleman and Robert Ihaka in 1993 as a free implementation of the S programming language developed at Bell Laboratories (https://www.r-project.org/about.html)\nSome important facts about R are that:\n\nR is free, open source, and runs on many different types of computers (Windows, Mac, Linux, and others).\nR is an interactive programming language.\n\nWe can type and run a command in the Console for immediate feedback, in contrast to a compiled programming language, which compiles a program that is then executed.\n\nR is highly extendable.\n\nWe can extend the functionality of R beyond what is available by default by installing user-created packages.\nWe can write our own functions to add additional capabilities to R.\n\n\n\n\n1.1.2 Installing R\nTo install R on our personal computer, we will need to download an installer program from the R Project’s website (https://www.r-project.org/). Links to download the installer program for a specific operating system are found at https://cloud.r-project.org/.\nWe should click on the download link appropriate for our computer’s operating system and then go through the process of installing R. A stable link for the most current installer program for the Windows operating system is available at https://cloud.r-project.org/bin/windows/base/release.html. (Similar links are not currently available for Mac and Linux computers.)\n\n\n1.1.3 Installing RStudio\nRStudio Desktop is a free “front end” for R provided by Posit Software (https://posit.co/). RStudio Desktop makes doing data analysis with R much easier by adding an Integrated Development Environment (IDE) and providing many other features. Currently, we can download RStudio at https://posit.co/download/rstudio-desktop/. We should download the Free version of RStudio Desktop appropriate for our computer and install it.\nHaving installed both R and RStudio Desktop, we will want to open RStudio Desktop as we continue to learn about R.\n\n\n1.1.4 RStudio Layout\nRStudio Desktop has four panes:\n\nConsole: the pane where commands are run.\nSource: the pane where we prepare commands to run.\nEnvironment/History: the pane where we can see all the objects in our workspace, our command history, and other information.\nFiles/Plot/Packages/Help: the pane where we navigate between directories, view plots, see the packages available to be loaded, or get help.\n\nTo see all RStudio panes, press the keys Ctrl + Alt + Shift + 0 on a PC or Cmd + Option + Shift + 0 on a Mac.\nFigure 1.1 displays a labeled graphic of the panes. The position of the panes can change depending on how we set the display preferences, but the look of each pane will be similar.\n\n\n\n\n\n\n\n\nFigure 1.1: The RStudio panes labeled for convenience.\n\n\n\n\n\n\n\n1.1.5 Customizing the RStudio workspace\nAt this point, we should customize our workspace to prevent us from experiencing future frustration. R provides a “feature” that allows us to “save a workspace”. This allows us to easily pick up our analysis where we last left off. Unfortunately, over time, we will accumulate many environmental artifacts that can conflict with each other. This can lead to errors and incorrect results that we will need to deal with. Additionally, this “feature” hinders the ability of others to reproduce our analysis because other users will not have the same workspace.\nTo turn off this feature, in the RStudio menu bar, we click Tools → Global Options and then make sure the “General” option is selected. Then we make the following changes (if necessary):\n\nUncheck the box for “Restore .RData into workspace at startup”.\nChange the toggle box for “Save workspace to .RData on exit” to “Never”.\nClick “Apply” then “OK” to save the changes.\n\nFigure 1.2 displays what these options should look like.\n\n\n\n\n\n\n\n\nFigure 1.2: The General options window.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>R Foundations</span>"
    ]
  },
  {
    "objectID": "r-foundations.html#running-code-scripts-and-comments",
    "href": "r-foundations.html#running-code-scripts-and-comments",
    "title": "1  R Foundations",
    "section": "1.2 Running code, scripts, and comments",
    "text": "1.2 Running code, scripts, and comments\nWe can run code in R by typing it in the Console next to the &gt; symbol and pressing the Enter key.\nIf we need to successively run multiple commands, it’s better to write our commands in a “script” file and then save the file. The commands in a Script file are often generically referred to as “code”.\nScript files make it easy to:\n\nReproduce our data analysis without retyping all our commands.\nShare our code with others.\n\nA new Script file can be obtained by:\n\nClicking File → New File → R Script in the RStudio menu bar.\nPressing Ctrl + Shift + n on a PC or Cmd + Shift + n on a Mac.\n\nThere are various ways to run code from a Script file. The most common ones are:\n\nHighlight the code we want to run and click the “Run” button at the top of the Script pane. Figure 1.3 displays the Run button.\nHighlight the code we want to run and press Ctrl + Enter on our keyboard. If we don’t highlight anything, by default, RStudio runs the command the cursor currently lies on.\n\n\n\n\n\n\n\n\n\nFigure 1.3: The Run icon can be clicked to run a selection of commands.\n\n\n\n\n\nTo save a Script file:\n\nClick File → Save in the RStudio menu bar.\nPress Ctrl + s on a PC or Cmd + s on a Mac.\n\nA comment is a set of text ignored by R when submitted to the Console. The # symbol indicates the start of a comment. Nothing to the right of the # is executed by the Console. We can comment (or uncomment) multiple lines of code in the Source pane of RStudio by highlighting the code we want to comment and pressing Ctrl + Shift + c on a PC or Cmd + Shift + c on a Mac.\n\n\nHands-on Practice\nPerform the following tasks:\n\nType 1+1 in the Console and press the Enter key.\nOpen a new Script in RStudio.\nType mean(1:3) in the Script file.\nType # mean(1:3) in the Script file.\nRun the commands from the Script using an approach mentioned above.\nSave the Script file.\nUse the keyboard shortcut to “comment out” some of the lines of the Script file.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>R Foundations</span>"
    ]
  },
  {
    "objectID": "r-foundations.html#sec-assignment",
    "href": "r-foundations.html#sec-assignment",
    "title": "1  R Foundations",
    "section": "1.3 Assignment",
    "text": "1.3 Assignment\nR works on various types of objects that we’ll learn more about later.\nTo store an object in the computer’s memory, we must assign it a name using the assignment operator (&lt;-) or the equal sign (=).\nSome comments:\n\nIn general, both &lt;- and = can be used for assignment.\nPressing Alt + - on a PC or Option + - on a Mac will insert &lt;- into the R Console and Script files.\n\nIf we are creating an R Markdown (rmd) or Quarto Markdown (qmd) file, then this shortcut will only insert &lt;- if we are in an R code block.\n\n&lt;- and = are NOT synonyms, but can be used identically most of the time.\n\nIt is best to use &lt;- for assigning a name to an object and reserving = for specifying function arguments. See Section Section 1.14.1 for a deeper explanation.\nOnce an object has been assigned a name, it can be printed by running the name of the object in the Console or using the print function.\n\n\nHands-on Practice\nRun the following commands in the Console:\n\nm &lt;- mean(1:10)\n\nIn the code above, we compute the sample mean of the values \\(1, 2, \\ldots, 10\\), then assign it the name m. However, nothing is printed by the R Console.\n\nm\n\nWhen we type the name of an object in the R Console and run the command, R will print at least some of the information in the object. In this case, the value stored in m, the value 5.5, will be returned.\n\nprint(m)\n\nThe command above is a formal way of printing an object in R and can be especially useful when an object has a defined print method with additional arguments. We will discuss this later when it is relevant.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>R Foundations</span>"
    ]
  },
  {
    "objectID": "r-foundations.html#functions",
    "href": "r-foundations.html#functions",
    "title": "1  R Foundations",
    "section": "1.4 Functions",
    "text": "1.4 Functions\nA function is an object that performs a certain action or set of actions based on the objects it receives from its arguments. We use a sequence of function calls to perform data analysis.\nWe use a function by typing the function’s name in the Console (or Script), supplying the function’s arguments between parentheses, (), and then pressing the Enter key.\nThe arguments of a function are pieces of data or information that the function needs to perform the requested task (i.e., the function “inputs”). Each argument we supply is separated by a comma, ,. Some functions have default values for certain arguments and do not need to specified unless something besides the default behavior is desired.\nThe mean function computes the sample mean of an R object x. (We can look at the documentation for the function by running ?mean in the Console. We’ll talk more about getting help with R shortly.) The mean function also has a trim argument that indicates the, “… fraction … of observations to be trimmed from each end of x before the mean is computed” (R Core Team 2024, ?mean).\nConsider the command below, in which we compute the mean of the set of values 1, 5, 3, 2, 10.\n\nmean(c(1, 5, 3, 4, 10))\n\n[1] 4.6\n\n\nIn the next command, we compute a trimmed mean of the numeric vector. Since the trim argument is 0.2, we “trim” the smallest 20% of values and the largest 20% of values from the numeric vector prior to computing the mean.\n\nmean(c(1, 5, 3, 4, 10), trim = 0.2)\n\n[1] 4\n\n\nIn the first function call, we compute (1 + 5 + 3 + 4 + 10)/5 = 23/5 = 4.6. In the second function call, we remove the smallest 20% and largest 20% of the values (i.e., dropping 1 and 10) and compute (5 + 3 + 4)/3 = 12/3 = 4.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>R Foundations</span>"
    ]
  },
  {
    "objectID": "r-foundations.html#packages",
    "href": "r-foundations.html#packages",
    "title": "1  R Foundations",
    "section": "1.5 Packages",
    "text": "1.5 Packages\nPackages are collections of functions, data, and other objects that extend the functionality available in R by default.\nR packages can be installed using the install.packages function and loaded using the library function.\n\n\nHands-on Practice\nThe tidyverse (Wickham 2023c) is a popular ecosystem of R packages used for manipulating, tidying, and plotting data. Currently, the tidyverse is comprised of the following packages:\n\nggplot2: A package for plotting based on the “Grammar of Graphics” (Wickham et al. 2024).\npurrr: A package for functional programming (Wickham and Henry 2023).\ntibble: A package providing a more advanced data frame (Müller and Wickham 2023).\ndplyr: A package for manipulating data (Wickham et al. 2023). More specifically, it provides ” a grammar of data manipulation”.\ntidyr: A package to help create “tidy” data (Wickham, Vaughan, and Girlich 2024). Tidy data is a data organization style often convenient for data analysis.\nstringr: A package for working with character/string data (Wickham 2023b).\nreadr: A package for importing data (Wickham, Hester, and Bryan 2024).\nforcats: A package for working with categorical data (Wickham 2023a).\n\nInstall the set of tidyverse R packages by running the command below in the Console.\n\ninstall.packages(\"tidyverse\")\n\nAfter we install the tidyverse, we load the collection of packages by running the command below.\n\nlibrary(tidyverse)\n\nWe should see something like the output below.\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n\nWhen we loaded the tidyverse package above, notice that dplyr::lag() masks stats::lag(). “Masking” means that functions or objects from different packages have the same name. In this case, both the dplyr and stats packages have a function called lag. Different packages may use the same function name to provide certain functionality. The functions will likely be used for different tasks or require different arguments.\nTo refer to a function in a specific package, we should add the syntax package:: prior to the function name. In the code below, we run stats::lag and dplyr::lag on two different objects using the :: syntax.\n\n# run stats::lag on a numeric vector\nstats::lag(1:10, 2)\n\n [1]  1  2  3  4  5  6  7  8  9 10\nattr(,\"tsp\")\n[1] -1  8  1\n\n\n\n# run dplyr::lag on a numeric vector\ndplyr::lag(1:10, 2)\n\n [1] NA NA  1  2  3  4  5  6  7  8\n\n\nThe output returned by the two functions is different because the functions are intended to do different things. The stats::lag function call shifts the time base of the provided time series object back 2 units, while the call to dplyr::lag provides the values 2 positions earlier in the object.\nNote: this example is not intended to help us understand the lag function. This example highlights how to use the :: syntax to call a function in a specific package when the function name is used in multiple packages.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>R Foundations</span>"
    ]
  },
  {
    "objectID": "r-foundations.html#getting-help",
    "href": "r-foundations.html#getting-help",
    "title": "1  R Foundations",
    "section": "1.6 Getting help",
    "text": "1.6 Getting help\nThere are many ways to get help in R.\nIf we know the command for which we want help, then we run ?command in the Console to access the documentation for the object (where command is replaced by the name of the relevant command). This approach will also work with data sets, package names, object classes, etc. If we need to refer to a function in a specific package, we can use ?package::function to get help on a specific function, e.g., ?dplyr::filter.\nThe documentation will provide:\n\nA Description section with general information about the function or object.\nA Usage section with a generic template for using the function or object.\nAn Arguments section summarizing the inputs the function needs.\nA Details section may be provided with additional information about the function or object.\nA Value section that describes what is returned by the function.\nAn Examples section providing examples of how to use the function. Usually, these can be copied and pasted into the Console to better understand the function arguments and what it produces.\n\nIf we need to find a command related to a certain topic, then ??topic will search for the topic through all installed documentation and bring up any vignettes, code demonstrations, or help pages that include the topic for which we searched.\nIf we are trying to figure out why an error is being produced, what packages can be used to perform a certain analysis, how to perform a complex task that we can’t seem to figure out, etc., then we can simply do a web search for what we’re trying to figure out! Because R is such a popular programming language, it is likely we will find a stackoverflow response, a helpful blog post, an R users forum response, etc., that addresses our question.\nLastly, we can query artificial intelligence chatbots with questions about how to use R to perform a certain analysis or how to fix bugs in our code.\n\n\nHands-on Practice\nDo the following:\n\nRun?lm in the Console to get help on the lm function, which is one of the main functions used for fitting linear models.\nRun ??logarithms in the Console to search the R documentation for information about logarithms. It is likely that we will see multiple help pages that mention “logarithm”, so we may end up needing to find the desired entry via trial and error.\nRun a web search for something along the lines of “How do I change the size of the axis labels in an R plot?”.\nQuery an artificial intelligence chatbot regarding how to create a scatter plot using base R. Were the results useful and correct?",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>R Foundations</span>"
    ]
  },
  {
    "objectID": "r-foundations.html#data-types-and-structures",
    "href": "r-foundations.html#data-types-and-structures",
    "title": "1  R Foundations",
    "section": "1.7 Data types and structures",
    "text": "1.7 Data types and structures\n\n1.7.1 Basic data types\nR has 6 basic (“atomic”) vector types (https://cran.r-project.org/doc/manuals/r-release/R-lang.html#Basic-types) (R Core Team 2024):\n\ncharacter: collections of characters. E.g., \"a\", \"hello world!\".\ndouble: decimal numbers. e.g., 1.2, 1.0.\ninteger: positive and negative whole numbers. In R, we must add L to the end of a number to specify it as an integer. E.g., 1L is an integer but 1 is a double.\nlogical: Boolean values, TRUE and FALSE.\ncomplex: complex numbers. E.g., 1+3i.\nraw: a type to hold raw bytes.\n\nBoth double and integer values are specific types of numeric values.\nThe typeof function returns the R internal type or storage mode of any object.\nConsider the output of the commands below. How does the output differ?\n\ntypeof(1)\n\n[1] \"double\"\n\n\n\ntypeof(1L)\n\n[1] \"integer\"\n\n\n\ntypeof(\"hello world!\")\n\n[1] \"character\"\n\n\n\n\n1.7.2 Other important object types\nThere are other important types of objects in R that are not basic. We will discuss a few. The R Project manual provides additional information about available types (https://cran.r-project.org/doc/manuals/r-release/R-lang.html#Basic-types).\n\n1.7.2.1 Numeric\nAn object is numeric if it is of type integer or double. In that case, it’s mode is said to be numeric.\nThe is.numeric function tests whether an object can be interpreted as a set of numbers. In the code below, we use the is.numeric function to query whether certain objects are numeric.\n\nis.numeric(\"1\")\n\n[1] FALSE\n\n\n\nis.numeric(1)\n\n[1] TRUE\n\n\n\nis.numeric(1L)\n\n[1] TRUE\n\n\n\n\n1.7.2.2 NULL\nNULL is a special object to indicate an object is absent. An object having a length of zero is not the same thing as an object being absent.\n\n\n1.7.2.3 NA\nA “missing value” occurs when the value of an object isn’t known. R uses the special object NA to represent a missing value.\nIf we have a missing value, we should represent that value as NA. \"NA\" is not the same object as NA.\n\n\n1.7.2.4 Functions\nFrom R’s perspective, a function is simply another data type.\n\n\n1.7.2.5 A comment about classes\nEvery R object has a class that may be distinct from its type. Many functions will operate differently depending on an object’s class.\n\n\n\n1.7.3 Data structures\nR operates on data structures. A data structure is a “container” that holds certain kinds of information.\nR has 5 basic data structures:\n\nvector.\nmatrix.\narray.\ndata frame.\nlist.\n\nVectors, matrices, and arrays are homogeneous objects that can only store a single data type at a time. Data frames and lists can store multiple data types.\nVectors and lists are considered one-dimensional objects. A list is technically a vector. Vectors of a single type are atomic vectors (https://cran.r-project.org/doc/manuals/r-release/R-lang.html#List-objects). Matrices and data frames are considered two-dimensional objects. Arrays can have one or more dimensions.\nTable 1.1 summarizes the relationship between dimensionality and data type for the basic data structures. It is based on a table in the first edition of Hadley Wickham’s Advanced R (Wickham 2019).\n\n\n\n\nTable 1.1: A table summarizing the relationship between dimensionality and data type homogeneity for the 5 basic data structures.\n\n\n\n\n\n\nnumber of dimensions\nhomogeneous data type\nheterogeneous data types\n\n\n\n\n1\natomic vector\nlist\n\n\n2\nmatrix\ndata frame\n\n\n1 or more\narray",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>R Foundations</span>"
    ]
  },
  {
    "objectID": "r-foundations.html#vectors",
    "href": "r-foundations.html#vectors",
    "title": "1  R Foundations",
    "section": "1.8 Vectors",
    "text": "1.8 Vectors\nA vector is a one-dimensional set of data of the same type.\n\n1.8.1 Direct creation\nThe most basic way to create a vector is the c (combine) function. The c function combines values into an atomic vector or list.\nThe following commands create vectors of type numeric, character, and logical, respectively.\n\nc(1, 2, 5.3, 6, -2, 4)\nc(\"one\", \"two\", \"three\")\nc(TRUE, TRUE, FALSE, TRUE)\n\n\n\n\n1.8.2 Hands-on Practice\nRun the commands below in the Console.\n\nc(1, 2, 5.3, 6, -2, 4)\n\n\nc(\"one\", \"two\", \"three\")\n\n\nc(TRUE, TRUE, FALSE, TRUE)\n\n\nR provides two main functions for creating vectors with specific patterns: seq and rep.\n\n\n1.8.3 The seq function\nThe seq (sequence) function is used to create an equidistant series of numeric values. We provide some examples of using the seq function below:\n\nseq(1, 10) creates a sequence of numbers from 1 to 10 in increments of 1.\n1:10 creates a sequence of numbers from 1 to 10 in increments of 1.\nseq(1, 20, by = 2) creates a sequence of numbers from 1 to 20 in increments of 2.\nseq(10, 20, len = 10) creates a sequence of numbers from 10 to 20 of length 10.\n\n\n\n\nHands-on Practice\nRun the commands below in the Console and try to answer the questions below.\n\nWhat does the by argument of the seq function control?\nWhat does the len argument of the seq function control?\n\n\nseq(1, 10)\n\n\n1:10\n\n\nseq(1, 20, by = 2)\n\n\nseq(10, 20, len = 10)\n\n\n\n\n1.8.4 The rep function\nThe rep (replicate) function can be used to create a vector by replicating values. We provide some examples of using the rep function below:\n\nrep(1:3, times = 3) replicates the sequence 1, 2, 3 three times in a row.\nrep(c(\"trt1\", \"trt2\", \"trt3\"), times = 1:3) replicates \"trt1\" once, \"trt2\" twice, and \"trt3\" three times.\nrep(1:3, each = 3) replicates each element of the sequence 1, 2, 3 three times.\n\n\n\n\nHands-on Practice\nRun the commands below in the Console and try to answer the questions below.\n\nWhat does the times argument of the rep function control?\nWhat does the each argument of the rep function control?\n\n\nrep(1:3, times = 3)\n\n\nrep(c(\"trt1\", \"trt2\", \"trt3\"), times = 1:3)\n\n\nrep(1:3, each = 3)\n\n\n\n\n1.8.5 Combining vectors\nMultiple vectors of the same type can be combined into a new vector object using the c function. E.g., c(v1, v2, v3) will combine vectors v1, v2, and v3.\n\n\n\nHands-on Practice\nRun the commands below in the Console. Determine what action each command performs.\n\nv1 &lt;- 1:5\n\n\nv2 &lt;- c(1, 10, 11)\n\n\nv3 &lt;- rep(1:2, each = 3)\n\n\nnew &lt;- c(v1, v2, v3)\n\n\nnew\n\n\n\n\n1.8.6 Categorical vectors\nCategorical data should be stored as a factor in R. Sometimes, we might naively store categorical data as character or numeric objects for simplicity. In that situation, our code might still work because a cautious developer planned for our laziness, but it is best to use good coding practices that minimize potential issues. For that reason, represent categorical data as a factor.\n\n1.8.6.1 Creating a factor object\nThe factor function takes a vector of values that can be coerced to type character and converts them to an object of class factor. In the code chunks below, we create two factor objects from vectors.\n\nf1 &lt;- factor(rep(1:6, times = 3))\nf1\n\n [1] 1 2 3 4 5 6 1 2 3 4 5 6 1 2 3 4 5 6\nLevels: 1 2 3 4 5 6\n\n\n\nf2 &lt;- factor(c(\"a\", 7, \"blue\", \"blue\", FALSE))\nf2\n\n[1] a     7     blue  blue  FALSE\nLevels: 7 a blue FALSE\n\n\nA printed factor object lists the Levels (i.e., unique categories) of the object.\nThe is.factor function can be used to determine whether an object is a factor.\nfactor objects aren’t technically vectors (e.g., running is.factor(f2) based on the above code will return FALSE) though they essentially behave like vectors, which is why discuss them here.\n\n\n\nHands-on Practice\nComplete the following tasks:\n\nCreate a vector named grp that has two levels: a and b, where the first 7 values are a and the second 4 values are b.\nRun is.factor(grp) in the Console.\nRun is.vector(grp) in the Console.\nRun typeof(grp) in the Console.\n\nRelated to the last task, a factor object is technically a collection of integers that have labels associated with each unique integer value.\n\n\n\n1.8.6.2 Creating an ordered factor object\nCategorical data sometimes has a natural ordering that we want to acknowledge in our analysis. E.g., we may have categorical data with the levels small, medium, and large. The levels have a natural ordering (either smallest to largest or vice versa, depending on the context).\nSuppose we create the size vector with character values \"small\", \"medium\", and \"large\".\n\nsize &lt;- c(\"small\", \"medium\", \"small\", \"large\", \"medium\", \"medium\", \"large\")\n\nIf we convert size to a factor object, R will automatically order the levels of size alphabetically, as seen from the code chunk below.\n\nfactor(size)\n\n[1] small  medium small  large  medium medium large \nLevels: large medium small\n\n\nThis is not technically a problem, but can result in undesirable side effects such as plots with levels in an undesirable order.\nTo create an ordered factor object, we specify the desired order of the levels using the levels argument and set the ordered argument to TRUE, as in the code below.\n\nfactor(size, levels = c(\"small\", \"medium\", \"large\"), ordered = TRUE)\n\n[1] small  medium small  large  medium medium large \nLevels: small &lt; medium &lt; large\n\n\n\n\n\n1.8.7 Extracting parts of a vector\nParts a vector can be extracted by appending an index vector in square brackets [] to the name of the vector, where the index vector indicates which parts of the vector to retain or exclude. We can include either numbers or logical values in our index vector. We discuss both approaches below.\n\n1.8.7.1 Selection using a numeric index vector\nLet’s create a numeric vector a with the values 2, 4, 6, 8, 10, 12, 14, 16.\n\na &lt;- seq(2, 16, by = 2)\na\n\n[1]  2  4  6  8 10 12 14 16\n\n\nTo extract the 2nd, 4th, and 6th elements of a, we can use the code below. The code indicates that the 2nd, 4th, and 6th elements of a should be extracted.\n\na[c(2, 4, 6)]\n\n[1]  4  8 12\n\n\nWe can also use “negative” indexing to indicate the elements of the vector we want to exclude. Specifically, supplying a negative index vector indicates the values we want to exclude from our selection.\nIn the example below, we use the minus (-) sign in front of the index vector c(2, 4, 6) to indicate we want all elements of a EXCEPT the 2nd, 4th, and 6th.\n\na[-c(2, 4, 6)]\n\n[1]  2  6 10 14 16\n\n\nRunning the code chunk below excludes the 3rd through 6th elements of a.\n\na[-(3:6)]\n\n[1]  2  4 14 16\n\n\n\n\n1.8.7.2 Logical expressions\nA logical expression uses one or more logical operators to determine which elements of an object satisfy the specified statement. The basic logical operators are:\n\n&lt;, &lt;=: less than, less than or equal to.\n&gt;, &gt;=: greater than, greater than or equal to.\n==: equal to.\n!=: not equal to.\n\nCreating a logical expression with a vector will result in a logical vector indicating whether each element satisfies the logical expression.\n\n\n\nHands-on Practice\nRun the following commands in R and see what is printed. What task is each statement performing?\n\na &gt; 10\n\n\na &lt;= 4\n\n\na == 10\n\n\na != 10\n\n\n\n\n1.8.7.3 The “and”, “or”, and “not” operators\nWe can create more complicated logical expressions using the “and”, “or”, and “not” operators.\n\n&: and.\n|: or.\n!: not, i.e., not true.\n\nThe & operator returns TRUE if all logical values connected by the & are TRUE, otherwise it returns FALSE.\nThe | operator returns TRUE if any logical values connected by the | are TRUE, otherwise it returns FALSE.\nThe ! operator returns the complement of a logical value or expression.\n\n\n\nHands-on Practice\nRun the following commands below in the Console.\n\nWhat role does & serve in a sequence of logical values?\nWhat role does | serve in a sequence of logical values?\nWhat role does ! serve in a sequence of logical values?\n\n\nTRUE & TRUE & TRUE\n\n\nTRUE & TRUE & FALSE\n\n\nFALSE | TRUE | FALSE\n\n\nFALSE | FALSE | FALSE\n\n\n!TRUE\n\n\n!FALSE\n\n\n\n\n1.8.7.4 Connecting logical expressions\nLogical expressions can be connected via & and | (and impacted via !), in which case the operators are applied elementwise (i.e., to all of the first elements in the expressions, then all the second elements in the expressions, etc).\n\n\n\nHands-on Practice\nRun the following commands in R and see what is printed. What task is each statement performing?\nNote that the parentheses () are used to group logical expressions to more easily understand what is being done. This is a good coding style to follow.\n\n(a &gt; 6) & (a &lt;= 10)\n\n\n(a &lt;= 4) | (a &gt;= 12)\n\n\n!((a &lt;= 4) | (a &gt;= 12))\n\n\n\n1.8.7.5 Selection using logical expressions\nLogical expressions can be used to return parts of an object satisfying the appropriate criteria. Specifically, we pass logical expressions within the square brackets to access part of a data structure. This syntax will return each element of the object for which the expression is TRUE.\n\n\n\n\nHands-on Practice\nRun the following commands in R and see what is printed. What task is each statement performing?\n\na[a &lt; 6]\n\n\na[a == 10]\n\n\na[(a &lt; 6)|(a == 10)]",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>R Foundations</span>"
    ]
  },
  {
    "objectID": "r-foundations.html#helpful-functions",
    "href": "r-foundations.html#helpful-functions",
    "title": "1  R Foundations",
    "section": "1.9 Helpful functions",
    "text": "1.9 Helpful functions\nWe provide a brief overview of R functions we often use in our data analysis.\n\n1.9.1 General functions\nFor brevity, Table 1.2 provides a table of functions commonly useful for basic data analysis along with a description of their purpose.\n\n\n\n\nTable 1.2: Functions frequently useful for data analysis.\n\n\n\n\n\n\n\n\n\n\nfunction.\npurpose\n\n\n\n\nlength\nDetermines the length/number of elements in an object.\n\n\nsum\nSums the elements in the object.\n\n\nmean\nComputes the sample mean of the elements in an object.\n\n\nvar\nComputes the sample variance of the elements in an object.\n\n\nsd\nComputes the sample standard deviation the elements of an object.\n\n\nrange\nDetermines the range (minimum and maximum) of the elements of an object.\n\n\nlog\nComputes the (natural) logarithm of elements in an object.\n\n\nsummary\nReturns a summary of an object. The output changes depending on the class type of the object.\n\n\nstr\nProvides information about the structure of an object. Usually, the class of the object and some information about its size.\n\n\n\n\n\n\n\n\n\n\n\nHands-on Practice\nRun the following commands in the Console. Determine what task each command is performing.\n\nx &lt;- rexp(100) # sample 100 iid values from an Exponential(1) distribution\n\n\nlength(x)\n\n\nsum(x)\n\n\nmean(x)\n\n\nvar(x)\n\n\nsd(x)\n\n\nrange(x)\n\n\nlog(x)\n\n\nsummary(x)\n\n\nstr(x) # structure of x\n\n\n\n\n1.9.2 Functions related to statistical distributions\nR is designed specifically for statistical analysis, so it natively includes functionality for determining properties of statistical distributions. R makes it easy to evaluate the cumulative distribution function (CDF) of a distribution, the quantiles of a distribution, the density or mass of a distribution, and to sample random values from a distribution.\nSuppose that a random variable \\(X\\) has the dist distribution. The function templates in the list below describe how to obtain certain properties of \\(X\\).\n\np[dist](q, ...): returns the cdf of \\(X\\) evaluated at q, i.e., \\(p=P(X\\leq q)\\).\nq[dist](p, ...): returns the inverse cdf (or quantile function) of \\(X\\) evaluated at \\(p\\), i.e., \\(q = \\inf\\{x: P(X\\leq x) \\geq p\\}\\).\nd[dist](x, ...): returns the mass or density of \\(X\\) evaluated at \\(x\\) (depending on whether \\(X\\) is a discrete or continuous random variable).\nr[dist](n, ...): returns an independent and identically distributed random sample of size n having the same distribution as \\(X\\).\nThe ... indicates that additional arguments describing the parameters of the distribution may be required.\n\nTo determine the distributions available by default in R, run ?Distributions in the R Console. We demonstrate some of this functionality in the practice below.\nNote: If we are using functions related to statistical distributions in R, then it is imperative that we look at the associated documentation to determine the parameterization of the distribution, as this dramatically impacts the results. Some distributions have multiple common parameterizations.\n\n\n\nHands-on Practice\nRun the following commands in R to see the output. Before each command is a description of the action performed by the command.\npnorm(1.96, mean = 0, sd = 1) returns the probability that a standard normal random variable is less than or equal to 1.96, i.e., \\(P(X \\leq 1.96)\\).\n\npnorm(1.96, mean = 0, sd = 1)\n\nqunif(0.6, min = 0, max = 1) returns the value \\(x\\) such that \\(P(X\\leq x) = 0.6\\) for a uniform random variable on the interval \\([0, 1]\\).\n\nqunif(0.6, min = 0, max = 1)\n\ndbinom(2, size = 20, prob = .2) returns the probability that \\(X\\) equals 2 when \\(X\\) has a Binomial distribution with \\(n=20\\) trials and the probability of a successful trial is \\(0.2\\).\n\ndbinom(2, size = 20, prob = .2)\n\ndexp(1, rate = 2) evaluates the density of an exponential random variable with mean = 1/2 (i.e., the reciprocal of the rate) at \\(x=1\\).\n\ndexp(1, rate = 2)\n\nrchisq(10, df = 5) draws a sample of 10 observations from a chi-squared random variable with 5 degrees of freedom.\n\nrchisq(10, df = 5)",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>R Foundations</span>"
    ]
  },
  {
    "objectID": "r-foundations.html#data-frames",
    "href": "r-foundations.html#data-frames",
    "title": "1  R Foundations",
    "section": "1.10 Data Frames",
    "text": "1.10 Data Frames\nData frames are a fundamental data structure used by most of R’s modeling functions. Data frames are two-dimensional data objects where each column of a data frame is a vector (or variable) of possibly different data types. The class of a base R data frame is data.frame, which is technically a specially structured list.\nIn general, we prefer our data to be tidy, which means that each variable forms a column of the data frame, and each observation forms a row.\n\n1.10.1 Direct creation\nData frames are directly created by passing vectors into the data.frame function. The names of the columns in the data frame are the names of the vectors we give the data.frame function. Consider the following simple example.\n\n# simple data frame creation\nd &lt;- c(1, 2, 3, 4)\ne &lt;- c(\"red\", \"white\", \"blue\", NA)\nf &lt;- c(TRUE, TRUE, TRUE, FALSE)\ndf &lt;- data.frame(d,e,f)\ndf\n\n  d     e     f\n1 1   red  TRUE\n2 2 white  TRUE\n3 3  blue  TRUE\n4 4  &lt;NA&gt; FALSE\n\n\nThe columns of a data frame can be renamed using the names function on the data frame and assigning a vector of names to the data frame.\n\n# name columns of data frame\nnames(df) &lt;- c(\"ID\", \"Color\", \"Passed\")\ndf\n\n  ID Color Passed\n1  1   red   TRUE\n2  2 white   TRUE\n3  3  blue   TRUE\n4  4  &lt;NA&gt;  FALSE\n\n\nThe columns of a data frame can be named when we create the data frame by using the syntax name = for each vector of data.\n\n# create data frame with better column names\ndf2 &lt;- data.frame(ID = d, Color = e, Passed = f)\ndf2\n\n  ID Color Passed\n1  1   red   TRUE\n2  2 white   TRUE\n3  3  blue   TRUE\n4  4  &lt;NA&gt;  FALSE\n\n\n\n\n1.10.2 Importing Data\nDirect creation of data frames is only appropriate for very small data sets. In practice, we will import data from a file into R.\nThe read.table function imports data in table format from file into R as a data frame.\nThe basic usage of this function is: read.table(file, header = TRUE, sep = \",\").\n\nfile is the file path and name of the file we want to import into R.\n\nIf we don’t know the file path, setting file = file.choose() will bring up a dialog box asking us to locate the file we want to import.\n\nheader specifies whether the data file has a header (variable labels for each column of data in the first row of the data file).\n\nIf we don’t specify this option in R or specify header = FALSE, then R will assume the file doesn’t have any headings.\nheader = TRUE tells R to read in the data as a data frame with column names taken from the first row of the data file.\n\nsep specifies the delimiter separating elements in the file.\n\nIf each column of data in the file is separated by a space, then use sep = \" \".\nIf each column of data in the file is separated by a comma, then use sep = \",\".\nIf each column of data in the file is separated by a tab, then use sep = \"\\t\".\n\n\n\n\n\nHands-on Practice\nConsider reading in a csv (comma separated file) with a header.\nThe file in question is available in the api2lm package (French 2023) and contains crime-related information for U.S. states for the year 2009. However, we will import the data directly from a location where it is stored on GitHub.\nIn the code below, we specify the path of the file prior to specifying the file name (crime2009.csv). Since the file has a header, we specify header = TRUE. Since the data values are separated by commas, we specify sep = \",\".\n\n# specify file path\npath &lt;- \"https://raw.githubusercontent.com/jfrench/api2lm/main/inst/extdata/crime2009.csv\"\n# import data as data frame\ncrime2009 &lt;- read.table(file = path, header = TRUE, sep = \",\")\n# view data structure\nstr(crime2009)\n\n'data.frame':   51 obs. of  8 variables:\n $ state  : chr  \"Alabama\" \"Alaska\" \"Arizona\" \"Arkansas\" ...\n $ violent: num  460 633 423 530 473 ...\n $ murder : num  7.1 3.2 5.5 6.3 5.4 3.2 3 4.6 24.2 5.5 ...\n $ hs_grad: num  82.1 91.4 84.2 82.4 80.6 89.3 88.6 87.4 87.1 85.3 ...\n $ poverty: num  17.5 9 16.5 18.8 14.2 12.9 9.4 10.8 18.4 14.9 ...\n $ single : num  29 25.5 25.7 26.3 27.8 21.4 25 27.6 48 26.6 ...\n $ white  : num  70 68.3 80 78.4 62.7 84.6 79.1 71.9 38.7 76.9 ...\n $ urban  : num  48.6 44.5 80.1 39.5 89.7 ...\n\n\nRunning str on the data frame gives us a general picture of the values stored in the data frame, the variable names, and the default variable types.\nThe read_table function in the readr package (Wickham, Hester, and Bryan 2024) is arguably a better way of importing tabular data from file and uses similar syntax. We can import data contained in a Microsoft Excel file using functions available in the readxl package (Wickham and Bryan 2023).\n\n\n\n1.10.3 Extracting parts of a data frame\nR provides many ways to extract parts of a data frame. We will provide several examples using the mtcars data frame in the datasets package.\nThe mtcars data frame has 32 observations for 11 variables. The variables are:\n\nmpg: miles per gallon.\ncyl: number of cylinders.\ndisp: engine displacement (cubic inches).\nhp: horsepower.\ndrat: rear axle ratio.\nwt: weight in 1000s of pounds.\nqsec: time in seconds to travel 0.25 of a mile.\nvs: engine shape (0 = V-shaped, 1 = straight).\nam: transmission type (0 = automatic, 1 = manual).\ngear: number of forward gears.\ncarb: number of carburetors.\n\nWe load the data set and examine the basic structure by running the commands below.\n\ndata(mtcars) # load data set\nstr(mtcars)  # examine data structure\n\n'data.frame':   32 obs. of  11 variables:\n $ mpg : num  21 21 22.8 21.4 18.7 18.1 14.3 24.4 22.8 19.2 ...\n $ cyl : num  6 6 4 6 8 6 8 4 4 6 ...\n $ disp: num  160 160 108 258 360 ...\n $ hp  : num  110 110 93 110 175 105 245 62 95 123 ...\n $ drat: num  3.9 3.9 3.85 3.08 3.15 2.76 3.21 3.69 3.92 3.92 ...\n $ wt  : num  2.62 2.88 2.32 3.21 3.44 ...\n $ qsec: num  16.5 17 18.6 19.4 17 ...\n $ vs  : num  0 0 1 1 0 1 0 1 1 1 ...\n $ am  : num  1 1 1 0 0 0 0 0 0 0 ...\n $ gear: num  4 4 4 3 3 3 3 4 4 4 ...\n $ carb: num  4 4 1 1 2 1 4 2 2 4 ...\n\n\nWe should do some data cleaning on this data set (see Chapter 2), but we will refrain from this for simplicity.\n\n\n1.10.3.1 Direct extraction\nThe column variables of a data frame may be extracted from a data frame by specifying the data frame’s name, then $, and then specifying the name of the desired variable. This pulls the actual variable vector out of the data frame, so the thing extracted is a vector, not a data frame.\nBelow, we extract the mpg variable from the mtcars data frame.\n\nmtcars$mpg\n\n [1] 21.0 21.0 22.8 21.4 18.7 18.1 14.3 24.4 22.8 19.2 17.8 16.4 17.3 15.2 10.4\n[16] 10.4 14.7 32.4 30.4 33.9 21.5 15.5 15.2 13.3 19.2 27.3 26.0 30.4 15.8 19.7\n[31] 15.0 21.4\n\n\nAnother way to extract a variable from a data frame as a vector is df[, \"var\"], where df is the name of our data frame and var is the desired variable name. We extract the mpg vector from mtcars below.\n\nmtcars[,\"mpg\"]\n\n [1] 21.0 21.0 22.8 21.4 18.7 18.1 14.3 24.4 22.8 19.2 17.8 16.4 17.3 15.2 10.4\n[16] 10.4 14.7 32.4 30.4 33.9 21.5 15.5 15.2 13.3 19.2 27.3 26.0 30.4 15.8 19.7\n[31] 15.0 21.4\n\n\nThe syntax above is a special case of extracting information from a data frame using df[rows, columns] style syntax, where rows and columns indicate the desired rows or columns to extract. If either the rows or columns are left blank, then all rows or columns, respectively, are extracted.\nThe df[rows, columns] syntax has confusing behavior when we only specify a single column. Specifically, the [ operator has a drop argument that is set to TRUE by default. The drop argument controls whether the result is coerced to the lowest possible dimension. If we we only extract a single column of our data frame using [, then R will coerce the result to a vector by default.\nTo get around the default drop behavior we can change the drop argument to FALSE, as shown below. We only retain the first 5 rows for brevity.\n\n# extract mpg variable, keep as data frame\nmtcars[1:5, \"mpg\", drop = FALSE]\n\n                   mpg\nMazda RX4         21.0\nMazda RX4 Wag     21.0\nDatsun 710        22.8\nHornet 4 Drive    21.4\nHornet Sportabout 18.7\n\n\nAn easier approach to avoiding the default drop behavior is to use the slightly different syntax df[\"var\"] (notice we no longer have the comma to separate rows and columns). We use this syntax below for the mpg variable in mtcars. For brevity, we use the print method for a data.frame object and specify the max argument to limit the number of data frame entries we print.\n\n# extract mpg variable, keep as data frame, show only 5 rows\nprint(mtcars[\"mpg\"], max = 5)\n\n                   mpg\nMazda RX4         21.0\nMazda RX4 Wag     21.0\nDatsun 710        22.8\nHornet 4 Drive    21.4\nHornet Sportabout 18.7\n [ reached 'max' / getOption(\"max.print\") -- omitted 27 rows ]\n\n\nTo select multiple variables in a data frame, we can provide a character vector with multiple variable names between []. In the example below, we extract both the mpg and cyl variables from mtcars.\n\nprint(mtcars[c(\"mpg\", \"cyl\")], max = 10)\n\n                   mpg cyl\nMazda RX4         21.0   6\nMazda RX4 Wag     21.0   6\nDatsun 710        22.8   4\nHornet 4 Drive    21.4   6\nHornet Sportabout 18.7   8\n [ reached 'max' / getOption(\"max.print\") -- omitted 27 rows ]\n\n\nWe can also use numeric indices to directly indicate the rows or columns of the data frame that we would like to extract. We can also combine this syntax style with the syntax styles previously discussed.\n\ndf[1,] accesses the first row of df.\ndf[1:2,] accesses the first two rows of df.\ndf[, 2] accesses the second column of df.\ndf[1:2, 2:3] accesses the information in rows 1 and 2 of columns 2 and 3 of df.\ndf[c(1, 3, 5), c(\"var1\", \"var2\")] accesses the information in rows 1, 3, and 5 of the var1 and var2 variables.\n\nWe practice these techniques below.\n\n\n\nHands-on Practice\nRun the following commands in the Console. Determine what task each command is performing.\n\ndf3 &lt;- data.frame(numbers = 1:5,\n                  characters = letters[1:5],\n                  logicals = c(TRUE, TRUE, FALSE, TRUE, FALSE))\n\n\ndf3\n\n\ndf3$logicals\n\n\ndf3[1, ]\n\n\ndf3[, 3]\n\n\ndf3[, 2:3]\n\n\ndf3[, c(\"numbers\", \"logicals\")]\n\n\ndf3[c(\"numbers\", \"logicals\")]\n\n\n\n\n1.10.3.2 Extraction using logical expressions\nLogical expressions can be used to subset a data frame.\nTo select specific rows of a data frame, we use the syntax df[logical vector, ], where logical vector is a valid logical vector whose length matches the number of rows in the data frame. Usually, the logical vector is created using a logical expression involving one or more data frame variables. In the code below, we extract the rows of the mtcars data frame for which the hp variable is more than 250.\n\nmtcars[mtcars$hp &gt; 250,]\n\n                mpg cyl disp  hp drat   wt qsec vs am gear carb\nFord Pantera L 15.8   8  351 264 4.22 3.17 14.5  0  1    5    4\nMaserati Bora  15.0   8  301 335 3.54 3.57 14.6  0  1    5    8\n\n\nWe can make the logical expression more complicated and also select specific variables using the syntax discussed in Section 1.10.3.1. Below, we extract the rows of mtcars with 8 cylinders and mpg &gt; 17, while extracting only the mpg, cyl, disp, and hp variables.\n\n# return rows with `cyl == 8` and `mpg &gt; 17`\n# return columns mpg, cyl, disp, hp\nmtcars[mtcars$cyl == 8 & mtcars$mpg &gt; 17, c(\"mpg\", \"cyl\", \"disp\", \"hp\")]\n\n                   mpg cyl  disp  hp\nHornet Sportabout 18.7   8 360.0 175\nMerc 450SL        17.3   8 275.8 180\nPontiac Firebird  19.2   8 400.0 175\n\n\n\n\n1.10.3.3 Extraction using the subset function\nThe techniques for extracting parts of a data frame discussed in -Section 1.10.3.1 and -Section 1.10.3.2 are the fundamental approaches for selecting desired parts of a data frame. However, these techniques can seem complex and difficult to interpret, particularly when looking back at code we have written in the past. A sleeker approach to extracting part of a data frame is to use the subset function.\nThe subset function returns the elements of a data frame that meets the specified conditions. The basic usage of this function is: subset(x, subset, select, drop = FALSE)\n\nx is the object we want to subset.\n\nx can be a vector, matrix, or data frame.\n\nsubset is a logical expression that indicates the elements or rows of x to keep (TRUE means keep).\nselect is a vector that indicates the columns to keep.\ndrop is a logical value indicating whether the data frame should “drop” into a vector if only a single row or column is kept. The default is FALSE, meaning that a data frame will always be returned by the subset function by default.\n\nThere are many clever ways of using subset to select specific parts of a data frame. We encourage the reader to run ?base::subset in the Console for more details.\n\n\n\nHands-on Practice\nRun the following commands in the Console to use the subset function to extract parts of the mtcars data frame.\nThe command below subsets the rows of mtcars that have more than 4 gears. Note that any variables referred to in the subset function are assumed to be part of the supplied data frame or are available in memory.\n\nsubset(mtcars, subset = gear &gt; 4)\n\nThe command below will select the disp, hp, and gear variables of mtcars but will exclude the other columns.\n\nsubset(mtcars, select = c(disp, hp, gear))\n\nLastly, we can use the following command to perform the two previous subsetting actions in one step.\n\nsubset(mtcars, subset = gear &gt; 4, select = c(disp, hp, gear))\n\nAn advantage of the subset function is that it makes code easily readable. Using conventional base R syntax, the final code example above would be: mtcars[mtcars$gear&gt;4, c(\"disp\", \"hp\", \"gear\")].\nIt can be difficult to look at base R code and immediately tell what it happening, so the subset function adds clarity. This is important for collaborating with others, including our future selves!\n\n\n\n\n1.10.4 Modifying a Data Frame\nColumns can be added to a data frame using $ and the assignment operator. In the example below, we add a new column, kpg, to the mtcars data set based on a transformatino of the mpg column.\n\nmtcars$kpg &lt;- mtcars$mpg*1.6\nhead(mtcars)\n\n                   mpg cyl disp  hp drat    wt  qsec vs am gear carb   kpg\nMazda RX4         21.0   6  160 110 3.90 2.620 16.46  0  1    4    4 33.60\nMazda RX4 Wag     21.0   6  160 110 3.90 2.875 17.02  0  1    4    4 33.60\nDatsun 710        22.8   4  108  93 3.85 2.320 18.61  1  1    4    1 36.48\nHornet 4 Drive    21.4   6  258 110 3.08 3.215 19.44  1  0    3    1 34.24\nHornet Sportabout 18.7   8  360 175 3.15 3.440 17.02  0  0    3    2 29.92\nValiant           18.1   6  225 105 2.76 3.460 20.22  1  0    3    1 28.96",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>R Foundations</span>"
    ]
  },
  {
    "objectID": "r-foundations.html#using-the-pipe-operator",
    "href": "r-foundations.html#using-the-pipe-operator",
    "title": "1  R Foundations",
    "section": "1.11 Using the pipe operator",
    "text": "1.11 Using the pipe operator\nR’s native pipe operator (|&gt;) allows us to “pipe” the object on the left side of the operator into the first argument of the function on the right side of the operator. There are ways to modify this default behavior, but we will not discuss them.\nThe pipe operator is a convenient way to string together numerous steps in a string of commands. This coding style is generally considered more readable than other approaches because we can incrementally modify the object through each pipe, and each step of the pipe is easy to understand. Ultimately, it’s a stylistic choice that we can decide to adopt or ignore.\nConsider the following approaches to extracting part of mtcars. We choose the rows for which engine displacement is more than 400 and only keep the mpg, disp, and hp columns. We first do this in a single function call using the subset function.\n\n# two styles for selecting certain rows and columns of mtcars\nsubset(mtcars,\n       subset = disp &gt; 400,\n       select = c(mpg, disp, hp))\n\n                     mpg disp  hp\nCadillac Fleetwood  10.4  472 205\nLincoln Continental 10.4  460 215\nChrysler Imperial   14.7  440 230\n\n\nNext, we use the piping approach to break the action into smaller parts.\n\nmtcars |&gt;\n  subset(subset = disp &gt; 400) |&gt;\n  subset(select = c(mpg, disp, hp))\n\n                     mpg disp  hp\nCadillac Fleetwood  10.4  472 205\nLincoln Continental 10.4  460 215\nChrysler Imperial   14.7  440 230\n\n\nWhen reading code with pipes, the pipe can be thought of as the word “then”. In the code above, we take mtcars then subset it based on disp and then select some columns.\nMost parts of the world do not use miles per gallon to measure fuel economy because they don’t measure distance in miles nor volume in gallons. A common measure of fuel economy is the liters of fuel required to travel 100 kilometers. Noting that 3.8 liters is approximately equivalent to 1 U.S. gallon and 1.6 kilometers is approximately equivalent to 1 mile, we can convert fuel economy of \\(x\\) miles per gallon to liters per 100 kilometers by noting:\n\\[\\frac{1}{x}\\frac{\\mathrm{gal}}{\\mathrm{mi}}\\times\\frac{3.8}{1}\\frac{\\mathrm{L}}{\\mathrm{gal}}\\times\\frac{1}{1.6}\\frac{\\mathrm{mi}}{\\mathrm{km}}\\times\\frac{100\\;\\mathrm{km}}{100\\;\\mathrm{km}} = \\frac{237.5}{x}\\frac{\\mathrm{L}}{100\\;\\mathrm{km}}.\\]\nThus, to convert from miles per gallon to liters per 100 kilometers, we take 237.5 and divide by the number of miles per gallon.\nWe consider two approaches for converting the units of a variable in miles per gallon to liters per 100 km. In the first approach, we use the base::transform function create a new variable, lp100km, in the mtcars data frame that describes the liters of fuel each car requires to travel 100 kilometers and assign it the name mtcars2. Then we select only the columns mpg and lp100km from mtcars and assign it the name mtcars3. We then use the head function to print only the first 5 observations. Run ?base::transform in the Console for more details and examples.\n\n# create new variable\nmtcars2 &lt;- transform(mtcars, lp100km = 237.5/mpg)\n# select certain columns\nmtcars3 &lt;- subset(mtcars2, select = c(mpg, lp100km))\n# print first 5 rows\nhead(mtcars3, n = 5)\n\n                   mpg  lp100km\nMazda RX4         21.0 11.30952\nMazda RX4 Wag     21.0 11.30952\nDatsun 710        22.8 10.41667\nHornet 4 Drive    21.4 11.09813\nHornet Sportabout 18.7 12.70053\n\n\nNext, we perform the actions above with pipes.\n\n# create new variable, select columns, extract first 5 rows\nmtcars |&gt;\n  transform(lp100km = 237.5/mpg) |&gt;\n  subset(select = c(mpg, lp100km)) |&gt;\n  head(n = 5)\n\n                   mpg  lp100km\nMazda RX4         21.0 11.30952\nMazda RX4 Wag     21.0 11.30952\nDatsun 710        22.8 10.41667\nHornet 4 Drive    21.4 11.09813\nHornet Sportabout 18.7 12.70053",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>R Foundations</span>"
    ]
  },
  {
    "objectID": "r-foundations.html#dealing-with-common-problems",
    "href": "r-foundations.html#dealing-with-common-problems",
    "title": "1  R Foundations",
    "section": "1.12 Dealing with common problems",
    "text": "1.12 Dealing with common problems\nWe are going to have to deal with many errors and problems as we use R. It happens even to the best programmers.\nEvery problem is unique, but there are common mistakes that we try to provide insight for below.\n\n1.12.1 Error in ...: could not find function \"...\"\nWe probably forgot to load the package needed to use the function. We may also have misspelled the function name.\n\n\n1.12.2 Error: object '...' not found\nThe object doesn’t exist in loaded memory. Perhaps we forget to assign that name to an object or misspelled the name of the object we are trying to access.\n\n\n1.12.3 Error in plot.new() : figure margins too large\nThis typically happens because our Plots pane is too small. We should increase the size of the Plots pane and try again.\n\n\n1.12.4 Code was working, but isn’t anymore\nWe may have run code out of order. It may work if we run it in order. Or we may have run something in the Console that we don’t have in our Script file. It is good practice to clear our environment (the objects R has loaded in memory) using the broom icon in the Environment pane and rerunning our entire Script file to ensure it behaves as expected. The broom icon is shown in Figure 1.4.\n\n\n\n\n\n\n\n\nFigure 1.4: The broom icon can be clicked to clear the objects loaded in the Environment.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>R Foundations</span>"
    ]
  },
  {
    "objectID": "r-foundations.html#ecosystem-debate",
    "href": "r-foundations.html#ecosystem-debate",
    "title": "1  R Foundations",
    "section": "1.13 Ecosystem debate",
    "text": "1.13 Ecosystem debate\nIn general, we favor performing analysis using base R functionality, which means we try to perform our analysis with features R offers by default. Base R code is easier to maintain since base R features change very slowly and are nearly always backwards compatible. Additionally, base R functions are often faster than alternatives. However, more complicated aspects of regression analysis are not easy to perform using base R, so we will introduce new packages and functions as we progress.\nMany readers may have previous experience working with the tidyverse (https://www.tidyverse.org) and wonder how frequently we use tidyverse functionality. The tidyverse offers a unified framework for data manipulation and visualization that tends to be more consistent than base R. However, there are many situations where a base R solution is more straightforward than a tidyverse solution, not to mention the fact that there are many aspects of R programming (e.g., S3 and S4 objects, method dispatch) that require knowledge of base R features. Because the R universe is vast and there are many competing coding styles, we will prioritize analysis approaches using base R, which gives users a stronger programming foundation. However, we will use parts of the tidyverse when it greatly simplifies analysis, data manipulation, or visualization because it provides an extremely useful feature set.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>R Foundations</span>"
    ]
  },
  {
    "objectID": "r-foundations.html#going-deeper",
    "href": "r-foundations.html#going-deeper",
    "title": "1  R Foundations",
    "section": "1.14 Going deeper",
    "text": "1.14 Going deeper\n\n1.14.1 Comparing assignment operators\nAs previously mentioned in Section Section 1.3, both &lt;- and = can mostly be used interchangeably for assignment. But there are times when using = for assignment can be problematic. Consider the examples below where we want to use system.time to time how long it takes to draw 100 values from a standard normal distribution and assign it the name result.\nThis code works:\n\nsystem.time(result &lt;- rnorm(100))\n\n   user  system elapsed \n      0       0       0 \n\n\nThis code doesn’t work:\n\nsystem.time(result = rnorm(100))\n\nError in system.time(result = rnorm(100)): unused argument (result = rnorm(100))\n\n\nWhat’s the difference? In the second case, R thinks we are setting the result argument of the system.time function (which doesn’t exist) to the value produced by rnorm(100).\nThus, it is best to use &lt;- for assigning a name to an object and reserving = for specifying function arguments.\n\n\n\n\nFrench, Joshua P. 2023. Api2lm: Functions and Data Sets for the Book \"a Progressive Introduction to Linear Models\". https://CRAN.R-project.org/package=api2lm.\n\n\nMüller, Kirill, and Hadley Wickham. 2023. Tibble: Simple Data Frames. https://tibble.tidyverse.org/.\n\n\nR Core Team. 2024. R: A Language and Environment for Statistical Computing. Vienna, Austria: R Foundation for Statistical Computing. https://www.R-project.org/.\n\n\nWickham, Hadley. 2019. Advanced R. CRC press. http://adv-r.had.co.nz/.\n\n\n———. 2023a. Forcats: Tools for Working with Categorical Variables (Factors). https://forcats.tidyverse.org/.\n\n\n———. 2023b. Stringr: Simple, Consistent Wrappers for Common String Operations. https://stringr.tidyverse.org.\n\n\n———. 2023c. Tidyverse: Easily Install and Load the Tidyverse. https://tidyverse.tidyverse.org.\n\n\nWickham, Hadley, and Jennifer Bryan. 2023. Readxl: Read Excel Files. https://readxl.tidyverse.org.\n\n\nWickham, Hadley, Winston Chang, Lionel Henry, Thomas Lin Pedersen, Kohske Takahashi, Claus Wilke, Kara Woo, Hiroaki Yutani, Dewey Dunnington, and Teun van den Brand. 2024. Ggplot2: Create Elegant Data Visualisations Using the Grammar of Graphics. https://ggplot2.tidyverse.org.\n\n\nWickham, Hadley, Romain François, Lionel Henry, Kirill Müller, and Davis Vaughan. 2023. Dplyr: A Grammar of Data Manipulation. https://dplyr.tidyverse.org.\n\n\nWickham, Hadley, and Lionel Henry. 2023. Purrr: Functional Programming Tools. https://purrr.tidyverse.org/.\n\n\nWickham, Hadley, Jim Hester, and Jennifer Bryan. 2024. Readr: Read Rectangular Text Data. https://readr.tidyverse.org.\n\n\nWickham, Hadley, Davis Vaughan, and Maximilian Girlich. 2024. Tidyr: Tidy Messy Data. https://tidyr.tidyverse.org.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>R Foundations</span>"
    ]
  },
  {
    "objectID": "data-exploration.html",
    "href": "data-exploration.html",
    "title": "2  Data Cleaning and Exploration",
    "section": "",
    "text": "2.1 Raw Palmer penguins data –&gt;\nTo attach a data set from an R package into our working environment, we run the data function while specifying the name of the data set to be loaded and the package that contains the data. We do this for the penguins_raw data below.\ndata(penguins, package = \"palmerpenguins\")\nThis command actually loads two data sets: penguins_raw, the data set we will be looking at, and penguins, a simplified version of penguins_raw. Note that this particular data set loads in a nonstandard way; we would normally load the penguins_raw data using data(penguins_raw, package = \"palmerpenguins\").\nWe could have also loaded the data set by running the following commands in the Console.\nlibrary(palmerpenguins)\ndata(penguins)\nThis second approach loads and attaches everything the package includes into memory (functions, data, etc). If we are going to be using many functions or objects from a package, then the second approach is sensible. Otherwise, the first approach is more precise and is better coding practice to prevent masking (as discussed in Chapter 1) and namespace pollution.\nThe penguins_raw data set provides data related to various penguin species measured in the Palmer Archipelago (Antarctica), originally provided by Gorman, Williams, and Fraser (2014).\nThe data set includes 344 observations of 17 variables. The variables are:",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Data Cleaning and Exploration</span>"
    ]
  },
  {
    "objectID": "data-exploration.html#raw-palmer-penguins-data",
    "href": "data-exploration.html#raw-palmer-penguins-data",
    "title": "2  Data Cleaning and Exploration",
    "section": "",
    "text": "studyName: the expedition from which the data were collected (character).\nSample Number: the continuous number sequence for each sample (numeric).\nSpecies: the penguin’s species (character).\nRegion: the region of the Palmer LTER sampling grid the sample was obtained (character).\nIsland: the island on which the penguin was observed (character).\nStage: the reproductive stage of the observation (character).\nIndividual ID: the unique identification number of the observations (character).\nClutch Completion: whether the study nest was observed with a “full clutch” of 2 eggs (character).\nDate Egg: the date that the study nest was observed with 1 egg (Date).\nCulman Length (mm): the length of the dorsal ridge of the penguin’s bill in millimeters (numeric).\nCulmen Depth (mm): the depth of the dorsal ridge of the penguin’s bill in millimeters (numeric).\nFlipper Length (mm): the penguin’s flipper length in millimeters (numeric).\nBody Mass (g): the penguin’s body mass in grams (numeric).\nSex: the penguin’s sex (character).\nDelta 15 N (o/oo): the ratio of stable isotopes 15N:14N (numeric).\nDelta 13 C (o/oo): the ratio of stable isotopes 15C:12C (numeric).\nComments: additional information about the observation (character).",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Data Cleaning and Exploration</span>"
    ]
  },
  {
    "objectID": "data-exploration.html#initial-data-cleaning",
    "href": "data-exploration.html#initial-data-cleaning",
    "title": "2  Data Cleaning and Exploration",
    "section": "2.2 Initial data cleaning",
    "text": "2.2 Initial data cleaning\nThe str function is a great first function to apply on a newly loaded data set because it provides a general overview of the data’s structure.\n\nstr(penguins_raw, give.attr = FALSE)\n\ntibble [344 × 17] (S3: tbl_df/tbl/data.frame)\n $ studyName          : chr [1:344] \"PAL0708\" \"PAL0708\" \"PAL0708\" \"PAL0708\" ...\n $ Sample Number      : num [1:344] 1 2 3 4 5 6 7 8 9 10 ...\n $ Species            : chr [1:344] \"Adelie Penguin (Pygoscelis adeliae)\" \"Adelie Penguin (Pygoscelis adeliae)\" \"Adelie Penguin (Pygoscelis adeliae)\" \"Adelie Penguin (Pygoscelis adeliae)\" ...\n $ Region             : chr [1:344] \"Anvers\" \"Anvers\" \"Anvers\" \"Anvers\" ...\n $ Island             : chr [1:344] \"Torgersen\" \"Torgersen\" \"Torgersen\" \"Torgersen\" ...\n $ Stage              : chr [1:344] \"Adult, 1 Egg Stage\" \"Adult, 1 Egg Stage\" \"Adult, 1 Egg Stage\" \"Adult, 1 Egg Stage\" ...\n $ Individual ID      : chr [1:344] \"N1A1\" \"N1A2\" \"N2A1\" \"N2A2\" ...\n $ Clutch Completion  : chr [1:344] \"Yes\" \"Yes\" \"Yes\" \"Yes\" ...\n $ Date Egg           : Date[1:344], format: \"2007-11-11\" \"2007-11-11\" ...\n $ Culmen Length (mm) : num [1:344] 39.1 39.5 40.3 NA 36.7 39.3 38.9 39.2 34.1 42 ...\n $ Culmen Depth (mm)  : num [1:344] 18.7 17.4 18 NA 19.3 20.6 17.8 19.6 18.1 20.2 ...\n $ Flipper Length (mm): num [1:344] 181 186 195 NA 193 190 181 195 193 190 ...\n $ Body Mass (g)      : num [1:344] 3750 3800 3250 NA 3450 ...\n $ Sex                : chr [1:344] \"MALE\" \"FEMALE\" \"FEMALE\" NA ...\n $ Delta 15 N (o/oo)  : num [1:344] NA 8.95 8.37 NA 8.77 ...\n $ Delta 13 C (o/oo)  : num [1:344] NA -24.7 -25.3 NA -25.3 ...\n $ Comments           : chr [1:344] \"Not enough blood for isotopes.\" NA NA \"Adult not sampled.\" ...\n\n\nWe see that the penguins_raw object is a tibble, a special kind of data frame provided by the tibble package (Müller and Wickham 2023) as part of the broader tidyverse. It has 344 rows and 17 columns. In general, a tibble will function like a standard data frame, though its behavior may change when tidyverse packages are loaded. In the code below, we confirm that penguins_raw qualifies as a base R data.frame.\n\nis.data.frame(penguins_raw)\n\n[1] TRUE\n\n\nAn alternative to str is the glimpse function from the dplyr package. dplyr::glimpse also summarizes the structure of an object, but also automatically formats the printed output to the size of the Console to make it more readable. An example is provided below.\n\ndplyr::glimpse(penguins_raw)\n\nRows: 344\nColumns: 17\n$ studyName             &lt;chr&gt; \"PAL0708\", \"PAL0708\", \"PAL0708\", \"PAL0708\", \"PAL…\n$ `Sample Number`       &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 1…\n$ Species               &lt;chr&gt; \"Adelie Penguin (Pygoscelis adeliae)\", \"Adelie P…\n$ Region                &lt;chr&gt; \"Anvers\", \"Anvers\", \"Anvers\", \"Anvers\", \"Anvers\"…\n$ Island                &lt;chr&gt; \"Torgersen\", \"Torgersen\", \"Torgersen\", \"Torgerse…\n$ Stage                 &lt;chr&gt; \"Adult, 1 Egg Stage\", \"Adult, 1 Egg Stage\", \"Adu…\n$ `Individual ID`       &lt;chr&gt; \"N1A1\", \"N1A2\", \"N2A1\", \"N2A2\", \"N3A1\", \"N3A2\", …\n$ `Clutch Completion`   &lt;chr&gt; \"Yes\", \"Yes\", \"Yes\", \"Yes\", \"Yes\", \"Yes\", \"No\", …\n$ `Date Egg`            &lt;date&gt; 2007-11-11, 2007-11-11, 2007-11-16, 2007-11-16,…\n$ `Culmen Length (mm)`  &lt;dbl&gt; 39.1, 39.5, 40.3, NA, 36.7, 39.3, 38.9, 39.2, 34…\n$ `Culmen Depth (mm)`   &lt;dbl&gt; 18.7, 17.4, 18.0, NA, 19.3, 20.6, 17.8, 19.6, 18…\n$ `Flipper Length (mm)` &lt;dbl&gt; 181, 186, 195, NA, 193, 190, 181, 195, 193, 190,…\n$ `Body Mass (g)`       &lt;dbl&gt; 3750, 3800, 3250, NA, 3450, 3650, 3625, 4675, 34…\n$ Sex                   &lt;chr&gt; \"MALE\", \"FEMALE\", \"FEMALE\", NA, \"FEMALE\", \"MALE\"…\n$ `Delta 15 N (o/oo)`   &lt;dbl&gt; NA, 8.94956, 8.36821, NA, 8.76651, 8.66496, 9.18…\n$ `Delta 13 C (o/oo)`   &lt;dbl&gt; NA, -24.69454, -25.33302, NA, -25.32426, -25.298…\n$ Comments              &lt;chr&gt; \"Not enough blood for isotopes.\", NA, NA, \"Adult…\n\n\nAnother thing that we notice about penguins_raw is that it has terrible variable names. The variable names have a mixture of lowercase and uppercase letters, parentheses, and even spaces! This makes it complicated to access variables in the data frame. To access the flipper length variable, we would have to use something like the command below. Note the ` ` around Flipper Length (mm) because of the spaces in the variable name.\n\noptions(max.print = 18)\n\n\npenguins_raw$`Flipper Length (mm)`\n\n [1] 181 186 195  NA 193 190 181 195 193 190 186 180 182 191 198 185 195 197\n [ reached getOption(\"max.print\") -- omitted 326 entries ]\n\n\n\noptions(max.print = 99999)\n\nIn The tidyverse style guide (Wickham 2022), Hadley Wickham recommends:\n\nVariable and function names should use only lowercase letters, numbers, and _. Use underscores (_) (so called snake case) to separate words within a name.\n\nWe will apply this recommendation to the penguins_raw data below.\nAdditionally, many variables will be extraneous for our future analyses, so we will select only the ones that we will use in the future. We the subset function to select the Species, Island, Culmen Length (mm), Culmen Depth (mm), Flipper Length (mm), Body Mass (g), and Sex variables of penguins_raw and assign the subsetted data frame the name penguins_clean.\n\npenguins_clean &lt;-\n  penguins_raw |&gt;\n  subset(select = c(\"Species\", \"Island\", \"Culmen Length (mm)\", \"Culmen Depth (mm)\",\n                    \"Flipper Length (mm)\", \"Body Mass (g)\", \"Sex\"))\n\nTo rename the columns of penguins_clean, we use the names function to extract the variable names of the data frame and replace them with a vector containing the desired variable names. A second usage of names confirms that the data frame now has improved column names.\n\n# access variable names and replace with new names\nnames(penguins_clean) &lt;- c(\"species\", \"island\", \"bill_length\", \"bill_depth\",\n                           \"flipper_length\", \"body_mass\", \"sex\")\n# look at new variable names\nnames(penguins_clean)\n\n[1] \"species\"        \"island\"         \"bill_length\"    \"bill_depth\"    \n[5] \"flipper_length\" \"body_mass\"      \"sex\"           \n\n\nThere are still some issues with penguins_clean. Notably, the species, island, and sex variables are categorical but are represented as character vectors. These variables should each be converted to a factor. We use the transform function to convert each variable to a factor. Notice that we must replace the original penguins_clean object with the transformed object using the assignment operator. We then run the str function to confirm the changes.\n\n# convert sex variable to factor, replace original object\npenguins_clean &lt;-\n  penguins_clean |&gt;\n  transform(species = factor(species), island = factor(island), sex = factor(sex))\n# view structure\nstr(penguins_clean)\n\n'data.frame':   344 obs. of  7 variables:\n $ species       : Factor w/ 3 levels \"Adelie Penguin (Pygoscelis adeliae)\",..: 1 1 1 1 1 1 1 1 1 1 ...\n $ island        : Factor w/ 3 levels \"Biscoe\",\"Dream\",..: 3 3 3 3 3 3 3 3 3 3 ...\n $ bill_length   : num  39.1 39.5 40.3 NA 36.7 39.3 38.9 39.2 34.1 42 ...\n $ bill_depth    : num  18.7 17.4 18 NA 19.3 20.6 17.8 19.6 18.1 20.2 ...\n $ flipper_length: num  181 186 195 NA 193 190 181 195 193 190 ...\n $ body_mass     : num  3750 3800 3250 NA 3450 ...\n $ sex           : Factor w/ 2 levels \"FEMALE\",\"MALE\": 2 1 1 NA 1 2 1 2 NA NA ...\n\n\nOur conversion of species, island, and sex to factor variables was successful. However, we notice that the levels of sex are MALE and FEMALE, which is visually unappealing. Also, the levels of species are extremely long, which can create formatting challenges. We simplify both below. First, we confirm the order of the factor levels of the two variables.\n\n# determine levels of species and sex\nlevels(penguins_clean$species)\n\n[1] \"Adelie Penguin (Pygoscelis adeliae)\"      \n[2] \"Chinstrap penguin (Pygoscelis antarctica)\"\n[3] \"Gentoo penguin (Pygoscelis papua)\"        \n\nlevels(penguins_clean$sex)\n\n[1] \"FEMALE\" \"MALE\"  \n\n\nWe now change the levels of each variable in the same order they are printed above and confirm that the changes were successful.\n\n# update factor levels of species and sex\nlevels(penguins_clean$species) &lt;- c(\"adelie\", \"chinstrap\", \"gentoo\")\nlevels(penguins_clean$sex) &lt;- c(\"female\", \"male\")\n# confirm that changes took effect\nstr(penguins_clean)\n\n'data.frame':   344 obs. of  7 variables:\n $ species       : Factor w/ 3 levels \"adelie\",\"chinstrap\",..: 1 1 1 1 1 1 1 1 1 1 ...\n $ island        : Factor w/ 3 levels \"Biscoe\",\"Dream\",..: 3 3 3 3 3 3 3 3 3 3 ...\n $ bill_length   : num  39.1 39.5 40.3 NA 36.7 39.3 38.9 39.2 34.1 42 ...\n $ bill_depth    : num  18.7 17.4 18 NA 19.3 20.6 17.8 19.6 18.1 20.2 ...\n $ flipper_length: num  181 186 195 NA 193 190 181 195 193 190 ...\n $ body_mass     : num  3750 3800 3250 NA 3450 ...\n $ sex           : Factor w/ 2 levels \"female\",\"male\": 2 1 1 NA 1 2 1 2 NA NA ...\n\n\nOur initial data cleaning process is now completed. As we explore our data further, it may become clear that additional data cleaning is needed.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Data Cleaning and Exploration</span>"
    ]
  },
  {
    "objectID": "data-exploration.html#numerical-summarization-of-data",
    "href": "data-exploration.html#numerical-summarization-of-data",
    "title": "2  Data Cleaning and Exploration",
    "section": "2.3 Numerical summarization of data",
    "text": "2.3 Numerical summarization of data\nNumerical exploration of a data set generally consists of computing various relevant statistics for each of the variables in a data set in order to summarize the data. The data type determines which statistics are relevant.\nTable 2.1 provides an overview of common numerical summaries used to explore data, the type of data that can be summarized, the characteristic summarized, and the function used to compute the statistic.\n\n\n\n\nTable 2.1: A summary of statistics frequently used to numerically summarize data, the variable type that can be summarized, the characteristic summarized, and the R function used to compute the statistic.\n\n\n\n\n\n\n\n\n\n\n\n\nnumeric summary\nvariable type\nsummarizes\nR function\n\n\n\n\nmean\nnumeric\ncenter\nmean\n\n\nmedian\nnumeric\ncenter\nmedian\n\n\nvariance\nnumeric\nspread\nvar\n\n\nstandard deviation\nnumeric\nspread\nsd\n\n\ninterquartile range\nnumeric\nspread\nquantile (modified)\n\n\nquantiles\nnumeric\ncenter and spread\nquantile\n\n\ncorrelation\nnumeric\nsimilarity\ncor\n\n\nfrequency distribution\nfactor\ncounts\ntable\n\n\nrelative frequency distribution\nfactor\nproportions\ntable (modified)\n\n\n\n\n\n\n\n\nWe provide additional explanation about the numeric summaries in what follows.\n\n2.3.1 Numeric data\nNumerical exploration of a set of numeric values usually focuses on determining the:\n\ncenter\nspread\nquantiles (less common).\n\nComputing the correlation between two numeric variables can also be useful.\n\n2.3.1.1 Measures of center\nThe sample mean and median are the most common statistics used to represent the “center” of a set of numeric values.\nThe sample mean or average is obtained by adding all values in the sample and dividing by the number of observations. The sample mean is the most commonly used measure of center. A weakness of the sample mean is that it is easily affected by outliers (values that are very large or small compared to the rest of the data values). Formally, if \\(x_1, x_2, \\ldots, x_n\\) are a sample of \\(n\\) numeric values, then the sample mean is computed as \\[\n\\bar{x} = \\frac{1}{n}\\sum_{i=1}^n x_i.\n\\] The mean function computes the sample mean of a set of numeric values.\nThe sample median is the middle value of an ordered set of values (the actual middle value when the number of values is odd and the average of the two middle values when the number of values is even). Alternatively, the median is identical to the 0.5 quantile of the data. The median is considered more “resistant” because it is not so greatly affected by outliers. The median of the values 1, 8, 7, 6, 100 is 7. The median of the values 1, 8, 7, 6, 100, 4 is 6.5. The median function computes the sample median of a set of a numeric values.\nWe compute the sample mean of the body_mass variable of the penguins_clean data in the code below.\n\nmean(penguins_clean$body_mass)\n\n[1] NA\n\n\nWhy is the result NA instead of a number? In general, an NA value “poisons” any calculation it is part of, with the function returning NA. Even a single NA value will cause a calculation to return NA, even if there are thousands or millions of non-NA values. If you want to compute the sample mean of the non-NA values, then you must change the na.rm argument of mean to TRUE, which makes the mean function to temporarily remove NAs prior to calculation. The na.rm argument is provided in many other functions, as we’ll see in subsequent examples. We now compute the sample mean and median of the body_mass variable in penguins_clean, ignoring NA values.\n\n# compute sample mean and median body_mass, ignoring NAs\nmean(penguins_clean$body_mass, na.rm = TRUE)\n\n[1] 4201.754\n\nmedian(penguins_clean$body_mass, na.rm = TRUE)\n\n[1] 4050\n\n\nWe see that the average penguin body_mass is approximately 4,201 grams, while the median penguin body_mass is 4,050 grams. Since the median is less than the mean (i.e., large values are pulling the mean in the positive direction) the data may be positively skewed, but we will need to look at a histogram or density plot of the data to be sure (these plots are discussed in Section 2.4.2.3 and Section 2.4.2.4).\n\n\n2.3.1.2 Quantiles\nInformally, the pth quantile (where \\(0\\leq p \\leq 1\\)) of a set of values is the value that separates the smallest \\(100 p\\)% of the values from the upper \\(100(1-p)\\)% of the values. E.g., the 0.25 sample quantile (often called Q1) of a set of values is the value that separates the smallest 25% of the values from the largest 75% of the values. Similarly, the 0.75 sample quantile (often called Q3) of a set of values is the value that separates the smallest 75% of the values from the largest 25% of the values.\nThe quantile function is used to compute sample quantiles. There are many competing approaches to computing sample quantiles (which we won’t discuss or worry about). Run ?quantile in the Console to learn more about the approaches R can use to compute sample quantiles.\nQuantiles are useful quantifying both the center (median) and spread (minimum and maxmimum or interquartile range) of a set of values.\nWe use the quantile function to compute the minimum (0 quantile), Q1 (0.25 quantile), median (0.5 quantile), Q3 (0.75 quantile), and maximum (1 quantile) of body_mass in the code below. The desired quantiles are provided as a numeric vector to the probs argument.\n\nquantile(penguins_clean$body_mass, probs = c(0, 0.25, 0.5, 0.75, 1), na.rm = TRUE)\n\n  0%  25%  50%  75% 100% \n2700 3550 4050 4750 6300 \n\n\nWe see that the smallest and largest body masses are 2,700 grams and 6,300 grams, so the range of the data is 6,300 - 2,700 = 3,600 grams. Q1 is 3,550 grams, while Q3 is 4,750 grams. Since Q3 and the maximum are further from the median than Q1 and the minimum, respectively, this is additional evidence that this variable may be positively skewed (stretched out in the positive direction), but we must visualize the distribution of this variable to confirm this.\n\n\n2.3.1.3 Measures of spread\nIn addition to identifying the center of a set of values, it is important to measure their spread, i.e., how much the values vary.\nThe sample variance and standard deviation are the most common measures of spread for numeric values. The sample variance of a set of values is the (approximate) average of the squared deviation of each observation from the sample mean. Formally, the equation for the sample variance is \\[\ns^2 = \\frac{1}{n-1}\\sum_{i=1}^n (x_i - \\bar{x})^2.\n\\] The sample standard deviation is the square root of the sample variance and is generally a more useful measure of spread because it is has the same units as the original data. The equation for the sample standard deviation is \\[\ns = \\sqrt{\\frac{1}{n-1}\\sum_{i=1}^n (x_i - \\bar{x})^2}.\n\\] The larger the standard deviation or variance of a set of values, the more the values vary from their sample mean. The sample standard deviation and variance can be greatly affected by outliers. The var function computes the sample variance while sd computes the sample standard deviation.\nThe interquartile range is a more resistant measure of spread based on quantiles. The interquartile range is the difference between the 0.75 and 0.25 quantiles of a data set.\nThe minimum and maximum of a set of values (in relation to their sample mean or median) can also be used to ascertain the spread of a data set. The min and max functions compute the minimum and maximum values of a set of values, respectively.\nWe compute these measures of spread for the body_mass variable below.\n\nvar(penguins_clean$body_mass, na.rm = TRUE) # sample variance\n\n[1] 643131.1\n\nsd(penguins_clean$body_mass, na.rm = TRUE) # sample standard deviation\n\n[1] 801.9545\n\n# interquartile range (names = FALSE removes text above the results)\nquantile(penguins_clean$body_mass, probs = 0.75, na.rm = TRUE, names = FALSE) -\n  quantile(penguins_clean$body_mass, probs = 0.25, na.rm = TRUE, names = FALSE)\n\n[1] 1200\n\nmin(penguins_clean$body_mass, na.rm = TRUE) # minimum\n\n[1] 2700\n\nmax(penguins_clean$body_mass, na.rm = TRUE) # maximum\n\n[1] 6300\n\n\nThe sample variance of body_mass is 643,131.1 grams2, which isn’t easy to interpret. The sample standard deviation is almost 802 grams. So the “typical” deviation of a body_mass value from the sample mean is about 800 grams. The interquartile range is 1,200 grams. The minimum and maximum values of penguin body_mass match what we computed in Section 2.3.1.2.\n\n\n2.3.1.4 Correlation\nThe correlation between two numeric variables quantifies the strength and direction of their linear relationship. The most common correlation statistic is Pearson’s correlation statistic. If \\(x_1, x_2, x_n\\) and \\(y_1, y_2, \\ldots, y_n\\) are two sets of numeric values, then the sample correlation statistic is computed as \\[\nr = \\frac{1}{n-1}\\sum_{i=1}^n\\left(\\frac{x_i - \\bar{x}}{s_x}\\right)\\left(\\frac{y_i - \\bar{y}}{s_y}\\right),\n\\] where \\(\\bar{x}\\) and \\(s_x\\) denote the sample mean and standard deviation of the \\(x\\)’s while \\(\\bar{y}\\) and \\(s_y\\) denote the sample mean and standard deviation of the \\(y\\)’s. \\(r\\) must be between -1 and 1. The cor function computes the sample correlation between two numeric variables.\nThe closer \\(r\\) is to -1 or 1, the closer the data values fall to a straight line when we plot \\((x_i, y_i)\\), \\(i=1,2,\\ldots,n\\) in a scatter plot (discussed in Section 2.4.3.1). Sample correlation values close to 0 indicate that there is no linear relationship between the two variables. Negative \\(r\\) values indicate a negative relationship between the two variables (as values of one variable increase, the values of the other variable tend to decrease). Positive \\(r\\) values indicate a positive relationship between the two variables (as values of one variable increase, the values of the other variable also tend to increase).\nIn the code below, we compute the sample correlation between all numeric variables in penguins_clean. We specify use = \"pairwise.complete.obs\" so that all non-NA pairs of values are used in the calculation. We first determine which variables are numeric using the code below.\n\n# determine whether each variable is numeric\nnum_col &lt;- unlist(lapply(penguins_clean, is.numeric))\n# observe results\nnum_col\n\n       species         island    bill_length     bill_depth flipper_length \n         FALSE          FALSE           TRUE           TRUE           TRUE \n     body_mass            sex \n          TRUE          FALSE \n\n\nWe then compute the sample correlation between all pairs of numeric variables.\n\n# compute correlation of numeric variables\ncor(penguins_clean[, num_col], use = \"pairwise.complete.obs\")\n\n               bill_length bill_depth flipper_length  body_mass\nbill_length      1.0000000 -0.2350529      0.6561813  0.5951098\nbill_depth      -0.2350529  1.0000000     -0.5838512 -0.4719156\nflipper_length   0.6561813 -0.5838512      1.0000000  0.8712018\nbody_mass        0.5951098 -0.4719156      0.8712018  1.0000000\n\n\nCommenting on the output, we see that the values of each variable are perfectly correlated with themselves (this is always true since the values in each pair are identical). The correlation between bill_length and body_mass is 0.87, so heavier penguins tend to have longer bills. Perhaps surprisingly, the correlation between bill_length and bill_depth is -0.24, so penguins with longer bills tend to have shallower (narrower) bills. Similarly, the correlation between bill_depth and body_mass is -0.47, so heavier penguins tend to have narrower bills.\nWe briefly explain why we didn’t simply use the cor function on penguins_clean directly. If we try to use cor on penguins_clean naively, then R will return an error because not all variables in penguins_clean are numeric. To account for this, we create a vector that determines whether a variable in penguins_clean is numeric. Recall that a data.frame object is a specially-structured list object, with each variable being an element of the list. The lapply function applies a function (is.numeric in this case) to each element of the supplied list. In our case, we use this to determine whether each variable is numeric. The result is returned as a list, so we use the unlist function to simplify the list to a vector.\n\n\n\n2.3.2 Categorical data\nThe statistics mentioned in the previous section are generally not appropriate for a categorical variable. Instead, frequency distributions and relative frequency distributions are useful numeric summaries of categorical data.\nA frequency distribution summarizes the number of observations having each level of a categorical variable. The table function produces a frequency distribution (contingency table) summarizing the number of observations for each level of a categorical variable. Note that by default, the table function ignores NA values. We provide an example below.\n\ntable(penguins_clean$sex)\n\n\nfemale   male \n   165    168 \n\n\nWe see that for the sex variable, there are 165 female penguins and 168 male penguins.\nTo count the NA values (if present), we can set the useNA argument of table to \"ifany\", as in the example below.\n\ntable(penguins_clean$sex, useNA = \"ifany\")\n\n\nfemale   male   &lt;NA&gt; \n   165    168     11 \n\n\nWe see that 11 of the observations had no available information on sex.\nA relative frequency distribution summarizes the proportion or percentage of observations with each level of a categorical variable. To compute the relative frequency distribution of a variable, we must divide the frequency distribution by the number of observations. If we want to ignore the NAs, then we can use the following code, which takes the frequency distribution of sex and divides by the number of non-NA sex values.\n\n# divide the frequence distribution of sex by the number of non-NA values\ntable(penguins_clean$sex)/sum(!is.na(penguins_clean$sex))\n\n\n   female      male \n0.4954955 0.5045045 \n\n\nWe see that slightly under 50% of the sex values are female (not accounting for NA values), while slightly more than 50% are male.\nWhat is the command sum(!is.na(penguins_clean$sex)) doing in the code above? The is.na function returns a TRUE for each value that is NA but otherwise returns FALSE. The ! in front of is.na inverts the logical expression so that we are determining whether each value is NOT an NA (and returns TRUE if the value is not NA). It is common in programming to associate TRUE with 1 and FALSE with 0. So, if we sum the values that are not NA, that is equivalent to counting the number of non-NA observations.\nIf we want to include the NA values in our relative frequency distribution, we can use the code below.\n\ntable(penguins_clean$sex, useNA = \"ifany\")/length(penguins_clean$sex)\n\n\n    female       male       &lt;NA&gt; \n0.47965116 0.48837209 0.03197674 \n\n\nWe do not know the sex of approximately 3% of the observed penguins.\n\n\n2.3.3 The summary function\nThe summary function provides a simple approach for quickly quantifying the center and spread of each numeric variable in a data frame or determining the frequency distribution of a factor variable. More specifically, the summary function will compute the minimum, 0.25 quantile, mean, median, 0.75 quantile, and maximum of a numeric variable and will return the frequency distribution of a factor variable. This summary will also count the number of NA values in each variable when they are present.\nA summary method is available for a data.frame object, which means that we can apply the summary function directly to our penguins_clean data frame, which we do below.\n\nsummary(penguins_clean)\n\n      species          island     bill_length      bill_depth   \n adelie   :152   Biscoe   :168   Min.   :32.10   Min.   :13.10  \n chinstrap: 68   Dream    :124   1st Qu.:39.23   1st Qu.:15.60  \n gentoo   :124   Torgersen: 52   Median :44.45   Median :17.30  \n                                 Mean   :43.92   Mean   :17.15  \n                                 3rd Qu.:48.50   3rd Qu.:18.70  \n                                 Max.   :59.60   Max.   :21.50  \n                                 NA's   :2       NA's   :2      \n flipper_length    body_mass        sex     \n Min.   :172.0   Min.   :2700   female:165  \n 1st Qu.:190.0   1st Qu.:3550   male  :168  \n Median :197.0   Median :4050   NA's  : 11  \n Mean   :200.9   Mean   :4202               \n 3rd Qu.:213.0   3rd Qu.:4750               \n Max.   :231.0   Max.   :6300               \n NA's   :2       NA's   :2                  \n\n\nWe conveniently get a numeric summary of all of the variables in our data set (we will see different results for variables that are not factor or numeric type). The summary function makes it easy to identify the presence of any NAs in a variable.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Data Cleaning and Exploration</span>"
    ]
  },
  {
    "objectID": "data-exploration.html#visual-summaries-of-data",
    "href": "data-exploration.html#visual-summaries-of-data",
    "title": "2  Data Cleaning and Exploration",
    "section": "2.4 Visual summaries of data",
    "text": "2.4 Visual summaries of data\nVisual summaries (i.e., plots) of data are vital to understanding our data prior to modeling. They help us spot data errors, unusual observations, and simple patterns. They also help us communicate the results of our analysis.\nWe will introduce basic visualization approaches using base R functions as well as the popular ggplot2 package (Wickham et al. 2024). It is important to know the basic plotting capabilities of base R (particularly the plot function, which has been extended by many packages to provide standard plots for complex objects produced by those packages). However, ggplot2 is able to produce complex graphics with automated legends in a consistent, systematic way, which provides it advantages over base graphics in many contexts.\nTable 2.2 provides an overview of different plots types that can be used to summarize data, the type of data being summarized, whether the plot is for univariate (one variable), bivariate (two variable), or multivariate (3 or more variables) data, the base R functions used to create the plot, and the main ggplot2 functions needed to create the plot. The table is not intended to be an exhaustive list of useful graphics we should use for data exploration.\n\n\n\n\nTable 2.2: A summary of common plot types used to explore data, the type of variable(s) they summarize, the number of variables summarized, and the base R and ggplot2 functions used to create the plot.\n\n\n\n\n\n\n\n\n\n\n\n\n\nplot type\nvariable types\nnumber of variables\nbase R\nggplot2\n\n\n\n\nbox plot\nnumeric\nunivariate\nboxplot\ngeom_boxplot\n\n\nhistogram\nnumeric\nunivariate\nhist\ngeom_histogram\n\n\ndensity plot\nnumeric\nunivariate\nplot, density\ngeom_density\n\n\nbar plot\nfactor\nunivariate\nplot or barplot, table\ngeom_bar\n\n\nscatter plot\n2 numeric\nbivariate\nplot\ngeom_point\n\n\nparallel box plot\n1 numeric, 1 factor\nbivariate\nplot or boxplot\ngeom_boxplot\n\n\ngrouped scatter plot\n2 numeric, 1 factor\nmultivariate\nplot\ngeom_point\n\n\nfacetted plots\nmixed\nmultivariate\nnone\nfacet_wrap or facet_grid\n\n\ninteractive plots\nmixed\nmultivariate\nnone\nplotly::ggplotly\n\n\n\n\n\n\n\n\n\n2.4.1 The ggplot recipe\nThere are 4 main components needed to produce a graphic using ggplot2.\n\nA data frame containing our data.\n\nEach column should be a variable and each row should be an observation of data.\n\nA ggplot object.\n\nThis is initialized using the ggplot function.\n\nA geometric object.\n\nThese are called “geoms” for short.\ngeoms indicate the geometric object used to visualize the data. E.g., points, lines, polygons etc. More generally, geoms indicate the type of plot that is desired, e.g., histogram, density, or box plot, which are complex geometric objects.\n\nAn aesthetic.\n\nAn aesthetic mapping indicates what role a variable plays in the plot.\ne.g., which variable will play the “x” variable in the plot, the “y” variable in the plot, control the “color” of the observations, etc.\n\n\nWe add “layers” of information to a ggplot, such as geoms, scales, or other customizations, using +.\n\n\n2.4.2 Univariate plots\nA univariate plot is a plot that only involves a single variable. Examples include bar plots, box plots, histograms, density plots, dot plots, pie charts, etc. (the last two are are generally poor choices.)\n\n2.4.2.1 Bar plots\nA bar plot (or bar chart) displays the number or proportion of observations in each category of a categorical variable (or using R terminology, each level of a factor variable).\nWhat are we looking for when we create a bar plot? Generally, we are interested in categories that have substantially more or fewer observations than the other categories.\nThe simplest way to create a bar plot in base R is using the plot function on a factor variable. In the code below, we create a bar plot for the island variable of penguins_clean. We use the main argument to add a title to the plot.\n\nplot(penguins_clean$island, main = \"distribution of island\")\n\n\n\n\n\n\n\n\nWe see that there are approximately 170 penguins from Biscoe island, 125 from Dream island, and 50 from Torgersen island.\nAlternatively, we can combine barplot with the table function. We do so below for the sex variable. To account for NAs in the sex variable, we specify useNA = \"ifany\" in the table function. Also, we specify names.arg = ... to specify the bar names, otherwise the bar for NA will be blank.\n\nbarplot(table(penguins_clean$sex, useNA = \"ifany\"), names.arg = c(\"female\", \"male\", \"NA\"))\n\n\n\n\n\n\n\n\nWe see that approximately 48% of the penguins are female, 49% are male, and 3% have an unknown sex.\nTo create a relative frequency bar plot, we divide the results of table by the number of relevant observations. For this particular example, we could use the code below. We use the length function to determine the number of observations to divide the counts by.\n\nbarplot(table(penguins_clean$sex, useNA = \"ifany\") / length(penguins_clean$sex),\n        names.arg = c(\"female\", \"male\", \"NA\"))\n\n\n\n\n\n\n\n\nTo create a bar plot with ggplot2, we first create a basic ggplot object containing our data frame. We must make sure to load the ggplot2 package prior to creating the plot, otherwise we’ll get errors!\n\n# load ggplot2 package\nlibrary(ggplot2)\n# create generic ggplot object with our data frame\ngg_penguin &lt;- ggplot(data = penguins_clean)\n\ngg_penguin is a minimal ggplot object with the raw information needed to produce future graphics. To create a bar plot, we add the geom geom_bar and map the species variable (in this example) to the x aesthetic using the aes function.\n\n# create bar plot for species variable\ngg_penguin + geom_bar(aes(x = species))\n\n\n\n\n\n\n\n\nWe see that there are about 150 Adelie penguins, 70 Chinstrap penguins, and 125 Gentoo penguins.\n\n\n2.4.2.2 Box plots\nA box plot is a simple graphic showing critical quantiles of a numeric variable, as well as outliers. A box plot indicates the median, 0.25 quantile (Q1), and 0.75 quantile (Q3) of the sample data and extends bars to the largest and smallest observations that are not outliers. Outliers are usually marked with stars or dots. The standard definition of an outlier in the context of box plots is any value that is more than Q3 + 1.5 (Q3 - Q1) and less than Q1 - 1.5 (Q3 - Q1). The box of a box plot extends from Q1 to Q3, with a line in the box indicating the median.\nBox plots are useful for identifying outliers and skewness in the variable. However, box plots throw away a lot of information, so we must be cautious in making conclusions about skewness and modality without creating a histogram or density plot of the data.\nThe boxplot function is the easiest approach for producing a box plot using base R. We do so for the body_mass variable below, using the main argument to customize the title of the plot.\n\nboxplot(penguins_clean$body_mass, main = \"distribution of body mass\")\n\n\n\n\n\n\n\n\nThe body_mass variable doesn’t have any outliers. It has perhaps a slight positive skew since the upper tail and upper part of the box are longer than the lower tail and lower part of the box. The median body_mass value is a bit more than 4,000 grams, while 50% of the body_mass values are approximately between 3,500 and 4,750 grams (i.e., between Q1 and Q3). The lightest penguin is less than 3,000 grams, while the heaviest penguin is more than 6,000 grams.\nTo create a box plot using ggplot2, we use geom_boxplot. We create a box plot for the bill_length variable below. We map bill_length to the y aesthetic so that we get a vertically-oriented box plot (mapping it to x will produce a horizontal box plot).\n\ngg_penguin + geom_boxplot(aes(y = bill_length))\n\n\n\n\n\n\n\n\nThere are no bill_length outliers. The minimum bill length is approximately 32 mm and the maximum is almost 60 mm. Q1 is approximately 39 mm, the median is approximately 44 mm, and Q3 is approximately 48 mm. It is difficult to assess the skewness of this data. The upper tail is longer than the shorter tail, but the upper box is shorter than the lower box, so the evidence is inconsistent.\n\n\n2.4.2.3 Histograms\nA histogram displays the distribution of a numeric variable. A histogram counts the number of values falling into (usually) equal-sized “bins” running from the smallest value to the largest value. The number of bins and width of the bins affect the shape of the histogram.\nHistograms are used to assess skewness, modality (the number of clear “peaks” in the plot), and to some extent, outliers.\nThe hist function creates a histogram of a numeric variable. We create a histogram of bill_length in the code below.\n\nhist(penguins_clean$bill_length)\n\n\n\n\n\n\n\n\nThe title and x-axis label are visually unappealing, so we use the main and xlab arguments to change them to a blank title and bill length (mm), respectively. We also increase the number of bins using the breaks argument.\n\nhist(penguins_clean$bill_length, main = \"\", xlab = \"bill length (mm)\", breaks = 20)\n\n\n\n\n\n\n\n\nThis distribution of bill_length is bimodal (has two prominent peaks or modes). That is why the box plot of bill_length provided inconsistent evidence of skewness. This also demonstrates why we should not draw conclusions about modality or skewness from numeric summaries alone.\nWe use geom_histogram to create a histogram using ggplot2, mapping the variable of interest to the x aesthetic. We do so for the flipper_length variable below.\n\ngg_penguin + geom_histogram(aes(x = flipper_length))\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\nWarning: Removed 2 rows containing non-finite outside the scale range\n(`stat_bin()`).\n\n\n\n\n\n\n\n\n\nFlipper length has a bimodal distribution, with prominent peaks centered around 190 and 220 mm.\n\n\n2.4.2.4 Density plots\nA density plot is similar to a smoothed histogram. The area under a density curve must equal 1. In general, density plots are more visually appealing than histograms, but both communicate similar information. However, density plots can sometimes have problems near the edges of a variable with a fixed upper or lower bound because it is difficult to know how to smooth the data in that case.\nWe combine the plot and density functions to construct a density plot using base R. We do that for the bill_depth variable below. Note the use of na.rm to remove NA values that would otherwise poison the density calculation, and use the main argument to create a blank title.\n\nplot(density(penguins_clean$bill_depth, na.rm = TRUE), main = \"\")\n\n\n\n\n\n\n\n\nThe bill_depth variable is bimodal with peaks around 14 mm and 18 mm. The graphic also indicates that 342 observations were used to estimate the density and the bandwidth parameter was 0.5533. The bandwidth parameter controls the amount of smoothing and can be changed. Run ?stats::density in the Console for more details.\nWe create a density plot with ggplot2 using geom_density. We do so for the body_mass variable, mapping it to the x aesthetic.\n\ngg_penguin + geom_density(aes(x = body_mass))\n\n\n\n\n\n\n\n\nThe body_mass variable is unimodal, with a peak around 3,700 grams. It is also positively skewed.\n\n\n\n2.4.3 Bivariate plots\nA bivariate plot is a plot involving two variables. A bivariate plot can involve more than one data type.\n\n2.4.3.1 Scatter plots\nScatter plots can be used to examine the relationship between two numeric variables.\nWe use the plot function to create a scatter plot of bill_length versus body_mass (the y variable versus the x variable) using base R below. The plot function is very flexible and can be used in multiple ways to produce a scatter plot, but we will use the formula method that takes a formula describing the variables (y ~ x) and the data frame from which the variables come.\n\n# xlab and ylab are used to customize the x-axis and y-axis labels\nplot(bill_length ~ body_mass, data = penguins_clean,\n     xlab = \"body mass (g)\", ylab = \"bill length (mm)\")\n\n\n\n\n\n\n\n\nOverall, there is a positive linear relationship between body_mass and bill_length. As body_mass increases, bill_length tends to increase. However, there is a group of points in the upper left part of the scatter plot that don’t fall into the linear pattern quite as well.\nThe geom_point function is used to create a scatter plot with ggplot2. We map the variables to be plotted to the x and y aesthetics. We create a scatter plot of bill_length versus bill_depth using ggplot2 below.\n\ngg_penguin + geom_point(aes(x = bill_depth, y = bill_length))\n\nWarning: Removed 2 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\n\nThe plot of bill_length versus bill_depth doesn’t reveal a clear pattern about their association.\n\n\n2.4.3.2 Parallel box plots\nA parallel box plot displays the distribution of a numeric variable split by the level of a factor variable. Specifically, the values of the numeric variable are grouped based on the associated level of a factor variable, and then a box plot is drawn for each group of numeric values. Parallel box plots are useful for determining if the distribution of a numeric variable substantially changes based on whether an observation has a certain level of a factor.\nWe use the plot function to draw a parallel box plot by associating a numeric variable with a factor variable. Specifically, we use the formula argument of the plot function using the syntax numeric variable ~ factor variable to create a set of vertically-oriented box plots for each level of the factor variable. We can reverse the roles of the variables in the formula to get a horizontal box plots. We create parallel box plots of body_mass versus sex below.\n\nplot(body_mass ~ sex, data = penguins_clean)\n\n\n\n\n\n\n\n\nWe can see that the body_mass values tend to be larger for the male penguins compared to the female penguins.\nWe can produce something similar with ggplot2 by mapping the numeric variable to the y aesthetic and the factor variable to the x aesthetic inside geom_boxplot. We do so below to compare bill_length for the different penguin species.\n\ngg_penguin + geom_boxplot(aes(x = species, y = bill_length))\n\n\n\n\n\n\n\n\nBased on the parallel box plot of bill_length, we see that the Chinstrap penguins tend to have slightly larger bill lengths than the Gentoo penguins, which typically have larger bill lengths than the Adelie penguins.\n\n\n\n2.4.4 Multivariate plots\nA multivariate plot displays relationships between two or more variables (so bivariate plots are technically multivariate plots). We focus on creating multivariate plots with ggplot2. While the same graphics can be created with base R, it is easier to create informative multivariate graphics with ggplot2.\n\n2.4.4.1 Grouped scatter plot\nA grouped scatter plot is a scatter plot that uses colors or symbols (or both) to indicate the level of a factor variable that each point corresponds to. We can actually use more than one factor variable, but interpreting the plot becomes more difficult. The most common way to create a grouped scatter plot with ggplot2 is to map a factor variable to the color or shape aesthetic of geom_point. ggplot2 will automatically map the factor variable to unique colors or shapes and then describe the mapping in a legend (this process is known as “scaling”). In the example below, we create a scatter plot of flipper_length versus body_mass that distinguishes the different species using color.\n\ngg_penguin + geom_point(aes(x = body_mass, y = flipper_length, color = species))\n\n\n\n\n\n\n\n\nThe flipper length and body mass of Gentoo penguins tend to be noticeably larger than the other two species. Chinstrap and Adelie penguins tend to have similar flipper length and body mass, with Chinstrap penguins tending to have slightly longer flipper length.\nWe can improve the graphic above can be made better in two ways. 1. Using better colors, and 2. Using more than one visual approach to distinguish the levels of the factor variable. The grouped scatter plot uses both red and green colors, which may be difficult to distinguish for individuals with certain forms of colorblindness. We should use a more friendly color palette. An excellent resource for choosing a color palette is https://colorbrewer2.org (Brewer (2022)). The Color Brewer website simplifies choosing a color palette based on certain desired characteristics such as whether the palette is colorblind-friendly, printer friendly, etc. The recommend palettes can be accessed using the scale_color_brewer function. We use a colorblind-friendly palette below. We also changes the x-axis label, y-axis label, and title using the xlab, ylab, and ggtitle functions respectively.\n\ngg_penguin +\n  geom_point(aes(x = body_mass, y = flipper_length, color = species, shape = species)) +\n  scale_color_brewer(type = \"qual\", palette = \"Dark2\") +\n  xlab(\"body mass (g)\") + ylab(\"flipper length (mm)\") +\n  ggtitle(\"body mass versus flipper length by species\")\n\n\n\n\n\n\n\n\n\n\n2.4.4.2 Facetted plots (and alternatives)\nFacetting creates separate panels (facets) of plots based on one or more facetting variables. The key functions to do this with ggplot2 are the facet_grid and facet_wrap functions. facet_grid is used to create a grid of plots based on one or two factor variables, while facet_wrap wraps facets of panels around the plot. Below, we facet scatter plots of bill_length versus bill_depth by species.\n\ngg_penguin + geom_point(aes(x = bill_depth, y = bill_length)) + facet_grid(~ species)\n\n\n\n\n\n\n\n\nWhereas we previously couldn’t discern a clear relationship between bill length and depth based on a single scatter plot, facetting by species makes it clear there is a positive relationship between bill_length and bill_depth after accounting for species. We could have used a grouped scatter plot for the same purpose.\nA simpler facetting example would be to facet density plots of body_mass by sex as shown below.\n\ngg_penguin + geom_density(aes(x = body_mass)) + facet_grid(~sex)\n\n\n\n\n\n\n\n\nThis plot is a bit difficult to interpret. We see that body mass is bimodal for the males and females. Perhaps this is related to species. Since the density plots are in different panels, its a bit tricky to see how they relate to each other. Also, the NA panel is probably not needed.\nTo get rid of the NA panel, we need to remove all of the observations with NA values. We do this below, using subset to select the desired columns and then using na.omit to remove any rows that have NA values for body_mass, sex, or species. Note that order matters here because na.omit removes any observation of the data frame that has an NA value. We save the filtered object as penguins_temp.\n\npenguins_temp &lt;-\n  penguins_clean |&gt;\n  subset(select = c(body_mass, sex, species)) |&gt;\n  na.omit()\n\nIn the next graphic, we create density plots of the body_mass variable. However, we use the fill aesthetic to scale the sex variable so that the we distinguish the densities of male and female penguins with different colors. We set the alpha argument to 0.5 OUTSIDE the aes function (because it is being manually specified) so that the colors are translucent and blend. We also facet by species to see what the patterns look like for the different species.\n\nggplot(data = penguins_temp) +\n  geom_density(aes(x = body_mass, fill = sex), alpha = 0.5) +\n  facet_grid(~ species)\n\n\n\n\n\n\n\n\nWe see that for all species, the body mass of the males tends to be larger than the females.\nThe examples above provide a small taste of the complex graphics we can create with ggplot2 using only a few lines of code.\n\n\n2.4.4.3 Interactive graphics\nThere are many tools for creating interactive graphics in R. We have found the ggiraph package (Gohel and Skintzos 2024) useful for creating interactive graphics based on ggplot2. However, it is a bit too complex to discuss here.\nThe plotly package (Sievert et al. 2024) provides the capabilities of plotly (Plotly Technologies Inc. 2015, https://plotly.com/), a well-known tool for creating interactive scientific plots, through R. The ggplotly function will instantly make a ggplot interactive (though we may need to customize it for our needs). We provide two examples below.\nFirst, we load the plotly package to have access to the ggplotly function. We then take our previous grouped scatter plot that plotted flipper_length versus body_mass while distinguishing by species and assign it the name ggi. We then use the ggplotly function to make the graphic interactive. When we hover over a point, the plot interactively provides the exact body_mass value, flipper_length value, and species of the observation.\n\n# load plotly package\nlibrary(plotly)\n# assign grouped scatter plot name\nggi &lt;-\n  gg_penguin +\n  geom_point(aes(x = body_mass, y = flipper_length, color = species, shape = species)) +\n  scale_color_brewer(type = \"qual\", palette = \"Dark2\") +\n  xlab(\"body mass (g)\") + ylab(\"flipper length (mm)\") +\n  ggtitle(\"body mass versus flipper length by species\")\n# make plot interactive\nggplotly(ggi)\n\n\n\n\n\nIn the next example, we make interactive parallel box plots of bill_length that distinguish between species.\n\n# assign parallel box plot name\nggi2 &lt;- gg_penguin + geom_boxplot(aes(x = species, y = bill_length))\n# make plot interactive\nggplotly(ggi2)\n\n\n\n\n\nThe interactive parallel box plot provides information about the box plot characteristics of each species (such as the minimum bill_length, Q1, median, Q3, etc.)",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Data Cleaning and Exploration</span>"
    ]
  },
  {
    "objectID": "data-exploration.html#a-plan-for-data-cleaning-and-exploration",
    "href": "data-exploration.html#a-plan-for-data-cleaning-and-exploration",
    "title": "2  Data Cleaning and Exploration",
    "section": "2.5 A plan for data cleaning and exploration",
    "text": "2.5 A plan for data cleaning and exploration\nWe have provided many examples of data cleaning and exploration using the penguins_raw data. The analysis above is NOT exhaustive, and there are many additional numeric and visual summaries we could consider. In what follows, we summarize a basic plan below for initial data cleaning and exploration that we have found useful. We assume that we are working with a data frame, which is the most common data structure used for data analysis in R.\n\nImport or create the data set.\nUse the str function to get an idea of the initial structure. This can help to identify clear issues we may have had in importing the data, problems with variable names and types, etc.\nClean the variable names based on our preferences.\nConvert the variables to the appropriate type (e.g., categorical variables to factor).\nRun the summary function on the data frame. Take note of NAs, impossible values that are data entry errors, etc. We may need to perform additional cleaning based on the information learned in this step.\nCompute any additional numeric summaries of the different variables, as desired.\nCreate univariate plots of all variables being considered. Use histograms for discrete numeric variables, density plots for continuous numeric variables, and bar plots for factor variables. Take note of any interesting patterns such as modality, skewness, overall shape, outliers, etc.\nCreate bivariate plots for any pairs of variables. Use scatter plots for two numeric variables. Use parallel box plots for numeric and factor variables, or perhaps create histogram plots of the numeric variable facetted by the factor variable, or density plots of the numeric variables filled with different colors by the factor variable. Once again, notice any patterns.\nCreate multivariate and interactive graphics based on what we learn in the previous steps.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Data Cleaning and Exploration</span>"
    ]
  },
  {
    "objectID": "data-exploration.html#final-notes-on-missing-or-erroneous-data",
    "href": "data-exploration.html#final-notes-on-missing-or-erroneous-data",
    "title": "2  Data Cleaning and Exploration",
    "section": "2.6 Final notes on missing or erroneous data",
    "text": "2.6 Final notes on missing or erroneous data\nWhat should we do with our data when observations are missing information or the information is clearly erroneous?\nIf the data are clearly erroneous, attempt to get the correct value. If the values cannot be corrected, replace them with NA since we don’t have that information.\nWhat should we do about NAs? There are many approaches for dealing with NAs. The proper approach depends a lot on why the data are missing. Speaking informally, if there is no systematic reason causing the data to be missing, then ignoring the observations with missing data isn’t a terrible approach. However, if there is a systematic reason behind why the data are missing (such as individuals not wanting to answer a sensitive question, subjects dying for a specific reason, etc.) then ignoring that data can lead to erroneous conclusions.\nIn what follows, we will assume our missing data problem is not systematic and ignore missing values.\n\n\n\n\nBrewer, Cynthia A. 2022. “ColorBrewer2.org.” https://colorbrewer2.org.\n\n\nGohel, David, and Panagiotis Skintzos. 2024. Ggiraph: Make Ggplot2 Graphics Interactive. https://davidgohel.github.io/ggiraph/.\n\n\nGorman, Kristen B., Tony D. Williams, and William R. Fraser. 2014. “Ecological Sexual Dimorphism and Environmental Variability Within a Community of Antarctic Penguins (Genus Pygoscelis).” PLOS ONE 9 (3): 1–14. https://doi.org/10.1371/journal.pone.0090081.\n\n\nHorst, Allison, Alison Hill, and Kristen Gorman. 2022. Palmerpenguins: Palmer Archipelago (Antarctica) Penguin Data. https://allisonhorst.github.io/palmerpenguins/.\n\n\nMüller, Kirill, and Hadley Wickham. 2023. Tibble: Simple Data Frames. https://tibble.tidyverse.org/.\n\n\nPlotly Technologies Inc. 2015. “Collaborative Data Science.” Montreal, QC: Plotly Technologies Inc. 2015. https://plot.ly.\n\n\nSievert, Carson, Chris Parmer, Toby Hocking, Scott Chamberlain, Karthik Ram, Marianne Corvellec, and Pedro Despouy. 2024. Plotly: Create Interactive Web Graphics via Plotly.js. https://plotly-r.com.\n\n\nWickham, Hadley. 2022. The Tidyverse Style Guide. https://style.tidyverse.org/.\n\n\nWickham, Hadley, Winston Chang, Lionel Henry, Thomas Lin Pedersen, Kohske Takahashi, Claus Wilke, Kara Woo, Hiroaki Yutani, Dewey Dunnington, and Teun van den Brand. 2024. Ggplot2: Create Elegant Data Visualisations Using the Grammar of Graphics. https://ggplot2.tidyverse.org.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Data Cleaning and Exploration</span>"
    ]
  },
  {
    "objectID": "parameter-estimation-and-model-fitting.html",
    "href": "parameter-estimation-and-model-fitting.html",
    "title": "3  Parameter Estimation and Model Fitting",
    "section": "",
    "text": "3.1 What is regression?\nPearson and Lee (1897) and Pearson and Lee (1903) collected a classical data set that measures the heights of mothers and their adult daughters. Figure 3.1 displays a scatter plot of 5 randomly selected observations from that data set. Is it be reasonable to use a mother’s height to predict the height of her adult daughter?\nFigure 3.1: A scatter plot displaying pairs of heights for mothers and their adult daughters.\nA regression model is a model describing the typical relationship between a set of variables. A regression analysis is the process of building a regression model using a set of variables based on \\(n\\) observations of these variables sampled from a population. In the present context, we want to model the height of adult daughters using the height of their mothers.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Parameter Estimation and Model Fitting</span>"
    ]
  },
  {
    "objectID": "parameter-estimation-and-model-fitting.html#sec-what-is-regression",
    "href": "parameter-estimation-and-model-fitting.html#sec-what-is-regression",
    "title": "3  Parameter Estimation and Model Fitting",
    "section": "",
    "text": "3.1.1 Response versus predictor variables\nThe variables in a regression analysis may be divided into two types: the response variable and the predictor variables.\nThe response variable is the outcome variable want to predict. It is also known as the outcome, output, or dependent variable. The response variable is denoted by \\(Y\\). The observed value of \\(Y\\) for observation \\(i\\) is denoted by \\(Y_i\\).\nPredictors variables are the variables available to model the response variable. Predictor variables are also known as explanatory, regressor, input, or independent variables, or simply as features. Following the convention of Weisberg (2014), we use the term regressor to refer to the variables used in our regression model, whether that is the original predictor variable, some transformation of a predictor, some combination of predictors, etc. Thus, every predictor can be a regressor but not all regressors are a predictor. The regressor variables are denoted by \\(X_1, X_2, \\ldots, X_{p-1}\\). The value of \\(X_j\\) for observation \\(i\\) is denoted by \\(x_{i,j}\\). If there is only a single regressor in the model, we can denote the single regressor as \\(X\\) and the observed values of \\(X\\) as \\(x_1, x_2, \\ldots, x_n\\). For the height data, the 5 pairs of observed data are denoted \\[\n(x_1, Y_1), (x_2, Y_2), \\ldots, (x_5, Y_5),\n\\] with \\((x_i, Y_i)\\) denoting the data for observation \\(i\\). In our height example shown in Figure 3.1, \\(x_i\\) denotes the mother’s height for observation \\(i\\) and \\(Y_i\\) denotes the daughter’s height for observation \\(i\\). Using the data provided in Table 3.1, we see that \\(x_3 = 63.5\\) and \\(Y_5 = 66.5\\).\n\n\n\n\nTable 3.1: The height (in) of the mothers and daughters displayed in Figure 3.1.\n\n\n\n\n\n\nobservation\nmother\ndaughter\n\n\n\n\n1\n57.5\n61.5\n\n\n2\n60.5\n63.5\n\n\n3\n63.5\n63.5\n\n\n4\n66.5\n66.5\n\n\n5\n69.5\n66.5\n\n\n\n\n\n\n\n\n\n\n3.1.2 Selecting the best model\nSuppose we want to find the straight line that best fits the points in the plot of mother and daughter heights in Figure 3.1. How do we determine the “best fitting” model? Consider Figure 3.2, in which 2 potential “best fitting” lines are drawn on the scatter plot of the height data. Which one is best?\n\n\n\n\n\n\n\n\nFigure 3.2: Comparison of two potential fitted models to some observed data. The fitted models are shown in grey.\n\n\n\n\n\nThe rest of this chapter focuses on defining and estimating the parameters of a linear regression model. We will start with the simplest type of linear regression, called simple linear regression, which only uses a single regressor variable to model the response. We will then consider more complicated linear regression models. After that, we learn how to evaluate how well an estimated regression model fits the data. We conclude with a summary of some important concepts from the chapter.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Parameter Estimation and Model Fitting</span>"
    ]
  },
  {
    "objectID": "parameter-estimation-and-model-fitting.html#sec-slr-estimation",
    "href": "parameter-estimation-and-model-fitting.html#sec-slr-estimation",
    "title": "3  Parameter Estimation and Model Fitting",
    "section": "3.2 Estimation of the simple linear regression model",
    "text": "3.2 Estimation of the simple linear regression model\nParameter estimation is the process of using observed data to estimate model parameters. There are many different methods of parameter estimation in statistics: method-of-moments, maximum likelihood, Bayesian, etc. The most common parameter estimation method for linear models is the least squares method, which is commonly called Ordinary Least Squares (OLS) estimation. OLS estimation estimates the regression coefficients with the values that minimize the residual sum of squares (RSS), which we will define shortly.\n\n3.2.1 Defining a simple linear regression model\nThe regression model for \\(Y\\) as a function of \\(X\\), denoted \\(E(Y \\mid X)\\), is the expected value of \\(Y\\) conditional on the value of regressor \\(X\\). Thus, a regression model specifically refers to the expected relationship between the response and regressors.\nThe simple linear regression model assumes the mean of the response variable \\(Y\\), conditional on a single regressor \\(X\\), is \\[\nE(Y\\mid X) = \\beta_0 + \\beta_1 X.\n\\tag{3.1}\\]\nThe response variable \\(Y\\) is modeled as \\[\n\\begin{aligned}\nY &= E(Y \\mid X) + \\epsilon \\\\\n&= \\beta_0 + \\beta_1 X + \\epsilon,\n\\end{aligned}\n\\tag{3.2}\\] where \\(\\epsilon\\) is the model error.\nThe error term \\(\\epsilon\\) is literally the deviation of the response variable from its mean. We typically assume that conditional on the regressor variable, the error term has mean 0 and variance \\(\\sigma^2\\), which can be written as \\[\nE(\\epsilon \\mid X) = 0\n\\tag{3.3}\\] and \\[\n\\mathrm{var}(\\epsilon \\mid X) = \\sigma^2.\n\\tag{3.4}\\] Using the response values \\(Y_1, \\ldots, Y_n\\) and their associated regressor values \\(x_1, \\ldots, x_n\\), the observed data are modeled as \\[\n\\begin{aligned}\nY_i &= \\beta_0 + \\beta_1 x_i + \\epsilon_i \\\\\n&= E(Y\\mid X = x_i) + \\epsilon_i,\n\\end{aligned}\n\\] for \\(i=1, 2, \\ldots, n\\), where \\(\\epsilon_i\\) denotes the error for observation \\(i\\).\n\n\n3.2.2 Important terminology\nThe estimated regression model or fitted model is defined as \\[\n\\hat{E}(Y|X) = \\hat{\\beta}_0 + \\hat{\\beta}_1 X,\n\\] where \\(\\hat{\\beta}_0\\) and \\(\\hat{\\beta}_1\\) denote the estimated values of our regression parameters.\nThe \\(i\\)th fitted value is defined as \\[\n\\hat{Y}_i = \\hat{E}(Y|X = x_i) = \\hat{\\beta}_0 + \\hat{\\beta}_1 x_i.\n\\tag{3.5}\\] Thus, the \\(i\\)th fitted value is the estimated mean of \\(Y\\) when the regressor \\(X=x_i\\). More specifically, the \\(i\\)th fitted value is the estimated mean response based on the regressor value observed for the \\(i\\)th observation.\nThe \\(i\\)th residual is defined as \\[\n\\hat{\\epsilon}_i = Y_i - \\hat{Y}_i.\n\\tag{3.6}\\] The \\(i\\)th residual is the difference between the response and estimated mean response of observation \\(i\\).\nThe residual sum of squares (RSS) of a regression model is the sum of its squared residuals, which we define as \\[\nRSS = \\sum_{i=1}^n \\hat{\\epsilon}_i^2.\n\\tag{3.7}\\]\nThere are many equivalent expressions for the RSS. Notably, in the context of simple linear regression, to emphasize the dependence of the RSS on the estimated regression coefficients, \\(\\hat{\\beta}_0\\) and \\(\\hat{\\beta}_1\\), Equation 3.7 can be rewritten using Equation 3.5 and Equation 3.6 as \\[\n\\begin{aligned}\nRSS(\\hat{\\beta}_0, \\hat{\\beta}_1) &= \\sum_{i=1}^n \\hat{\\epsilon}_i^2 \\\\\n&= \\sum_{i=1}^n (Y_i - \\hat{Y}_i)^2 & \\\\\n&= \\sum_{i=1}^n (Y_i - \\hat{E}(Y|X=x_i))^2 \\\\\n&= \\sum_{i=1}^n (Y_i - (\\hat{\\beta}_0 + \\hat{\\beta}_1 x_i))^2.\n\\end{aligned}\n\\tag{3.8}\\]\nThe fitted model is the estimated model that minimizes the RSS and is written as \\[\n\\hat{Y}=\\hat{E}(Y|X) = \\hat{\\beta}_0 + \\hat{\\beta}_1 X.\n\\tag{3.9}\\] Both \\(\\hat{Y}\\) and \\(\\hat{E}(Y|X)\\) are used to denote a fitted model. \\(\\hat{Y}\\) is used for brevity, while \\(\\hat{E}(Y|X)\\) is used for clarity. In a simple linear regression context, the fitted model is known as the line of best fit.\n\n\n3.2.3 Visualizing terms\nFigure 3.3 visualizes the response values, fitted values, residuals, and fitted model in a simple linear regression context.\n\nThe fitted model is shown as the dashed grey line and minimizes the RSS.\nThe observed values of the response variable, \\(Y\\), are shown as black dots.\nThe fitted values, shown as blue x’s, are the values returned by evaluating the fitted model at the observed regressor values.\nThe residuals, shown as solid orange lines, indicate the distance and direction between the observed responses and their corresponding fitted value. If the response is larger than the fitted value then the residual is positive, otherwise it is negative. The RSS is the sum of the squared vertical distances between the response and fitted values, i.e., the sum of the squared residuals.\n\n\n\n\n\n\n\n\n\nFigure 3.3: Visualization of the fitted model, response values, fitted values, and residuals.\n\n\n\n\n\n\n\n3.2.4 OLS estimators of the simple linear regression parameters\nThe estimators of \\(\\beta_0\\) and \\(\\beta_1\\) that minimize the RSS for a simple linear regression model can be obtained analytically using basic calculus under minimal assumptions. Specifically, the optimal analytical solutions for \\(\\hat{\\beta}_0\\) and \\(\\hat{\\beta}_1\\) are valid as long as the regressor values are not a constant value, i.e, \\(x_i \\neq x_j\\) for at least some \\(i,j\\in \\{1,2,\\ldots,n\\}\\).\nDefine \\(\\bar{x}=\\frac{1}{n}\\sum_{i=1}^n x_i\\) and \\(\\bar{Y} = \\frac{1}{n}\\sum_{i=1}^n Y_i\\). The expression \\(\\bar{x}\\) is read “x bar”, and it is the sample mean of the observed \\(x_i\\) values. The OLS estimators of the simple linear regression coefficients that minimize the RSS are \\[\n\\begin{aligned}\n\\hat{\\beta}_1 &= \\frac{\\sum_{i=1}^n x_i Y_i - \\frac{1}{n} \\biggl(\\sum_{i=1}^n x_i\\biggr)\\biggl(\\sum_{i=1}^n Y_i\\biggr)}{\\sum_{i=1}^n x_i^2 - \\frac{1}{n} \\biggl(\\sum_{i=1}^n x_i\\biggr)^2} \\\\\n&= \\frac{\\sum_{i=1}^n (x_i - \\bar{x})(Y_i - \\bar{Y})}{\\sum_{i=1}^n (x_i - \\bar{x})^2} \\\\\n&= \\frac{\\sum_{i=1}^n (x_i - \\bar{x})Y_i}{\\sum_{i=1}^n (x_i - \\bar{x})x_i}\n\\end{aligned}\n\\tag{3.10}\\] and \\[\n\\hat{\\beta}_0 = \\bar{Y} - \\hat{\\beta}_1 \\bar{x}.\n\\tag{3.11}\\] The various expressions given in Equation 3.10 are equivalent. In fact, in Equation 3.10, all of the numerators are equivalent, and all of the denominators are equivalent. We provide derivations of the estimators for \\(\\hat{\\beta}_0\\) and \\(\\hat{\\beta}_1\\) in Section 3.5.2.\nIn addition to the regression coefficients, the other parameter we mentioned in Section 3.2.1 is the error variance, \\(\\sigma^2\\). The most common estimator of the error variance is \\[\n\\hat{\\sigma}^2 = \\frac{RSS}{\\mathrm{df}_{RSS}}.\n\\tag{3.12}\\] where \\(\\mathrm{df}_{RSS}\\) is the degrees of freedom of the RSS. In a simple linear regression context, the denominator of Equation Equation 3.12) is \\(n-2\\). ?sec-degrees-of-freedom provides additional details about degrees of freedom.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Parameter Estimation and Model Fitting</span>"
    ]
  },
  {
    "objectID": "parameter-estimation-and-model-fitting.html#sec-penguins-slr",
    "href": "parameter-estimation-and-model-fitting.html#sec-penguins-slr",
    "title": "3  Parameter Estimation and Model Fitting",
    "section": "3.3 Penguins simple linear regression example",
    "text": "3.3 Penguins simple linear regression example\nWe will use the penguins data set in the palmerpenguins package (Horst, Hill, and Gorman 2022) to illustrate a very basic simple linear regression analysis.\nThe penguins data set provides data related to various penguin species measured in the Palmer Archipelago (Antarctica), originally provided by Gorman, Williams, and Fraser (2014). The data set includes 344 observations of 8 variables. The variables are:\n\nspecies: the penguin species (factor).\nisland: the island on which the penguin was observed (factor).\nbill_length_mm: the penguin’s bill length in millimeters (numeric).\nbill_depth_mm: the penguin’s bill depth in millimeters (numeric).\nflipper_length_mm: the penguin’s flipper length in millimeters (integer).\nbody_mass_g: the penguin’s body mass in grams (integer).\nsex: the penguin’s sex (factor).\nyear: the study year the penguin was observed (integer).\n\nWe start by loading the data into memory.\n\ndata(penguins, package = \"palmerpenguins\")\n\nWe use the head function to examine the first six rows of the data frame. We see that some observations have missing values.\n\nhead(penguins)\n\n# A tibble: 6 × 8\n  species island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n  &lt;fct&gt;   &lt;fct&gt;              &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt;\n1 Adelie  Torgersen           39.1          18.7               181        3750\n2 Adelie  Torgersen           39.5          17.4               186        3800\n3 Adelie  Torgersen           40.3          18                 195        3250\n4 Adelie  Torgersen           NA            NA                  NA          NA\n5 Adelie  Torgersen           36.7          19.3               193        3450\n6 Adelie  Torgersen           39.3          20.6               190        3650\n# ℹ 2 more variables: sex &lt;fct&gt;, year &lt;int&gt;\n\n\nWe begin by creating a scatter plot of bill_length_mm versus body_mass_g (y-axis versus x-axis) in Figure 3.4.\n\nplot(bill_length_mm ~ body_mass_g, data = penguins,\n     ylab = \"bill length (mm)\", xlab = \"body mass (g)\",\n     main = \"Penguin size measurements\")\n\n\n\n\n\n\n\nFigure 3.4: A scatter plot of penguin bill length (mm) versus body mass (g).\n\n\n\n\n\nWe see a clear positive association between body mass and bill length: as the body mass increases, the bill length tends to increase. The pattern is linear, i.e., roughly a straight line.\nWe will build a simple linear regression model that regresses bill_length_mm on body_mass_g. More specifically, we want to estimate the parameters of the regression model \\(E(Y \\mid X) = \\beta_0 + \\beta_1X\\), with \\(Y=\\mathtt{bill\\_length\\_mm}\\) and \\(X=\\mathtt{body\\_mass\\_g}\\), i.e., we want to estimate the parameters of the model \\[\nE(\\mathtt{bill\\_length\\_mm}\\mid \\mathtt{body\\_mass\\_g})=\\beta_0+\\beta_1\\mathtt{body\\_mass\\_g}.\n\\]\nThe lm function uses OLS estimation to fit a linear model to data. The function has two main arguments:\n\nformula: a Wilkinson and Rogers (1973) style formula describing the linear regression model. For complete details, run ?stats::formula in the Console. If y is the response variable and x is an available numeric predictor, then formula = y ~ x tells lm to fit the simple linear regression model \\(E(Y|X)=\\beta_0+\\beta_1 X\\).\ndata: the data frame in which the model variables are stored. This can be omitted if the variables are already stored in memory.\n\nWe use the code below to fit a linear model regressing bill_length_mm on body_mass_g using the penguins data frame and assign the result the name lmod. lmod is an object of class lm.\n\nlmod &lt;- lm(bill_length_mm ~ body_mass_g, data = penguins) # fit model\nclass(lmod) # class of lmod\n\n[1] \"lm\"\n\n\nThe summary function summarizes the results of a fitted model. When an lm object is supplied to the summary function, it returns:\n\nCall: the function call used to fit the model.\nResiduals: A 5-number summary of \\(\\hat{\\epsilon}_1, \\ldots, \\hat{\\epsilon}_n\\).\nCoefficients: A table that lists:\n\nThe regressors in the fitted model.\nEstimate: the estimated coefficient of each regressor.\nStd. Error: the estimated standard error of the estimated coefficients.\nt value: the computed test statistic associated with testing \\(H_0: \\beta_j = 0\\) versus \\(H_a: \\beta_j \\neq 0\\) for each regression coefficient in the model.\nPr(&gt;|t|): the associated p-value of each test.\n\nVarious summary statistics:\n\nResidual standard error is the value of \\(\\hat{\\sigma}\\), the estimate of the error standard deviation. The degrees of freedom is \\(\\mathrm{df}_{RSS}\\), the number of observations minus the number of estimated coefficients in the model.\nMultiple R-squared is a measure of model fit discussed in Section @ref(evaluating-model-fit).\nAdjusted R-squared is a modified version of Multiple R-squared.\nF-statistic is the test statistic of the test that compares the model with an only an intercept to the fitted model. The DF (degrees of freedom) values relate to the statistic under the null hypothesis, and the p-value is the p-value of the test.\n\n\nWe use the summary function on lmod to produce the output below.\n\n# summarize results stored in lmod\nsummary(lmod)\n\n\nCall:\nlm(formula = bill_length_mm ~ body_mass_g, data = penguins)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-10.1251  -3.0434  -0.8089   2.0711  16.1109 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 2.690e+01  1.269e+00   21.19   &lt;2e-16 ***\nbody_mass_g 4.051e-03  2.967e-04   13.65   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4.394 on 340 degrees of freedom\n  (2 observations deleted due to missingness)\nMultiple R-squared:  0.3542,    Adjusted R-squared:  0.3523 \nF-statistic: 186.4 on 1 and 340 DF,  p-value: &lt; 2.2e-16\n\n\nUsing the output above, we see that the estimated parameters are \\(\\hat{\\beta}_0=26.9\\) and \\(\\hat{\\beta}_1=0.004\\). Thus, our fitted model is \\[\n\\widehat{\\mathtt{bill\\_length\\_mm}}=26.9+0.004 \\mathtt{body\\_mass\\_g}.\n\\]\nIn the context of a simple linear regression model, the intercept term is the expected response when the value of the regressor is zero, while the slope is the expected change in the response when the regressor increases by 1 unit. Thus, based on the model we fit to the penguins data, we can make the following interpretations:\n\n\\(\\hat{\\beta}_1\\): If a penguin has a body mass 1 gram larger than another penguin, we expect the larger penguin’s bill length to be 0.004 millimeters longer.\n\\(\\hat{\\beta}_0\\): A penguin with a body mass of 0 grams is expected to have a bill length of 26.9 millimeters.\n\nThe latter interpretation is nonsensical. It doesn’t make sense to discuss a penguin with a body mass of 0 grams unless we are talking about an embryo, in which case it doesn’t even make sense to discuss bill length. This is caused by the fact that we are extrapolating far outside the observed body mass values. Our data only includes information for adult penguins, so we should be cautious about drawing conclusions for penguins at other life stages.\nThe abline function can be used to automatically overlay the fitted model on the observed data. We run the code below to produce Figure 3.5. The fit of the model to our observed data seems reasonable.\n\nplot(bill_length_mm ~ body_mass_g, data = penguins, main = \"Penguin size measurements\",\n     ylab = \"bill length (mm)\", xlab = \"body mass (g)\")\n# draw fitted line on plot\nabline(lmod)\n\n\n\n\n\n\n\nFigure 3.5: The fitted model overlaid on the penguin data.\n\n\n\n\n\nR provides many additional methods (generic functions that do something specific when applied to a certain type of object) for lm objects. Commonly used ones include:\n\nresiduals: extracts the residuals, \\(\\hat{\\epsilon}_1, \\ldots, \\hat{\\epsilon}_n\\) from an lm object.\nfitted: extracts the fitted values, \\(\\hat{Y}_1, \\ldots, \\hat{Y}_n\\) from an lm object.\npredict: by default, computes \\(\\hat{Y}_1, \\ldots, \\hat{Y}_n\\) for an lm object. It can also be used to make arbitrary predictions for the lm object.\ncoef or coefficients: extracts the estimated coefficients from an lm object.\ndeviance: extracts the RSS from an lm object.\ndf.residual: extracts \\(\\mathrm{df}_{RSS}\\), the degrees of freedom for the RSS, from an lm object.\nsigma: extracts \\(\\hat{\\sigma}\\) from an lm object.\n\nWe now use some of the methods to extract important characteristics of our fitted model.\nThe coef function extracts the estimated regression coefficients, \\(\\hat{\\beta}_0\\) and \\(\\hat{\\beta}_1\\), from the fitted model.\n\n(coeffs &lt;- coef(lmod)) # extract, assign, and print coefficients\n\n (Intercept)  body_mass_g \n26.898872424  0.004051417 \n\n\nThe residuals function extracts the vector of residuals, \\(\\hat{\\epsilon}_1,\\ldots, \\hat{\\epsilon}_n\\) from the fitted model.\n\nehat &lt;- residuals(lmod) # extract and assign residuals\nhead(ehat) # first few residuals\n\n         1          2          3          5          6          7 \n-2.9916846 -2.7942554  0.2340237 -4.1762596 -2.3865430 -2.6852575 \n\n\nThe fitted function extracts the vector of fitted values, \\(\\hat{Y}_1,\\ldots, \\hat{Y}_n\\), from the fitted model.\n\nyhat &lt;- fitted(lmod) # extract and assign fitted values\nhead(yhat) # first few fitted values\n\n       1        2        3        5        6        7 \n42.09168 42.29426 40.06598 40.87626 41.68654 41.58526 \n\n\nThe predict function also extracts the vector of fitted values from the fitted model. It can be also used to predict the response of an observation for arbitrary values of the predictors.\n\nyhat2 &lt;- predict(lmod) # compute and assign fitted values\nhead(yhat2) # first few fitted values\n\n       1        2        3        5        6        7 \n42.09168 42.29426 40.06598 40.87626 41.68654 41.58526 \n\n\nThe deviance function extracts the RSS of the fitted model.\n\n(rss &lt;- deviance(lmod)) # extract, assign, and print rss\n\n[1] 6564.494\n\n\nThe df.residual function extracts the residual degrees of freedom from the fitted model.\n\n(dfr &lt;- df.residual(lmod)) # extract n - p\n\n[1] 340\n\n\nThe sigma function extracts the estimated error standard deviation, \\(\\hat{\\sigma}=\\sqrt{\\hat{\\sigma}^2}\\), from the fitted model. In the code below, we square \\(\\hat{\\sigma}\\) to estimate the error variance, \\(\\hat{\\sigma}^2\\).\n\n(sigmasqhat &lt;- sigma(lmod)^2) # estimated error variance\n\n[1] 19.30734\n\n\nFrom the output above, we that the the first 3 residuals are -2.99, -2.79, and 0.23. The first 3 fitted values are 42.09, 42.29, and 40.07. The RSS for the fitted model is 6564.49 with 340 degrees of freedom. The estimated error variance, \\(\\hat{\\sigma}^2\\), is 19.31.\nWe use the methods function to obtain a full list of methods available for lm objects using the code below.\n\nmethods(class = \"lm\")\n\n [1] add1           alias          anova          case.names     coerce        \n [6] confint        cooks.distance deviance       dfbeta         dfbetas       \n[11] drop1          dummy.coef     effects        extractAIC     family        \n[16] formula        hatvalues      influence      initialize     kappa         \n[21] labels         logLik         model.frame    model.matrix   nobs          \n[26] plot           predict        print          proj           qr            \n[31] residuals      rstandard      rstudent       show           simulate      \n[36] slotsFromS3    summary        variable.names vcov          \nsee '?methods' for accessing help and source code",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Parameter Estimation and Model Fitting</span>"
    ]
  },
  {
    "objectID": "parameter-estimation-and-model-fitting.html#defining-a-linear-model",
    "href": "parameter-estimation-and-model-fitting.html#defining-a-linear-model",
    "title": "3  Parameter Estimation and Model Fitting",
    "section": "3.4 Defining a linear model",
    "text": "3.4 Defining a linear model\n\n3.4.1 Necessary components and notation\nWe now wish to discuss linear models in a broader context. We begin by defining notation for the components of a linear model and provide some of their important properties. We repeat some of the previous discussion for clarity.\n\n\\(Y\\) denotes the response variable.\n\nThe response variable is treated as a random variable.\nWe will observe realizations of this random variable for each observation in our data set.\n\n\\(X\\) denotes a single regressor variable. \\(X_1, X_2, \\ldots, X_{p-1}\\) denote distinct regressor variables if we are performing regression with multiple regressor variables.\n\nThe regressor variables are treated as non-random variables.\nThe observed values of the regressor variables are treated as fixed, known values.\n\n\\(\\mathbb{X}=\\{X_0, X_1,\\ldots,X_{p-1}\\}\\) denotes the collection of all regressors under consideration, though this notation is really only needed in the context of multiple regression. \\(X_0\\) is usually the constant regressor 1, which is needed to include an intercept in the regression model.\n\\(\\beta_0\\), \\(\\beta_1\\), \\(\\ldots\\), \\(\\beta_{p-1}\\) denote regression coefficients.\n\nRegression coefficients are statistical parameters that we will estimate from our data.\nThe regression coefficients are treated as fixed, non-random but unknown values.\nRegression coefficients are not observable.\n\n\\(\\epsilon\\) denotes model error.\n\nThe model error is more accurately described as random variation of each observation from the regression model.\nThe error is treated as a random variable.\nThe error is assumed to have mean 0 for all values of the regressors. We write this as \\(E(\\epsilon \\mid \\mathbb{X}) = 0\\), which is read as, “The expected value of \\(\\epsilon\\) conditional on knowing all the regressor values equals 0”. The notation “\\(\\mid \\mathbb{X}\\)” extends the notation used in Equation 3.1 to multiple regressors.\nThe variance of the errors is assumed to be a constant value for all values of the regressors. We write this assumption as \\(\\mathrm{var}(\\epsilon \\mid \\mathbb{X})=\\sigma^2\\).\nThe error is not observable.\n\n\n\n\n3.4.2 Standard definition of linear model\nIn general, a linear regression model can have an arbitrary number of regressors. A multiple linear regression model has two or more regressors.\nA linear model for \\(Y\\) is defined by the equation \\[\n\\begin{aligned}\nY &= \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\cdots + \\beta_{p-1}\nX_{p-1} + \\epsilon \\\\\n&= E(Y \\mid \\mathbb{X}) + \\epsilon.\n\\end{aligned}\n\\tag{3.13}\\] We write the linear model in this way to emphasize the fact the response value equals the expected response for that combination of regressor values plus some error. It should be clear from Equation 3.13 that \\[\nE(Y \\mid \\mathbb{X}) = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\cdots + \\beta_{p-1} X_{p-1},\n\\] which we prove in Chapter 5 under mild assumptions.\nMore generally, we can say that a regression model is linear if the mean function can be written as \\[\nE(Y \\mid \\mathbb{X}) = \\sum_{j=0}^{p-1} c_j \\beta_j,\n\\tag{3.14}\\] where \\(c_0, c_1, \\ldots, c_{p-1}\\) are known functions of the regressor variables. For example, we could have \\(c_1 = X_1 X_2 X_3\\), \\(c_3 = X_2^2\\), \\(c_8 = \\ln(X_1)/X_2^2\\), etc.\nAlternatively, if \\(g_0,\\ldots,g_{p-1}\\) are functions of \\(\\mathbb{X}\\), then a linear regression model can be written as \\[\nE(Y\\mid \\mathbb{X}) = \\sum_{j=0}^{p-1} g_j(\\mathbb{X})\\beta_j.\n\\]\nThe key feature of the linear regression model is that the model is a linear combination of the regression coefficients.\n\n\n3.4.3 Examples of linear models\nA model is linear because of its form, not the shape it produces.\nSome examples of linear regression models are:\n\n\\(E(Y|X) = \\beta_0\\).\n\\(E(Y|X) = \\beta_0 + \\beta_1 X + \\beta_2 X^2\\).\n\\(E(Y|X_1, X_2) = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2\\).\n\\(E(Y|X_1, X_2) = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\beta_3 X_1 X_2\\).\n\\(E(Y|X_1, X_2) = \\beta_0 + \\beta_1 \\ln(X_1) + \\beta_2 X_2^{-1}\\).\n\\(E(\\ln(Y)|X_1, X_2) = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2\\).\n\\(E(Y^{-1}|X_1, X_2) = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2\\).\n\nMany of the linear model examples given above do not result in a straight line or surface but instead curve.\nSome examples of non-linear regression models are:\n\n\\(E(Y|X) = \\beta_0 + e^{\\beta_1 X}\\).\n\\(E(Y|X) = \\beta_0 + \\beta_1 X/(\\beta_2 + X)\\).\n\nThe latter regression models are non-linear models because there is no way to express them using the expression in Equation 3.14.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Parameter Estimation and Model Fitting</span>"
    ]
  },
  {
    "objectID": "parameter-estimation-and-model-fitting.html#estimation-of-the-multiple-linear-regression-model",
    "href": "parameter-estimation-and-model-fitting.html#estimation-of-the-multiple-linear-regression-model",
    "title": "3  Parameter Estimation and Model Fitting",
    "section": "3.5 Estimation of the multiple linear regression model",
    "text": "3.5 Estimation of the multiple linear regression model\nSuppose we want to estimate the parameters of the model relating the response variable to multiple regressors via the equation \\[\nY=\\beta_0 + \\beta_1 X_1 + \\cdots + \\beta_{p-1} X_{p-1} + \\epsilon.\n\\]\nThe system of equations relating the responses, the regressors, and the errors for all \\(n\\) observations can be written as \\[\nY_i = \\beta_0 + \\beta_1 x_{i,1} + \\beta_2 x_{i,2} + \\cdots + \\beta_{p-1} x_{i,p-1} + \\epsilon_i,\\quad i=1,2,\\ldots,n.\n\\tag{3.15}\\]\n\n3.5.1 Using matrix notation to represent a linear model\nWe can simplify the linear model described in Equation 3.15 using matrix notation. Appendix A provides an overview of matrix-related information that may be useful for understanding the discussion below.\nWe use the following notation:\n\n\\(\\mathbf{y} = [Y_1, Y_2, \\ldots, Y_n]\\) denotes the \\(n\\times 1\\) column vector containing the \\(n\\) observed response values.\n\\(\\mathbf{X}\\) denotes the \\(n\\times p\\) matrix containing a column of 1s and the observed regressor values for \\(X_1, X_2, \\ldots, X_{p-1}\\). This may be written as \\[\n\\mathbf{X} = \\begin{bmatrix}\n1 & x_{1,1} & x_{1,2} & \\cdots & x_{1,p-1} \\\\\n1 & x_{2,1} & x_{2,2} & \\cdots & x_{2,p-1} \\\\\n\\vdots & \\vdots & \\vdots & \\vdots & \\vdots \\\\\n1 & x_{n,1} & x_{n,2} & \\cdots & x_{n,p-1}\n\\end{bmatrix}.\n\\]\n\\(\\boldsymbol{\\beta} = [\\beta_0, \\beta_1, \\ldots, \\beta_{p-1}]\\) denotes the \\(p\\times 1\\) column vector containing the \\(p\\) regression coefficients.\n\\(\\boldsymbol{\\epsilon} = [\\epsilon_1, \\epsilon_2, \\ldots, \\epsilon_n]\\) denotes the \\(n\\times 1\\) column vector containing the \\(n\\) model errors.\n\nThe system of equations defining the model in Equation 3.15 can be written as \\[\n\\mathbf{y} = \\mathbf{X}\\boldsymbol{\\beta} + \\boldsymbol{\\epsilon}.\n\\] A regression model that cannot be represented as a system of linear equations using matrices is not a linear model.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n–&gt;\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n–&gt;\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n3.5.2 Derivation of the OLS estimators of the simple linear regression model coefficients\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n–&gt;\n\n–&gt;  –&gt;  –&gt;  –&gt;  –&gt;  –&gt;  –&gt;  –&gt;\n\n–&gt;  –&gt;  –&gt;  –&gt;  –&gt;  –&gt;  –&gt;  –&gt;\n\n–&gt;  –&gt;  –&gt;  –&gt;  –&gt;  –&gt;  –&gt;  –&gt;  –&gt;  –&gt;  –&gt;  –&gt;  –&gt;  –&gt;\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGorman, Kristen B., Tony D. Williams, and William R. Fraser. 2014. “Ecological Sexual Dimorphism and Environmental Variability Within a Community of Antarctic Penguins (Genus Pygoscelis).” PLOS ONE 9 (3): 1–14. https://doi.org/10.1371/journal.pone.0090081.\n\n\nHorst, Allison, Alison Hill, and Kristen Gorman. 2022. Palmerpenguins: Palmer Archipelago (Antarctica) Penguin Data. https://allisonhorst.github.io/palmerpenguins/.\n\n\nPearson, Karl, and Alice Lee. 1897. “Mathematical Contributions to the Theory of Evolution. On Telegony in Man. &c.” Proceedings of the Royal Society of London 60 (359-367): 273–83. https://doi.org/10.1098/rspl.1896.0048.\n\n\n———. 1903. “On the Laws of Inheritance in Man: I. Inheritance of Physical Characters: I. Inheritance of Physical Characters.” Biometrika 2 (4): 357–462. https://doi.org/10.1093/biomet/2.4.357.\n\n\nWeisberg, Sanford. 2014. Applied Linear Regression. Fourth. Hoboken NJ: Wiley. http://z.umn.edu/alr4ed.\n\n\nWilkinson, GN, and CE Rogers. 1973. “Symbolic Description of Factorial Models for Analysis of Variance.” Journal of the Royal Statistical Society: Series C (Applied Statistics) 22 (3): 392–99.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Parameter Estimation and Model Fitting</span>"
    ]
  },
  {
    "objectID": "interpretation.html",
    "href": "interpretation.html",
    "title": "4  Interpreting a Fitted Model",
    "section": "",
    "text": "–&gt;\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n–&gt;\n\n–&gt;\n\n–&gt;\n\n–&gt;",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Interpreting a Fitted Model</span>"
    ]
  },
  {
    "objectID": "matrix-facts.html",
    "href": "matrix-facts.html",
    "title": "Appendix A — Overview of matrix definitions, properties, operations, etc.",
    "section": "",
    "text": "A.1 Notation\nMatrices are commonly denoted by bold capital letters like \\(\\mathbf{A}\\) or \\(\\mathbf{B}\\), but this will sometimes be simplified to capital letters like \\(A\\) or \\(B\\). A matrix \\(\\mathbf{A}\\) with \\(m\\) rows and \\(n\\) columns (an \\(m\\times n\\) matrix) will be denoted as\n\\[\n\\mathbf{A} =\n\\begin{bmatrix}\n\\mathbf{A}_{1,1} & \\mathbf{A}_{2,1} & \\cdots & \\mathbf{A}_{1,n} \\\\\n\\mathbf{A}_{2,1} & \\mathbf{A}_{2,1} & \\cdots & \\mathbf{A}_{2,n} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\n\\mathbf{A}_{m,1} & \\mathbf{A}_{m,2} & \\cdots & \\mathbf{A}_{m,n} \\\\\n\\end{bmatrix},\n\\]\nwhere \\(\\mathbf{A}_{i,j}\\) denotes the element in row \\(i\\) and column \\(j\\) of matrix \\(\\mathbf{A}\\).\nA column vector is a matrix with a single column. A row vector is a matrix with a single row.\nA \\(p\\times 1\\) column vector \\(\\mathbf{a}\\) may constructed as \\[\n\\mathbf{a} = [a_1, a_2, \\ldots, a_p] =\n\\begin{bmatrix}\na_1 \\\\ a_2 \\\\ \\vdots \\\\ a_p\n\\end{bmatrix}.\n\\]\nA vector is assumed to be a column vector unless otherwise indicated.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Overview of matrix definitions, properties, operations, etc.</span>"
    ]
  },
  {
    "objectID": "matrix-facts.html#notation",
    "href": "matrix-facts.html#notation",
    "title": "Appendix A — Overview of matrix definitions, properties, operations, etc.",
    "section": "",
    "text": "Vectors are commonly denoted with bold lowercase letters such as \\(\\mathbf{a}\\) or \\(\\mathbf{b}\\), but this may be simplified to lowercase letters such as \\(a\\) or \\(b\\).",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Overview of matrix definitions, properties, operations, etc.</span>"
    ]
  },
  {
    "objectID": "matrix-facts.html#basic-mathematical-operations",
    "href": "matrix-facts.html#basic-mathematical-operations",
    "title": "Appendix A — Overview of matrix definitions, properties, operations, etc.",
    "section": "A.2 Basic mathematical operations",
    "text": "A.2 Basic mathematical operations\n\nA.2.1 Addition and subtraction\nConsider matrices \\(\\mathbf{A}\\) and \\(\\mathbf{B}\\) with identical sizes \\(m\\times n\\).\nWe add \\(\\mathbf{A}\\) and \\(\\mathbf{B}\\) by adding the element in position \\(i,j\\) of \\(\\mathbf{B}\\) with the element in position \\(i,j\\) of \\(A\\), i.e.,\n\\[\n(\\mathbf{A} + \\mathbf{B})_{i,j} = \\mathbf{A}_{i,j} + \\mathbf{B}_{i,j}.\n\\]\nSimilarly, if we subtract \\(\\mathbf{B}\\) from matrix \\(\\mathbf{A}\\), then we subtract the element in position \\(i,j\\) of \\(\\mathbf{B}\\) from the element in position \\(i,j\\) of \\(\\mathbf{A}\\), i.e.,\n\\[\n(\\mathbf{A} - \\mathbf{B})_{i,j} = \\mathbf{A}_{i,j} - \\mathbf{B}_{i,j}.\n\\]\nExample:\n\\[\n\\begin{bmatrix}\n1 & 2 & 3 \\\\\n4 & 5 & 6 \\\\\n\\end{bmatrix} +\n\\begin{bmatrix}\n2 & 9 & 1 \\\\\n1 & 3 & 1 \\\\\n\\end{bmatrix} =\n\\begin{bmatrix}\n3 & 11 & 4 \\\\\n5 & 8 & 7 \\\\\n\\end{bmatrix}.\n\\]\n\\[\n\\begin{bmatrix}\n1 & 2 & 3 \\\\\n4 & 5 & 6 \\\\\n\\end{bmatrix} -\n\\begin{bmatrix}\n2 & 9 & 1 \\\\\n1 & 3 & 1 \\\\\n\\end{bmatrix} =\n\\begin{bmatrix}\n-1 & -7 & 2 \\\\\n3 & 2 & 5 \\\\\n\\end{bmatrix}.\n\\]\n\n\nA.2.2 Scalar multiplication\nA matrix multiplied by a scalar value \\(c\\in\\mathbb{R}\\) is the matrix obtained by multiplying each element of the matrix by \\(c\\). If \\(\\mathbf{A}\\) is a matrix and \\(c\\in \\mathbb{R}\\), then \\[\n(c\\mathbf{A})_{i,j} = c\\mathbf{A}_{i,j}.\n\\] Example: \\[\n3\\begin{bmatrix}\n1 & 2 & 3 \\\\\n4 & 5 & 6 \\\\\n\\end{bmatrix}=\n\\begin{bmatrix}\n3\\cdot 1 & 3\\cdot 2 & 3\\cdot 3 \\\\\n3\\cdot 4 & 3\\cdot 5 & 3\\cdot 6 \\\\\n\\end{bmatrix}=\n\\begin{bmatrix}\n3 & 6 & 9 \\\\\n12 & 15 & 18 \\\\\n\\end{bmatrix}.\n\\]\n\n\nA.2.3 Matrix multiplication\nConsider two matrices \\(\\mathbf{A}\\) and \\(\\mathbf{B}\\).\nThe matrix product \\(\\mathbf{AB}\\) is only defined if the number of columns in \\(\\mathbf{A}\\) matches the number of rows in \\(\\mathbf{B}\\).\nAssume \\(\\mathbf{A}\\) is an \\(m\\times n\\) matrix and \\(\\mathbf{B}\\) is an \\(n\\times p\\) matrix. \\(\\mathbf{AB}\\) will be an \\(m\\times p\\) matrix and\n\\[\n(\\mathbf{AB})_{i,j} = \\sum_{k=1}^{n} \\mathbf{A}_{i,k}\\mathbf{B}_{k,j}.\n\\]\nExample: \\[\n\\begin{bmatrix}\n1 & 2 & 3 \\\\\n4 & 5 & 6\n\\end{bmatrix}\n\\begin{bmatrix}\n1 & 4\\\\\n2 & 5\\\\\n3 & 6\n\\end{bmatrix}=\n\\begin{bmatrix}\n1\\cdot 1 +  2 \\cdot 2 + 3 \\cdot 3 & 1 \\cdot 4 + 2 \\cdot 5 + 3 \\cdot 6\\\\\n4\\cdot 1 +  5 \\cdot 2 + 6 \\cdot 3 & 4 \\cdot 4 + 5 \\cdot 5 + 6 \\cdot 6\\\\\n\\end{bmatrix}=\n\\begin{bmatrix}\n14 & 32\\\\\n32 & 77\\\\\n\\end{bmatrix}.\n\\]\n\n\nA.2.4 Transpose\nThe transpose of a matrix \\(\\mathbf{A}\\), denoted \\(\\mathbf{A}^T\\), exchanges the rows and columns of the matrix. More formally, the \\(i,j\\) element of \\(\\mathbf{A}^T\\) is the \\(j,i\\) element of \\(\\mathbf{A}\\), i.e., \\((\\mathbf{A}^T)_{i,j} = \\mathbf{A}_{j,i}\\).\nExample: \\[\n\\begin{bmatrix}\n2 & 9 & 3 \\\\\n4 & 5 & 6\n\\end{bmatrix}^T =\n\\begin{bmatrix}\n2 & 4\\\\\n9 & 5\\\\\n3 & 6\n\\end{bmatrix}.\n\\]",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Overview of matrix definitions, properties, operations, etc.</span>"
    ]
  },
  {
    "objectID": "matrix-facts.html#basic-mathematical-properties",
    "href": "matrix-facts.html#basic-mathematical-properties",
    "title": "Appendix A — Overview of matrix definitions, properties, operations, etc.",
    "section": "A.3 Basic mathematical properties",
    "text": "A.3 Basic mathematical properties\n\nA.3.1 Associative property\nAddition and multiplication satisfy the associative property for matrices. Assuming that the matrices \\(\\mathbf{A}\\), \\(\\mathbf{B}\\), and \\(\\mathbf{C}\\) have the sizes required to do the operations below, then\n\\[\n(\\mathbf{A} + \\mathbf{B}) + \\mathbf{C} = \\mathbf{A} + (\\mathbf{B} + \\mathbf{C})\n\\] and \\[\n(\\mathbf{AB})\\mathbf{C}=\\mathbf{A}(\\mathbf{BC}).\n\\]\n\n\nA.3.2 Distributive property\nMatrix operations satisfy the distributive property. Assuming that the matrices \\(\\mathbf{A}\\), \\(\\mathbf{B}\\), and \\(\\mathbf{C}\\) have the sizes required to do the operations below, then\n\\[\n\\mathbf{A}(\\mathbf{B}+\\mathbf{C})=\\mathbf{AB} + \\mathbf{AC}\\quad\\mathrm{and}\\quad (\\mathbf{A}+\\mathbf{B})\\mathbf{C} = \\mathbf{AC} + \\mathbf{BC}.\n\\]\n\n\nA.3.3 No commutative property\nIn general, matrix multiplication does not satisfy the commutative property, i.e., \\[\n\\mathbf{AB} \\neq \\mathbf{BA},\n\\] even when the matrix sizes allow the operation to be performed.\nExample:\n\\[\n\\begin{bmatrix}\n1 & 2\n\\end{bmatrix}\n\\begin{bmatrix}\n1\\\\\n2\n\\end{bmatrix}\n=\n\\begin{bmatrix}\n5\n\\end{bmatrix}\n\\] while \\[\n\\begin{bmatrix}\n1\\\\\n2\n\\end{bmatrix}\n\\begin{bmatrix}\n1 & 2\n\\end{bmatrix}\n=\n\\begin{bmatrix}\n1 & 2\\\\\n2 & 4\n\\end{bmatrix}.\n\\]\n\n\nA.3.4 Transpose-related properties\nAssume that the matrices \\(\\mathbf{A}\\), \\(\\mathbf{B}\\), and \\(\\mathbf{C}\\) have the sizes required to perform the operations below. Additionally, assume that \\(c\\in \\mathbb{R}\\) is a scalar constant.\nThe following properties are true:\n\n\\(c^T = c\\).\n\\((\\mathbf{A} + \\mathbf{B})^T = \\mathbf{A}^T + \\mathbf{B}^T\\).\n\\((\\mathbf{AB})^T = \\mathbf{B}^T \\mathbf{A}^T\\), which can be extended to \\((\\mathbf{ABC})^T=\\mathbf{C}^T \\mathbf{B}^T \\mathbf{A}^T\\), etc.\n\\((\\mathbf{A}^T)^T=\\mathbf{A}\\).",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Overview of matrix definitions, properties, operations, etc.</span>"
    ]
  },
  {
    "objectID": "matrix-facts.html#special-matrices",
    "href": "matrix-facts.html#special-matrices",
    "title": "Appendix A — Overview of matrix definitions, properties, operations, etc.",
    "section": "A.4 Special matrices",
    "text": "A.4 Special matrices\n\nA.4.1 Square matrices\nA matrix is square if the number of rows equals the number of columns.\nThe diagonal elements of an \\(n\\times n\\) square matrix \\(\\mathbf{A}\\) are the elements \\(\\mathbf{A}_{i,i}\\) for \\(i = 1, 2, \\ldots, n\\). Any non-diagonal elements of \\(\\mathbf{A}\\) are called off-diagonal elements.\n\n\nA.4.2 Identity matrix\nThe \\(n\\times n\\) identity matrix \\(\\mathbf{I}_{n\\times n}\\) is 1 for its diagonal elements and 0 for its off-diagonal elements. Context often makes it clear what the dimensions of an identity matrix are, so \\(\\mathbf{I}_{n\\times n}\\) is often simplified to \\(\\mathbf{I}\\) or \\(I\\).\nExample:\n\\[\n\\mathbf{I}_{3\\times 3} = \\begin{bmatrix}\n1 & 0 & 0 \\\\\n0 & 1 & 0 \\\\\n0 & 0 & 1\n\\end{bmatrix}.\n\\]\n\n\nA.4.3 Diagonal matrices\nA square matrix \\(\\mathbf{A}\\) is diagonal if all its off-diagonal elements are zero. A \\(3\\times 3\\) diagonal matrix and will look something like \\[\n\\begin{bmatrix}\n1 & 0 & 0\\\\\n0 & -1 & 0\\\\\n0 & 0 & 5\n\\end{bmatrix},\n\\] where the non-zero values can be replaced by any real numbers.\n\n\nA.4.4 Symmetric matrices\nA matrix \\(\\mathbf{A}\\) is symmetric if \\(\\mathbf{A} = \\mathbf{A}^T\\), i.e., \\(\\mathbf{A}_{i,j} = \\mathbf{A}_{j,i}\\) for all potential \\(i,j\\).\nA symmetric matrix must be square.\n\n\nA.4.5 Idempotent matrices\nA matrix \\(\\mathbf{A}\\) is idempotent if \\(\\mathbf{AA} = \\mathbf{A}\\).\nAn idempotent matrix must be square.\n\n\nA.4.6 Positive definite matrices\nA matrix \\(\\mathbf{A}\\) is positive definite if \\[\n\\mathbf{a}^T \\mathbf{Aa} &gt; 0,\n\\] for every vector of real values \\(\\mathbf{a}\\) whose values are not identically 0.\n\n\nA.4.7 Inverse matrix\nAn \\(n\\times n\\) matrix \\(\\mathbf{A}\\) is invertible if there exists a matrix \\(\\mathbf{B}\\) such that \\(\\mathbf{AB}=\\mathbf{BA}=\\mathbf{I}_{n\\times n}\\). The inverse of \\(\\mathbf{A}\\) is denoted \\(\\mathbf{A}^{-1}\\).\nInverse matrices only exist for square matrices.\nSome other properties related to the inverse operator:\n\nIf \\(n\\times n\\) matrices \\(\\mathbf{A}\\) and \\(\\mathbf{B}\\) are invertible then \\((\\mathbf{AB})^{-1} = \\mathbf{B}^{-1} \\mathbf{A} ^{-1}\\).\nIf \\(\\mathbf{A}\\) is invertible then \\((\\mathbf{A}^{-1})^T = (\\mathbf{A}^T)^{-1}\\).",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Overview of matrix definitions, properties, operations, etc.</span>"
    ]
  },
  {
    "objectID": "matrix-facts.html#matrix-derivatives",
    "href": "matrix-facts.html#matrix-derivatives",
    "title": "Appendix A — Overview of matrix definitions, properties, operations, etc.",
    "section": "A.5 Matrix derivatives",
    "text": "A.5 Matrix derivatives\nWe start with some basic calculus results.\nLet \\(f(y)\\) be a function of a scalar value \\(b\\) and \\(\\frac{df(y)}{dy}\\) denote the derivative of the function with respect to \\(y\\). Assume \\(c \\in \\mathbb{R}\\) is a constant. Then the results in Table A.1 are true.\n\n\n\n\nTable A.1: Some basic calculus results for scalar functions taking scalar inputs.\n\n\n\n\n\n\n\\(f(y)\\)\n\\(\\frac{df(y)}{dy}\\)\n\n\n\n\n\\(cy\\)\n\\(c\\)\n\n\n\\(y^2\\)\n\\(2y\\)\n\n\n\\(c y^2\\)\n\\(2cy\\)\n\n\n\n\n\n\n\n\nNow let’s look at the derivative of a scalar function \\(f\\) with respect to a vector (i.e., the function takes a vector of values and produces a single real number).\nLet \\(f(\\mathbf{y})\\) be a function of a \\(p\\times 1\\) column vector \\(\\mathbf{y}=[y_1, y_2, \\ldots,  y_p]^T\\). The derivative of \\(f(\\mathbf{y})\\) with respect to \\(\\mathbf{y}\\) is denoted \\(\\frac{\\partial f(\\mathbf{y})}{\\partial \\mathbf{y}}\\) and \\[\n\\frac{\\partial f(\\mathbf{y})}{\\partial \\mathbf{y}} = \\begin{bmatrix}\n\\frac{\\partial f(\\mathbf{y})}{\\partial y_1}\\\\\n\\frac{\\partial f(\\mathbf{y})}{\\partial y_2}\\\\\n\\vdots \\\\\n\\frac{\\partial f(\\mathbf{y})}{\\partial y_p}\n\\end{bmatrix}.\n\\]\nIn words, the derivative of a scalar function with respect to its input vector is the vector of partial derivatives with respect to the elements of the input vector.\nAssume \\(\\mathbf{A}\\) is an \\(m\\times p\\) matrix of constant values. Then the results in Table A.2 are true.\n\n\n\n\nTable A.2: Some basic calculus results for scalar functions taking vector inputs.\n\n\n\n\n\n\n\n\n\n\n\\(f(\\mathbf{y})\\)\n\\(\\frac{df(\\mathbf{y})}{d\\mathbf{y}}\\)\n\n\n\n\n\\(\\mathbf{y}^T \\mathbf{A}\\)\n\\(\\mathbf{A}\\)\n\n\n\\(\\mathbf{y}^T \\mathbf{y}\\)\n\\(2\\mathbf{y}\\)\n\n\n\\(\\mathbf{y}^T \\mathbf{A} \\mathbf{y}\\)\n\\(2\\mathbf{A}\\mathbf{y}\\)\n\n\n\n\n\n\n\n\nComparing Tables Table A.1 and Table A.2, one can make parallels with the derivative results from the two contexts.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Overview of matrix definitions, properties, operations, etc.</span>"
    ]
  },
  {
    "objectID": "matrix-facts.html#additional-topics",
    "href": "matrix-facts.html#additional-topics",
    "title": "Appendix A — Overview of matrix definitions, properties, operations, etc.",
    "section": "A.6 Additional topics",
    "text": "A.6 Additional topics\n\nA.6.1 Determinant\nThe determinant of a matrix is a special function that is applied to a square matrix and returns a scalar value. The determinant of a matrix \\(\\mathbf{A}\\) is usually denoted \\(|\\mathbf{A}|\\) or \\(\\mathrm{det}(\\mathbf{A})\\). We do not discuss how to compute the determinant of a matrix, which is not needed for our purposes. You can find out more about matrix determinants at https://en.wikipedia.org/wiki/Determinant.\n\n\nA.6.2 Linearly independent vectors\nLet \\(\\mathbf{x}_1, \\mathbf{x}_2, \\ldots, \\mathbf{x}_n\\) be a set of \\(n\\) vectors of size \\(p\\times 1\\).\nThen \\(\\mathbf{x}_1, \\mathbf{x}_2, \\ldots, \\mathbf{x}_n\\) are linearly dependent if there exists \\(\\mathbf{a}=[a_1, a_2, \\ldots, a_n]\\neq \\boldsymbol{0}_{n\\times 1}\\) such [ a_1 _1 + a_2 _2 + + a_p p = {p}, ] where \\(\\boldsymbol{0}_{p\\times 1}\\) defines an \\(p\\times 1\\) column vector of zeros.\nLet \\(\\mathbf{X}\\) be an \\(n\\times p\\) matrix such that \\(\\mathbf{x}_1^T, \\mathbf{x}_2^T, \\ldots, \\mathbf{x}_n^T\\) make up its \\(n\\) rows and vectors \\(\\mathbf{X}_{[1]}, \\mathbf{X}_{[2]}, \\ldots, \\mathbf{X}_{[p]}\\) make up its columns, so that \\[\n\\mathbf{X}=\n\\begin{bmatrix}\n\\mathbf{x}_1^T\\\\\n\\mathbf{x}_2^T\\\\\n\\vdots\\\\\n\\mathbf{x}_n^T\n\\end{bmatrix}=\n\\begin{bmatrix}\n\\mathbf{X}_{[1]} & \\mathbf{X}_{[1]} & \\cdots & \\mathbf{X}_{[p]}\n\\end{bmatrix}.\n\\]\nThe columns vectors of \\(\\mathbf{X}\\) are linearly independent if there is no \\(\\mathbf{a}=[a_1,a_2,\\ldots,a_p]\\neq \\boldsymbol{0}_{p\\times 1}\\) such that \\[\na_1 \\mathbf{X}_{[1]} + a_2 \\mathbf{X}_{[2]} + \\cdots + a_p \\mathbf{X}_{[p]} = \\boldsymbol{0}_{n\\times 1}.\n\\]\nThe rows of \\(\\mathbf{X}\\) are linearly independent if there is no \\(\\mathbf{a}=[a_1,a_2,\\ldots,a_n]\\neq \\boldsymbol{0}_{n\\times 1}\\) such that \\[\na_1 \\mathbf{x}_{1} + a_2 \\mathbf{x}_{2} + \\cdots + a_n \\mathbf{x}_{n} = \\boldsymbol{0}_{p\\times 1}.\n\\]\nYou can learn more about linear independence at https://en.wikipedia.org/wiki/Linear_independence.\n\n\nA.6.3 Rank\nThe rank of a matrix is the number of linearly independent columns of the matrix.\nIf \\(\\mathbf{X}\\) is an \\(n\\times p\\) matrix with linearly dependent columns but removing a single column results in a linearly independent matrix, then the rank of \\(\\mathbf{X}\\) is \\(p-1\\).\nAn \\(n\\times p\\) matrix has full rank if its rank equals \\(\\min(n, p)\\), i.e., the smaller of its number of rows and columns.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Overview of matrix definitions, properties, operations, etc.</span>"
    ]
  },
  {
    "objectID": "probability-basics.html",
    "href": "probability-basics.html",
    "title": "Appendix B — Probability Basics",
    "section": "",
    "text": "B.1 Basic set operations\nWe now review some basic set operations and related facts. Let \\(A\\) and \\(B\\) be two events contained in \\(\\Omega\\).\nThe intersection of \\(A\\) and \\(B\\), denoted \\(A \\cap B\\), is the set of outcomes that are common to both \\(A\\) and \\(B\\), i.e., \\(A \\cap B = \\{\\omega \\in \\Omega: \\omega \\in A\\;\\mathrm{and}\\;\\omega \\in B\\}\\).\nEvents \\(A\\) and \\(B\\) are disjoint if \\(A\\cap B = \\emptyset\\), i.e., if \\(A\\) and \\(B\\) have no common outcomes.\nThe union of \\(A\\) and \\(B\\), denoted \\(A \\cup B\\), is the set of outcomes that are in \\(A\\) or \\(B\\) or both, i.e., \\(A \\cup B = \\{\\omega \\in \\Omega: \\omega \\in A\\;\\mathrm{or}\\;\\omega \\in B\\}\\).\nThe complement of \\(A\\), denoted \\(A^c\\), is the set of outcomes that are in \\(\\Omega\\) but are not in \\(A\\), i.e., \\(A^c = \\{\\omega \\in \\Omega: \\omega \\not\\in A\\}\\). The complement of \\(A\\) may also be denoted as \\(\\overline{A}\\) or \\(A'\\).\nThe set difference between \\(A\\) and \\(B\\), denoted \\(A \\setminus B\\), is the set of outcomes in \\(A\\) that are not in \\(B\\), i.e., \\(A\\setminus B = \\{\\omega \\in A: \\omega \\not\\in B\\}\\). The set difference between \\(A\\) and \\(B\\) may also be denoted by \\(A-B\\). The set difference is order specific, i.e., \\((A\\setminus B) \\not= (B\\setminus A)\\) in general.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>Probability Basics</span>"
    ]
  },
  {
    "objectID": "probability-basics.html#probability-function",
    "href": "probability-basics.html#probability-function",
    "title": "Appendix B — Probability Basics",
    "section": "B.2 Probability function",
    "text": "B.2 Probability function\nA probability function is a function \\(P\\) that assigns a real number \\(P(A)\\) to every event \\(A \\subseteq \\Omega\\) and satisfies three properties:\n\n\\(P(A)\\geq 0\\) for all \\(A\\subseteq \\Omega\\).\n\\(P(\\Omega) = 1\\). Alternatively, \\(P(\\emptyset) = 0\\). Informally, the probability that at least one of the possible outcomes in the sample space occurs is 1.\nIf \\(A_1, A_2, \\ldots\\) are disjoint, then \\(P\\left(\\bigcup_{i=1}^\\infty A_i \\right)=\\sum_{i=1}^\\infty P(A_i)\\).",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>Probability Basics</span>"
    ]
  },
  {
    "objectID": "probability-basics.html#independence-and-conditional-probability",
    "href": "probability-basics.html#independence-and-conditional-probability",
    "title": "Appendix B — Probability Basics",
    "section": "B.3 Independence and conditional probability",
    "text": "B.3 Independence and conditional probability\nA set of events \\(\\{A_i:i\\in I\\}\\) are independent if \\[\nP\\left(\\cap_{i\\in J} A_i \\right)=\\prod_{i\\in J} P(A_i )\n\\] for every finite subset \\(J\\subseteq I\\).\nThe conditional probability of \\(A\\) given \\(B\\), denoted as \\(P(A\\mid B)\\), is the probability that \\(A\\) occurs given that \\(B\\) has occurred, and is defined as \\[\nP(A\\mid B) = \\frac{P(A\\cap B)}{P(B)}, \\quad P(B) &gt; 0.\n\\]",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>Probability Basics</span>"
    ]
  },
  {
    "objectID": "probability-basics.html#additional-probability-results",
    "href": "probability-basics.html#additional-probability-results",
    "title": "Appendix B — Probability Basics",
    "section": "B.4 Additional probability results",
    "text": "B.4 Additional probability results\nThe following probability results are frequently useful in solving practical problems:\n\nComplement rule: \\(P(A^c) = 1 - P(A)\\).\nAddition rule: \\(P(A\\cup B) = P(A) + P(B) - P(A \\cap B)\\).\nBayes’ rule: Assuming \\(P(A) &gt; 0\\) and \\(P(B) &gt; 0\\), then \\[P(A\\mid B) = \\frac{P(B\\mid A)P(A)}{P(B)}.\\]\nLaw of Total Probability: Let \\(B_1, B_2, \\ldots\\) be a countably infinite partition of \\(\\Omega\\). Then \\[P(A) = \\sum_{i=1}^{\\infty} P(A \\cap B_i) = \\sum_{i=1}^{\\infty} P(A \\mid B_i) P(B_i).\\]",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>Probability Basics</span>"
    ]
  },
  {
    "objectID": "random-variables.html",
    "href": "random-variables.html",
    "title": "Appendix C — Random Variables",
    "section": "",
    "text": "C.1 Discrete random variables\n\\(Y\\) is a discrete random variable if it takes countably many values. The support can be represented as \\(\\mathcal{S} = \\{y_1, y_2, \\dots \\}\\).\nThe probability mass function (pmf) for \\(Y\\) is \\(f_Y (y)=P(Y=y)\\), where \\(y\\in \\mathbb{R}\\), and must have the following properties:\nAdditionally, the following statements are true:\nThe expected value, mean, or first moment of \\(Y\\), is defined as \\[\nE(Y) = \\sum_{y\\in \\mathcal{S}} y f_Y(y),\n\\] assuming the sum is well-defined.\nThe variance of \\(Y\\) is defined as \\[\n\\mathrm{var}(Y)=E(Y-E(Y))^2 =\n\\sum_{y\\in \\mathcal{S}} (y - E(Y))^2 f_Y(y).\n\\]\nNote that \\(\\mathrm{var}(Y)=E(Y-E(Y))^2=E(Y^2)-[E(Y)]^2\\). This formula is often easier to compute.\nThe standard deviation of \\(Y\\) is defined as \\[\nSD(Y)=\\sqrt{\\mathrm{var}(Y)}.\n\\]\nMore generally, for a discrete random variable \\(Y\\) and a real-valued function \\(g\\), \\[E(g(Y))=\\sum_{y\\in\\mathcal{S}}g(y)f_Y(y),\\] assuming the sum is well-defined.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>C</span>  <span class='chapter-title'>Random Variables</span>"
    ]
  },
  {
    "objectID": "random-variables.html#discrete-random-variables",
    "href": "random-variables.html#discrete-random-variables",
    "title": "Appendix C — Random Variables",
    "section": "",
    "text": "\\(0 \\leq f_Y(y) \\leq 1\\).\n\\(\\sum_{y\\in \\mathcal{S}} f_Y(y) = 1\\).\n\n\n\n\\(F_Y(c) = P(Y \\leq c) = \\sum_{y\\in \\mathcal{S}:y \\leq c} f_Y(y)\\).\n\\(P(Y \\in A) = \\sum_{y \\in A} f_Y(y)\\) for some event \\(A\\). 0 \\(P(a \\leq Y \\leq b) = \\sum_{y\\in\\mathcal{S}:a\\leq y\\leq b} f_Y(y)\\).\n\n\n\n\n\n\n\nC.1.1 Example (Bernoulli distribution)\nA random variable \\(Y\\) has a Bernoulli distribution with probability \\(\\theta\\), denoted \\(Y\\sim \\mathsf{Bernoulli}(\\theta)\\), if \\(\\mathcal{S} = \\{0, 1\\}\\) and \\(P(Y = 1) = \\theta\\), where \\(\\theta\\in (0,1)\\).\nThe pmf of a Bernoulli random variable is \\[\nf_Y(y) = \\theta^y (1-\\theta)^{(1-y)},\\quad y\\in \\{0, 1\\}, \\theta \\in (0, 1).\n\\]\nThe mean of a Bernoulli random variable is \\[\nE(Y)=0(1-\\theta )+1(\\theta)=\\theta.\n\\]\nThe variance of a Bernoulli random variable is \\[\n\\mathrm{var}(Y)=(0-\\theta)^2(1-\\theta)+(1-\\theta)^2\\theta = \\theta(1-\\theta).\n\\]",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>C</span>  <span class='chapter-title'>Random Variables</span>"
    ]
  },
  {
    "objectID": "random-variables.html#continuous-random-variables",
    "href": "random-variables.html#continuous-random-variables",
    "title": "Appendix C — Random Variables",
    "section": "C.2 Continuous random variables",
    "text": "C.2 Continuous random variables\n\\(Y\\) is a continuous random variable if there exists a function \\(f_Y (y)\\) such that:\n\n\\(f_Y (y)\\geq 0\\) for all \\(y\\),\n\\(\\int_{-\\infty}^\\infty f_Y (y)  dy = 1\\),\n\\(a\\leq b\\), \\(P(a&lt;Y&lt;b)=\\int_a^b f_Y (y)  dy\\).\n\nThe function \\(f_Y\\) is called the probability density function (pdf).\nAdditionally, \\(F_Y (y)=\\int_{-\\infty}^y f_Y (y)  dy\\) and \\(f_Y (y)=F'_Y(y)\\) for any point \\(y\\) at which \\(F_Y\\) is differentiable.\nThe mean of a continuous random variables \\(Y\\) is defined as \\[\nE(Y) =\n\\int_{-\\infty}^{\\infty} y f_Y(y)  dy =\n\\int_{y\\in\\mathcal{S}} y f_Y(y).\n\\] assuming the integral is well-defined.\nThe variance of a continuous random variable \\(Y\\) is defined by \\[\n\\mathrm{var}(Y)=\nE(Y-E(Y))^2=\\int_{-\\infty}^{\\infty} (y - E(Y))^2 f_Y(y)  dy =\n\\int_{y\\in\\mathcal{S}} (y - E(Y))^2 f_Y(y) dy.\n\\]\nMore generally, for a continuous random variable \\(Y\\) and a real-valued function \\(g\\), \\[E(g(Y))=\\int_{y\\in\\mathcal{S}}g(y)f_Y(y)\\;dy,\\] assuming the integral is well-defined.\n\nC.2.1 Example (Uniform distribution)\nA random variable \\(Y\\) is said to have an uniform distribution with parameters \\(a&lt;b\\), written as \\(Y \\sim \\mathsf{U}(a, b)\\), if \\(\\mathcal{S} = \\{y\\in \\mathbb{R}:a\\leq y \\leq b\\}\\) and has the density function \\[\nf(y) = \\frac{1}{b-a}, \\quad a\\leq y \\leq b, a &lt; b.\n\\]\nThe mean of \\(Y\\) is computed as \\[\n\\begin{aligned}\nE(Y)&=\\int_{a}^{b} y\\frac{1}{b-a}\\;dy\\\\\n&=\\frac{y^2}{2}\\frac{1}{b-a}\\biggr]^{b}_{a}\\\\\n&=\\frac{1}{2}\\frac{b^2-a^2}{b-a} \\\\\n&=\\frac{1}{2}\\frac{(b-a)(b+a)}{b-a} \\\\\n&=\\frac{b+a}{2}.\n\\end{aligned}\n\\]\nAdditionally, \\[\n\\begin{aligned}\nE(Y^2)&=\\int_{a}^{b} y^2\\frac{1}{b-a}\\;dy\\\\\n&=\\frac{y^3}{3}\\frac{1}{b-a}\\biggr]^{b}_{a}\\\\\n&=\\frac{1}{3}\\frac{b^3-a^3}{b-a} \\\\\n&=\\frac{1}{3}\\frac{(b-a)(b^2+ab+a^2)}{b-a} \\\\\n&=\\frac{b^2+ab+a^2}{3}.\n\\end{aligned}\n\\]\nThus, the variance of \\(Y\\) is \\[\n\\begin{aligned}\n\\mathrm{var}(Y)&=\nE(Y^2)-[E(Y)]^2\\\\\n&=\\frac{b^2+ab+a^2}{3}-\\biggl[\\frac{(b+a)}{2}\\biggr]^2\\\\\n&=\\frac{4b^2+4ab+4a^2}{12}-\\frac{3b^2 +6ab + 3a^2}{12}\\\\\n&=\\frac{b^2-2ab-a^2}{12}\\\\\n&=\\frac{(b-a)^2}{12}.\n\\end{aligned}\n\\]",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>C</span>  <span class='chapter-title'>Random Variables</span>"
    ]
  },
  {
    "objectID": "random-variables.html#useful-facts-for-transformations-of-random-variables",
    "href": "random-variables.html#useful-facts-for-transformations-of-random-variables",
    "title": "Appendix C — Random Variables",
    "section": "C.3 Useful facts for transformations of random variables",
    "text": "C.3 Useful facts for transformations of random variables\nLet \\(Y\\) be a random variable and \\(a\\in\\mathbb{R}\\) be a constant. Then:\n\n\\(E(a) = a\\).\n\\(E(aY) = a E(Y)\\).\n\\(E(a + Y) = a + E(Y)\\).\n\\(\\mathrm{var}(a) = 0\\).\n\\(\\mathrm{var}(aY) = a^2 \\mathrm{var}(Y)\\).\n\\(\\mathrm{var}(a + Y) = \\mathrm{var}(Y)\\).",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>C</span>  <span class='chapter-title'>Random Variables</span>"
    ]
  },
  {
    "objectID": "multivariate-distributions.html",
    "href": "multivariate-distributions.html",
    "title": "Appendix D — Multivariate distributions",
    "section": "",
    "text": "D.1 Basic properties\nLet \\(Y_1,Y_2,\\ldots,Y_n\\) denote \\(n\\) random variables with supports \\(\\mathcal{S}_1,\\mathcal{S}_2,\\ldots,\\mathcal{S}_n\\), respectively.\nIf the random variables are jointly discrete (i.e., all discrete), then the joint pmf \\(f(y_1,\\ldots,y_n)=P(Y_1=y_1,\\ldots,Y_n=y_n)\\) satisfies the following properties:\nIn this context, \\[\nE(Y_1 \\cdots Y_n)=\\sum_{y_1\\in\\mathcal{S}_1} \\cdots \\sum_{y_n\\in\\mathcal{S}_n}y_1 \\cdots y_n  f(y_1,\\ldots,y_n).\n\\]\nIn general, \\[\nE(g(Y_1,\\ldots,Y_n))=\\sum_{y_1\\in\\mathcal{S}_1} \\cdots \\sum_{y_n\\in\\mathcal{S}_n} g(y_1, \\ldots, y_n) f(y_1,\\ldots,y_n),\n\\] where \\(g\\) is a function of the random variables.\nIf the random variables are jointly continuous, then \\(f(y_1,\\ldots,y_n)\\) is the joint pdf if it satisfies the following properties:\nIn this context, \\[\nE(Y_1 \\cdots Y_n)=\\int_{y_1\\in\\mathcal{S}_1} \\cdots \\int_{y_n\\in\\mathcal{S}_n} y_1 \\cdots y_n  f(y_1,\\ldots,y_n) dy_n \\ldots dy_1.\n\\]\nIn general, \\[\nE(g(Y_1,\\ldots,Y_n))=\\int_{y_1\\in\\mathcal{S}_1} \\cdots \\int_{y_n\\in\\mathcal{S}_n} g(y_1, \\ldots, y_n) f(y_1,\\ldots,y_n) dy_n \\cdots dy_1,\n\\] where \\(g\\) is a function of the random variables.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>D</span>  <span class='chapter-title'>Multivariate distributions</span>"
    ]
  },
  {
    "objectID": "multivariate-distributions.html#basic-properties",
    "href": "multivariate-distributions.html#basic-properties",
    "title": "Appendix D — Multivariate distributions",
    "section": "",
    "text": "\\(0\\leq f(y_1,\\ldots,y_n )\\leq 1\\),\n\\(\\sum_{y_1\\in\\mathcal{S}_1}\\cdots \\sum_{y_n\\in\\mathcal{S}_n} f(y_1,\\ldots,y_n ) = 1\\),\n\\(P((Y_1,\\ldots,Y_n)\\in A)=\\sum_{(y_1,\\ldots,y_n) \\in A} f(y_1,\\ldots,y_n)\\).\n\n\n\n\n\n\\(f(y_1,\\ldots,y_n ) \\geq 0\\),\n\\(\\int_{y_1\\in\\mathcal{S}_1}\\cdots \\int_{y_n\\in\\mathcal{S}_n} f(y_1,\\ldots,y_n ) dy_n \\cdots dy_1 = 1\\),\n\\(P((Y_1,\\ldots,Y_n)\\in A)=\\int \\cdots \\int_{(y_1,\\ldots,y_n) \\in A} f(y_1,\\ldots,y_n) dy_n\\ldots dy_1\\).",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>D</span>  <span class='chapter-title'>Multivariate distributions</span>"
    ]
  },
  {
    "objectID": "multivariate-distributions.html#marginal-distributions",
    "href": "multivariate-distributions.html#marginal-distributions",
    "title": "Appendix D — Multivariate distributions",
    "section": "D.2 Marginal distributions",
    "text": "D.2 Marginal distributions\nIf the random variables are jointly discrete, then the marginal pmf of \\(Y_1\\) is obtained by summing over the other variables \\(Y_2, ..., Y_n\\): \\[\nf_{Y_1}(y_1)=\\sum_{y_2\\in\\mathcal{S}_2}\\cdots \\sum_{y_n\\in\\mathcal{S}_n} f(y_1,\\ldots,y_n).\n\\]\nSimilarly, if the random variables are jointly continuous, then the marginal pdf of \\(Y_1\\) is obtained by integrating over the other variables \\(Y_2, ..., Y_n\\) \\[\nf_{Y_1}(y_1)=\\int_{y_2\\in\\mathcal{S}_2}\\cdots \\int_{y_n\\in\\mathcal{S}_n} f(y_1,\\ldots,y_n) dy_n \\cdots dy_2.\n\\]",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>D</span>  <span class='chapter-title'>Multivariate distributions</span>"
    ]
  },
  {
    "objectID": "multivariate-distributions.html#independence-of-random-variables",
    "href": "multivariate-distributions.html#independence-of-random-variables",
    "title": "Appendix D — Multivariate distributions",
    "section": "D.3 Independence of random variables",
    "text": "D.3 Independence of random variables\nRandom variables \\(X\\) and \\(Y\\) are independent if \\[\nF(x, y) = F_X(x) F_Y(y).\n\\]\nAlternatively, \\(X\\) and \\(Y\\) are independent if \\[\nf(x, y) = f_X(x)f_Y(y).\n\\]",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>D</span>  <span class='chapter-title'>Multivariate distributions</span>"
    ]
  },
  {
    "objectID": "multivariate-distributions.html#conditional-distributions",
    "href": "multivariate-distributions.html#conditional-distributions",
    "title": "Appendix D — Multivariate distributions",
    "section": "D.4 Conditional distributions",
    "text": "D.4 Conditional distributions\nLet \\(X\\) and \\(Y\\) be random variables. Then assuming \\(f_Y(y)&gt;0\\), the conditional distribution of \\(X\\) given \\(Y = y\\), denoted \\(X|Y=y\\) comes from Bayes’ formula: \\[\nf(x\\mid y) = \\frac{f(x, y)}{f_{Y}(y)}, \\quad f_Y(y)&gt;0.\n\\]",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>D</span>  <span class='chapter-title'>Multivariate distributions</span>"
    ]
  },
  {
    "objectID": "multivariate-distributions.html#covariance",
    "href": "multivariate-distributions.html#covariance",
    "title": "Appendix D — Multivariate distributions",
    "section": "D.5 Covariance",
    "text": "D.5 Covariance\nThe covariance between random variables \\(X\\) and \\(Y\\) is \\[\n\\mathrm{cov}(X,Y)=E[(X-E(X))(Y-E(Y))]=E(XY)-E(X)E(Y).\n\\]",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>D</span>  <span class='chapter-title'>Multivariate distributions</span>"
    ]
  },
  {
    "objectID": "multivariate-distributions.html#sec-trans-mult-rand-vars",
    "href": "multivariate-distributions.html#sec-trans-mult-rand-vars",
    "title": "Appendix D — Multivariate distributions",
    "section": "D.6 Useful facts for transformations of multiple random variables",
    "text": "D.6 Useful facts for transformations of multiple random variables\nLet \\(a\\) and \\(b\\) be scalar constants. Let \\(Y\\) and \\(Z\\) be random variables. Then:\n\n\\(E(aY+bZ)=aE(Y)+bE(Z)\\).\n\\(\\mathrm{var}(Y+Z)=\\mathrm{var}(Y)+\\mathrm{var}(Z)+2\\mathrm{cov}(Y, Z)\\).\n\\(\\mathrm{cov}(a,Y)=0\\).\n\\(\\mathrm{cov}(Y,Y)=\\mathrm{var}(Y)\\).\n\\(\\mathrm{cov}(aY, bZ)=ab\\mathrm{cov}(Y, Z)\\).\n\\(\\mathrm{cov}(a + Y,b + Z)=\\mathrm{cov}(Y, Z)\\).\n\nIf \\(Y\\) and \\(Z\\) are also independent, then:\n\n\\(E(YZ)=E(Y)E(Z)\\).\n\\(\\mathrm{cov}(Y, Z)=0\\).\n\nIn general, if \\(Y_1, Y_2, \\ldots, Y_n\\) are a set of random variables, then:\n\n\\(E(\\sum_{i=1}^n Y_i) = \\sum_{i=1}^n E(Y_i)\\), i.e., the expectation of the sum of random variables is the sum of the expectation of the random variables.\n\\(\\mathrm{var}(\\sum_{i=1}^n Y_i) = \\sum_{i=1}^n \\mathrm{var}(Y_i) + \\sum_{j=1}^n\\sum_{1\\leq i&lt;j\\leq n}2\\mathrm{cov}(Y_i, Y_j)\\), i.e., the variance of the sum of random variables is the sum fo the variables’ variances plus the sum of twice all possible pairwise covariances.\n\nIf in addition, \\(Y_1, Y_2, \\ldots, Y_n\\) are all independent of each other, then:\n\n\\(\\mathrm{var}(\\sum_{i=1}^n Y_i) = \\sum_{i=1}^n \\mathrm{var}(Y_i)\\) since all pairwise covariances are 0.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>D</span>  <span class='chapter-title'>Multivariate distributions</span>"
    ]
  },
  {
    "objectID": "multivariate-distributions.html#example-binomial",
    "href": "multivariate-distributions.html#example-binomial",
    "title": "Appendix D — Multivariate distributions",
    "section": "D.7 Example (Binomial)",
    "text": "D.7 Example (Binomial)\nA random variable \\(Y\\) is said to have a Binomial distribution with \\(n\\) trials and probability of success \\(\\theta\\), denoted \\(Y\\sim \\mathsf{Bin}(n,\\theta)\\) when \\(\\mathcal{S}=\\{0,1,2,\\ldots,n\\}\\) and the pmf is \\[\nf(y\\mid\\theta) = \\binom{n}{y} \\theta^y (1-\\theta)^{(n-y)}.\n\\]\nAn alternative explanation of a Binomial random variable is that it is the sum of \\(n\\) independent and identically-distributed Bernoulli random variables. Alternatively, let \\(Y_1,Y_2,\\ldots,Y_n\\stackrel{i.i.d.}{\\sim} \\mathsf{Bernoulli}(\\theta)\\), where i.i.d. stands for independent and identically distributed, i.e., \\(Y_1, Y_2, \\ldots, Y_n\\) are independent random variables with identical distributions. Then \\(Y=\\sum_{i=1}^n Y_i \\sim \\mathsf{Bin}(n,\\theta)\\).\nA Binomial random variable with \\(\\theta = 0.5\\) models the question: what is the probability of flipping \\(y\\) heads in \\(n\\) flips?\nUsing this information and the facts above, we can easily determine the mean and variance of \\(Y\\).\nUsing our results from Section C.1.1, we can see that \\(E(Y_i) = \\theta\\) for \\(i=1,2,\\ldots,n\\). Similarly, \\(\\mathrm{var}(Y_i)=\\theta(1-\\theta)\\) for \\(i=1,2,\\ldots,n\\).\nWe determine that: \\[\nE(Y)=E\\biggl(\\sum_{i=1}^n Y_i\\biggr)=\\sum_{i=1}^n E(Y_i) = \\sum_{i=1}^n \\theta = n\\theta.\n\\]\nSimilarly, since \\(Y_1, Y_2, \\ldots, Y_n\\) are i.i.d. and using the facts in Section D.6, we see that \\[\n\\mathrm{var}(Y) = \\mathrm{var}(\\sum_{i=1}^n Y_i) = \\sum_{i=1}^n\\mathrm{var}(Y_i)=\\sum_{i=1}^n \\theta(1-\\theta) = n\\theta(1-\\theta).\n\\]",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>D</span>  <span class='chapter-title'>Multivariate distributions</span>"
    ]
  },
  {
    "objectID": "multivariate-distributions.html#sec-continuous-bivariate-distribution-example",
    "href": "multivariate-distributions.html#sec-continuous-bivariate-distribution-example",
    "title": "Appendix D — Multivariate distributions",
    "section": "D.8 Example (Continuous bivariate distribution)",
    "text": "D.8 Example (Continuous bivariate distribution)\nHydration is important for health. Like many people, the author has a water bottle he uses to say hydrated through the day and drinks several liters of water per day. Let’s say the author refills his water bottle every 3 hours. Let \\(Y\\) denote the proportion of the water bottle filled with water at the beginning of the 3-hour window. Let \\(X\\) denote the amount of water the author consumes in the 3-hour window (measured in the the proportion of total water bottle capacity). We know that \\(0\\leq X \\leq Y \\leq 1\\). The joint density of the random variables is \\[\nf(x,y)=4y^2,\\quad 0 \\leq x\\leq y\\leq 1,\n\\] and 0 otherwise.\nWe answer a series of questions about this distribution.\nQ1: Determine \\(P(0.5\\leq X\\leq 1, 0.75\\leq Y)\\).\nNote that the comma between the two events means “and”.\nSince \\(X\\) must be no more than \\(Y\\), we can answer this question as \\[\n\\int_{3/4}^{1} \\int_{1/2}^{y} 4y^2\\;dx\\;dy=229/768\\approx 0.30.\n\\]\nQ2: Determine the marginal distributions of \\(X\\) and \\(Y\\).\nTo find the marginal distribution of \\(X\\), we must integrate the joint pdf with respect to the limits of \\(Y\\). Don’t forget to include the support of the pdf of \\(X\\) (which after integrating out \\(Y\\), must be between 0 and 1). \\[\n\\begin{aligned}\nf_X(x) &=\\int_{x}^1 4y^2\\;dy \\\\\n&=\\frac{4}{3}(1-x^3),\\quad 0\\leq x \\leq 1.\n\\end{aligned}\n\\]\nSimilarly, \\[\n\\begin{aligned}\nf_Y(y) &=\\int_{0}^y 4y^2\\;dx \\\\\n&=4y^3,\\quad 0\\leq y \\leq 1.\n\\end{aligned}\n\\]\nQ3: Determine the means of \\(X\\) and \\(Y\\).\nThe mean of \\(X\\) is the integral of \\(x f_X(x)\\) over the support of \\(X\\), i.e., \\[\nE(X) =\\int_{0}^1 x\\biggl(\\frac{4}{3}(1-x^3)\\biggr)\\;dx = \\frac{2}{5}\n\\]\nSimilarly, \\[\nE(Y) =\\int_{0}^1 y(4y^3)\\;dy = \\frac{4}{5}\n\\]\nQ4: Determine the variances of \\(X\\) and \\(Y\\).\nWe use the formula \\(\\mathrm{var}(X)=E(X^2)-[E(X)^2]\\) to compute the variances. First, \\[\nE(X^2) =\\int_{0}^1 x^2\\biggl(\\frac{4}{3}(1-x^3)\\biggr)\\;dx = \\frac{2}{9}.\n\\] Second, \\[\nE(Y^2) =\\int_{0}^1 y^2(4y^3)\\;dy = \\frac{2}{3}.\n\\] Thus, \\[\n\\mathrm{var}(X)=\\frac{2}{9}-\\biggl(\\frac{2}{5}\\biggr)^2=\\frac{14}{225}.\n\\] \\[\n\\mathrm{var}(Y)=\\frac{2}{3}-\\biggl(\\frac{4}{5}\\biggr)^2=\\frac{2}{75}.\n\\]\nQ5: Determine the mean of \\(XY\\).\nThe mean of \\(XY\\) requires us to integrate the product of \\(xy\\) and the joint pdf over the joint support of \\(X\\) and \\(Y\\). Specifically, \\[\nE(XY)=\\int_{0}^{1}\\int_{0}^{y} xy(4y^2)\\;dx\\;dy= \\frac{1}{3}.\n\\]\nQ6: Determine the covariance of \\(X\\) and \\(Y\\).\nUsing our previous work, we see that \\[\n\\mathrm{cov}(X,Y)=E(XY) - E(X)E(Y)=\\frac{1}{3}-\\frac{2}{5}\\cdot\\frac{4}{5}=\\frac{1}{75}.\n\\]\nQ7: Determine the mean and variance of \\(Y-X\\), i.e., the average amount of water remaining after a 3-hour window and the variability of that amount.\nUsing the results in Section D.6, we have that \\[\nE(Y-X)=E(Y)-E(X)=4/5-2/5=2/5,\n\\] and \\[\n\\mathrm{var}(Y-X)=\\mathrm{var}(Y)+\\mathrm{var}(X)-2\\mathrm{cov}(Y,X)=\n\\frac{2}{75}+\\frac{14}{225}-2\\cdot\\frac{1}{75}=\\frac{14}{225}.\n\\]",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>D</span>  <span class='chapter-title'>Multivariate distributions</span>"
    ]
  },
  {
    "objectID": "random-vectors.html",
    "href": "random-vectors.html",
    "title": "Appendix E — Random vectors",
    "section": "",
    "text": "E.1 Definition\nA random vector is a vector of random variables. A random vector is assumed to be a column vector unless otherwise specified.\nAdditionally, a random matrix is a matrix of random variables.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>E</span>  <span class='chapter-title'>Random vectors</span>"
    ]
  },
  {
    "objectID": "random-vectors.html#mean-variance-and-covariance",
    "href": "random-vectors.html#mean-variance-and-covariance",
    "title": "Appendix E — Random vectors",
    "section": "E.2 Mean, variance, and covariance",
    "text": "E.2 Mean, variance, and covariance\nLet \\(\\mathbf{y}=[Y_1,Y_2,\\dots,Y_n]\\) be an \\(n\\times1\\) vector.\nThe mean of a random vector is the vector containing the means of the random variables in the vector. More specifically, the mean of \\(\\mathbf{y}\\) is defined as \\[\nE(\\mathbf{y})=\\begin{bmatrix}E(Y_1)\\\\E(Y_2)\\\\\\vdots\\\\E(Y_n)\\end{bmatrix}.\n\\]\nThe variance of a random vector isn’t a number or a vector of numbers. Instead, it is the matrix of covariances of all pairs of random variables in the random vector. The variance matrix of \\(\\mathbf{y}\\) is defined as \\[\n\\begin{aligned}\n\\mathrm{var}(\\mathbf{y}) &= E\\Bigl[(\\mathbf{y} - E(\\mathbf{y}))(\\mathbf{y} - E(\\mathbf{y}))^T\\Bigr]\\\\\n&= \\begin{bmatrix}\\mathrm{cov}(Y_1, Y_1) & \\mathrm{cov}(Y_1,Y_2) &\\dots &\\mathrm{cov}(Y_1,Y_n)\\\\\\mathrm{cov}(Y_2,Y_1 )&\\mathrm{cov}(Y_2, Y_2)&\\dots&\\mathrm{cov}(Y_2,Y_n)\\\\\\vdots&\\vdots&\\vdots&\\vdots\\\\\n\\mathrm{cov}(Y_n,Y_1)&\\mathrm{cov}(Y_n,Y_2)&\\dots&\\mathrm{cov}(Y_n, Y_n)\\end{bmatrix}.\n\\end{aligned}\n\\] The variance matrix of \\(\\mathbf{y}\\) can also be computed as \\[\n\\mathrm{var}(\\mathbf{y}) = E(\\mathbf{y}\\mathbf{y}^T)-E(\\mathbf{y})E(\\mathbf{y})^T.\n\\] Also, noting that \\(\\mathrm{cov}(Y, Y) = \\mathrm{var}(Y)\\) for a random variable \\(Y\\), we can write the variance matrix of \\(\\mathbf{y}\\) as \\[\n\\begin{aligned}\n\\mathrm{var}(\\mathbf{y}) &= \\begin{bmatrix}\\mathrm{var}(Y_1) & \\mathrm{cov}(Y_1,Y_2) &\\dots &\\mathrm{cov}(Y_1,Y_n)\\\\\n\\mathrm{cov}(Y_2,Y_1 )&\\mathrm{var}(Y_2) &\\dots& \\mathrm{cov}(Y_2,Y_n) \\\\\n\\vdots&\\vdots&\\ddots&\\vdots\\\\\n\\mathrm{cov}(Y_n,Y_1)&\\mathrm{cov}(Y_n,Y_2)&\\dots&\\mathrm{var}(Y_n)\\end{bmatrix}.\n\\end{aligned}\n\\] The variance matrix of \\(\\mathbf{y}\\) is also called the covariance matrix or variance-covariance matrix of \\(\\mathbf{y}\\).\nLet \\(\\mathbf{x} = [X_1, X_2, \\ldots, X_n]\\) be an \\(n\\times 1\\) random vector. The covariance matrix between \\(\\mathbf{x}\\) and \\(\\mathbf{y}\\) is defined as \\[\n\\begin{align}\n\\mathrm{cov}(\\mathbf{x}, \\mathbf{y}) &= E\\bigl[(\\mathbf{x} - E(\\mathbf{x}))(\\mathbf{y}  - E(\\mathbf{y})^T\\bigr]\\\\\n&=\n\\begin{bmatrix}\n\\mathrm{cov}(X_1, Y_1) & \\mathrm{cov}(X_1,Y_2) &\\dots &\\mathrm{cov}(X_1,Y_n) \\\\\n\\mathrm{cov}(X_2,Y_1 )&\\mathrm{cov}(X_2, Y_2) &\\dots& \\mathrm{cov}(X_2,Y_n) \\\\\n\\vdots&\\vdots&\\dots&\\vdots\\\\\n\\mathrm{cov}(X_n,Y_1)&\\mathrm{cov}(X_n,Y_2)&\\dots&\\mathrm{cov}(X_n, Y_n)\n\\end{bmatrix}.\n\\end{align}\n\\] The covariance matrix between \\(\\mathbf{x}\\) and \\(\\mathbf{y}\\) may also be computed as \\(\\mathrm{cov}(\\mathbf{x}, \\mathbf{y}) = E(\\mathbf{x}\\mathbf{y}^T) - E(\\mathbf{x}) E(\\mathbf{y})^T.\\)\nWe note that \\(\\mathrm{var}(\\mathbf{y})=\\mathrm{cov}(\\mathbf{y}, \\mathbf{y})\\).",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>E</span>  <span class='chapter-title'>Random vectors</span>"
    ]
  },
  {
    "objectID": "random-vectors.html#properties-of-transformations-of-random-vectors",
    "href": "random-vectors.html#properties-of-transformations-of-random-vectors",
    "title": "Appendix E — Random vectors",
    "section": "E.3 Properties of transformations of random vectors",
    "text": "E.3 Properties of transformations of random vectors\nDefine:\n\n\\(\\mathbf{a}\\) to be an \\(n\\times 1\\) vector of real numbers.\n\\(\\mathbf{A}\\) to be an \\(m\\times n\\) matrix of real numbers.\n\\(\\mathbf{x}=[X_1,X_2,\\ldots,X_n]\\) to be an \\(n\\times 1\\) random vector.\n\\(\\mathbf{y}=[Y_1,Y_2,\\ldots,Y_n]\\) to be an \\(n\\times 1\\) random vector.\n\\(\\mathbf{z}=[Z_1,Z_2,\\ldots,Z_n]\\) to be an \\(n\\times 1\\) random vector.\n\\(0_{m\\times n}\\) to be an \\(m\\times n\\) matrix of zeros.\n\nThen:\n\n\\(E(\\mathbf{a}) = \\mathbf{a}\\).\n\\(E(\\mathbf{A}\\mathbf{y})=\\mathbf{A}E(\\mathbf{y})\\).\n\\(E(\\mathbf{y}\\mathbf{A}^T )=E(\\mathbf{y}) \\mathbf{A}^T\\).\n\\(E(\\mathbf{x}+\\mathbf{y})=E(\\mathbf{x})+E(\\mathbf{y})\\).\n\\(\\mathrm{var}(\\mathbf{A}\\mathbf{y})=\\mathbf{A}\\mathrm{var}(\\mathbf{y}) \\mathbf{A}^T\\).\n\\(\\mathrm{cov}(\\mathbf{x}+\\mathbf{y},\\mathbf{z})=\\mathrm{cov}(\\mathbf{x},\\mathbf{z})+\\mathrm{cov}(\\mathbf{y},\\mathbf{z})\\).\n\\(\\mathrm{cov}(\\mathbf{x},\\mathbf{y}+\\mathbf{z})=\\mathrm{cov}(\\mathbf{x},\\mathbf{y})+\\mathrm{cov}(\\mathbf{x},\\mathbf{z})\\).\n\\(\\mathrm{cov}(\\mathbf{A}\\mathbf{x},\\mathbf{y})=\\mathbf{A}\\ \\mathrm{cov}(\\mathbf{x},\\mathbf{y})\\).\n\\(\\mathrm{cov}(\\mathbf{x},\\mathbf{A}\\mathbf{y})=\\mathrm{cov}(\\mathbf{x},\\mathbf{y}) \\mathbf{A}^T\\).\n\\(\\mathrm{var}(\\mathbf{a})= 0_{n\\times n}\\).\n\\(\\mathrm{cov}(\\mathbf{a},\\mathbf{y})=0_{n\\times n}\\).\n\\(\\mathrm{var}(\\mathbf{a}+\\mathbf{y})=\\mathrm{var}(\\mathbf{y})\\).",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>E</span>  <span class='chapter-title'>Random vectors</span>"
    ]
  },
  {
    "objectID": "random-vectors.html#example-continuous-bivariate-distribution-continued",
    "href": "random-vectors.html#example-continuous-bivariate-distribution-continued",
    "title": "Appendix E — Random vectors",
    "section": "E.4 Example (Continuous bivariate distribution continued)",
    "text": "E.4 Example (Continuous bivariate distribution continued)\nUsing the definitions and results for random vectors, we want to answer Q7 of the hydration example in Section D.8. Summarizing only the essential details, we have a \\(2\\times 1\\) random vector \\(\\mathbf{z}=[X, Y]\\) with mean \\(E(\\mathbf{z})=[2/5, 4/5]\\) and covariance matrix \\[\n\\mathrm{var}(\\mathbf{z})=\n\\begin{bmatrix}\n14/225 & 1/75 \\\\\n1/75 & 2/75\n\\end{bmatrix}.\n\\]\nWe want to determine \\(E(Y-X)\\) and \\(\\mathrm{var}(Y-X)\\).\nDefine \\(\\mathbf{A}=[-1, 1]^T\\) (the ROW vector with 1 and -1). Then, \\[\n\\mathbf{Az}=\\begin{bmatrix}-1 & 1\\end{bmatrix}\n\\begin{bmatrix}\nX\\\\\nY\n\\end{bmatrix}\n=Y-X\n\\] and \\[\n\\begin{aligned}\nE(Y-X)&=E(\\mathbf{Az})\\\\\n&=\\begin{bmatrix}-1 & 1\\end{bmatrix}\n\\begin{bmatrix}\n2/5\\\\\n4/5\n\\end{bmatrix}\\\\\n&=-2/5+4/5\\\\\n&=2/5.\n\\end{aligned}\n\\] Additionally, \\[\n\\begin{aligned}\n& \\mathrm{var}(Y-X) \\\\\n&=\\mathrm{var}(\\mathbf{Az}) \\\\\n&=\\mathbf{A}\\mathrm{var}(\\mathbf{z})\\mathbf{A}^T \\\\\n&=\n\\begin{bmatrix}\n-1 & 1\n\\end{bmatrix}\n\\begin{bmatrix}\n14/225 & 1/75 \\\\\n1/75 & 2/75\n\\end{bmatrix}\n\\begin{bmatrix}\n-1 \\\\ 1\n\\end{bmatrix} \\\\\n&= \\begin{bmatrix}\n-14/225+1/75 & -1/75+2/75\n\\end{bmatrix}\n\\begin{bmatrix}\n-1 \\\\ 1\n\\end{bmatrix} \\\\\n&= 14/225 - 1/75 -1/75 + 2/75 \\\\\n&=14/225.\n\\end{aligned}\n\\]",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>E</span>  <span class='chapter-title'>Random vectors</span>"
    ]
  },
  {
    "objectID": "random-vectors.html#multivariate-normal-gaussian-distribution",
    "href": "random-vectors.html#multivariate-normal-gaussian-distribution",
    "title": "Appendix E — Random vectors",
    "section": "E.5 Multivariate normal (Gaussian) distribution",
    "text": "E.5 Multivariate normal (Gaussian) distribution\nThe random vector \\(\\mathbf{y}=[Y_1,\\dots,Y_n]\\) has a multivariate normal distribution with mean \\(E(\\mathbf{y})=\\boldsymbol{\\mu}\\) (an \\(n\\times 1\\) vector) and covariance matrix \\(\\mathrm{var}(\\mathbf{y})=\\boldsymbol{\\Sigma}\\) (an \\(n\\times n\\) matrix) if its joint pdf is \\[\nf(\\mathbf{y})=\\frac{1}{(2\\pi)^{n/2} |\\boldsymbol{\\Sigma}|^{1/2} }  \\exp\\left(-\\frac{1}{2} (\\mathbf{y}-\\boldsymbol{\\mu})^T \\boldsymbol{\\Sigma}^{-1} (\\mathbf{y}-\\boldsymbol{\\mu})\\right),\n\\] where \\(|\\boldsymbol{\\Sigma}|\\) is the determinant of \\(\\boldsymbol{\\Sigma}\\). Note that \\(\\boldsymbol{\\Sigma}\\) must be symmetric and positive definite.\nIn this case, we denote the distribution of \\(\\mathbf{y}\\) as \\[\n\\mathbf{y}\\sim \\mathsf{N}(\\boldsymbol{\\mu},\\boldsymbol{\\Sigma}).\n\\]",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>E</span>  <span class='chapter-title'>Random vectors</span>"
    ]
  },
  {
    "objectID": "random-vectors.html#linear-transformation-of-a-multivariate-normal-random-vector",
    "href": "random-vectors.html#linear-transformation-of-a-multivariate-normal-random-vector",
    "title": "Appendix E — Random vectors",
    "section": "E.6 Linear transformation of a multivariate normal random vector",
    "text": "E.6 Linear transformation of a multivariate normal random vector\nAssume that \\(\\mathbf{y}\\) is an \\(n\\times 1\\) random vector and \\(\\mathbf{y}\\sim \\mathsf{N}(\\boldsymbol{\\mu},\\boldsymbol{\\Sigma}).\\) Also, assume that \\(\\mathbf{a}\\) is an \\(m\\times 1\\) vector of real numbers and \\(\\mathbf{A}\\) is an \\(m\\times n\\) matrix of real numbers.\nA linear transformation of a multivariate normal random vector of the form \\(\\mathbf{a}+\\mathbf{A}\\mathbf{y}\\) is also a multivariate normal random vector (though it could collapse to a single random variable if \\(\\mathbf{A}\\) is a \\(1\\times n\\) vector).\nApplication: Suppose that \\(\\mathbf{y}\\sim \\mathsf{N}(\\boldsymbol{\\mu},\\boldsymbol{\\Sigma})\\). For an \\(m\\times n\\) matrix of constants \\(\\mathbf{A}\\), \\(\\mathbf{A}\\mathbf{y}\\sim \\mathsf{N}(\\mathbf{A}\\boldsymbol{\\mu},\\mathbf{A}\\boldsymbol{\\Sigma} \\mathbf{A}^T)\\).\nThe most common estimators used in linear regression are linear combinations of a (typically) multivariate normal random vectors, meaning that many of the estimators also have a (multivariate) normal distribution.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>E</span>  <span class='chapter-title'>Random vectors</span>"
    ]
  },
  {
    "objectID": "random-vectors.html#example-ols-matrix-form",
    "href": "random-vectors.html#example-ols-matrix-form",
    "title": "Appendix E — Random vectors",
    "section": "E.7 Example (OLS matrix form)",
    "text": "E.7 Example (OLS matrix form)\nOrdinary least squares regression is a method for fitting a linear regression model to data. Suppose that we have observed variables \\(X_1, X_2, X_3, \\ldots, X_{p-1}, Y\\) for each of \\(n\\) subjects from some population, with \\(X_{i,j}\\) denoting the value of \\(X_j\\) for observation \\(i\\) and \\(Y_i\\) denoting the value of \\(Y\\) for observation \\(i\\). In general, we want to use \\(X_1, \\ldots, X_{p-1}\\) to predict the value of \\(Y\\). Define \\(\\mathbf{X}\\) to be a full-rank \\(n\\times p\\) matrix as \\[\n\\mathbf{X} =\n\\begin{bmatrix}\n1 & X_{1,1} & X_{1,2} & \\cdots & X_{1,p-1} \\\\\n1 & X_{2,1} & X_{2,2} & \\cdots & X_{2,p-1} \\\\\n\\vdots & \\vdots & \\vdots & \\vdots & \\vdots \\\\\n1 & X_{n,1} & X_{n,2} & \\cdots & X_{n,p-1}\n\\end{bmatrix}\n\\] Define \\(\\mathbf{y}\\) to be the \\(n\\times 1\\) random vector of responses as \\(\\mathbf{y}=[Y_1, Y_2, \\ldots,Y_n]\\).\nAs is common in regression, we assume that, \\[\n\\mathbf{y}\\sim \\mathsf{N}(\\mathbf{X}\\boldsymbol{\\beta}, \\sigma^2 \\mathbf{I}_{n\\times n}),\n\\] where \\(\\boldsymbol{\\beta}=[\\beta_0,\\beta_1,\\ldots,\\beta_{p-1}]\\) is a \\(p\\times 1\\) vector of real numbers.\nThe matrix \\(\\mathbf{H}=\\mathbf{X}(\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\) projects \\(\\mathbf{y}\\) into the space spanned by the vectors in \\(\\mathbf{X}\\).\nDetermine the distribution of \\(\\mathbf{Hy}\\).\nNotice that \\[\n\\begin{aligned}\nE(\\mathbf{Hy}) &= \\mathbf{H}E(\\mathbf{y}) \\\\  &=\\mathbf{HX}\\boldsymbol{\\beta}\\\\\n&=\\mathbf{X}(\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{X}\\boldsymbol{\\beta} \\\\\n&=\\mathbf{X}\\mathbf{I}_{p\\times p}\\boldsymbol{\\beta} \\\\\n&=\\mathbf{X}\\boldsymbol{\\beta}.\n\\end{aligned}\n\\]\nAdditionally, \\[\n\\begin{aligned}\n\\mathrm{var}(\\mathbf{Hy}) &= \\mathbf{H}\\mathrm{var}(\\mathbf{y})\\mathbf{H}^T \\\\\n&=\\mathbf{H}\\sigma^2 \\mathbf{I}_{n\\times n}\\mathbf{\\mathbf{H^T}} \\\\\n&= \\sigma^2 \\mathbf{H}\\mathbf{H}^T\\\\\n&=\\sigma^2 \\mathbf{H}.\n\\end{aligned}\n\\]\nLastly, since \\(\\mathbf{Hy}\\) is a linear transformation of a multivariate normal random vector, it also have a multivariate normal distribution.\nWe combine these facts together to see that\n\\[\n\\mathbf{Hy}\\sim \\mathsf{N}(\\mathbf{H}\\boldsymbol{\\beta}, \\sigma^2 \\mathbf{H}).\n\\]",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>E</span>  <span class='chapter-title'>Random vectors</span>"
    ]
  },
  {
    "objectID": "estimation-inference-review.html",
    "href": "estimation-inference-review.html",
    "title": "Appendix F — Review of Estimation, Hypothesis Testing, and Confidence Intervals",
    "section": "",
    "text": "F.1 Estimation\nA parameter is a numeric characteristic that describes a population. E.g., the population mean, standard deviation, or cumulative distribution function.\nThe target parameter or parameter of interest is the population parameter we would like to estimate.\nThere are different kinds of estimates:\nAn estimate and an estimator are different but related concepts. An estimate is a specific number (for a point estimate) or a specific range of numbers (for an interval estimate). Once the data are observed, an estimate is fixed. An estimator is a formula we use to calculate an estimate once we get a sample of data. An estimator is a random variable. An estimator produces different estimates based on the sample of data we obtain from the population.\nThe sampling distribution of an estimator is the distribution of the estimates we get when we use the estimator to compute estimates from all possible samples of a fixed size \\(n\\) from the population of interest.\nA point estimator, \\(\\hat{\\theta}\\), is an unbiased estimator of a target parameter, \\(\\theta\\), if \\(E(\\hat{\\theta})=\\theta\\). An estimator is biased if it is not biased.\nThe bias of an estimator is defined as\n\\[\nB(\\hat{\\theta})=E(\\hat{\\theta})-\\theta.\n\\]\nThe variance of an estimator is defined as\n\\[\n\\mathrm{var}(\\hat{\\theta})=E[\\hat{\\theta}-E(\\hat{\\theta})]^2.\n\\]\nThe standard error of an estimator is the standard deviation of the estimator, i.e., \\[\n\\mathrm{se}(\\hat{\\theta})\\equiv\\mathrm{sd}(\\hat{\\theta})=\\sqrt{\\mathrm{var}(\\hat{\\theta})}.\n\\]\nTypically, we cannot compute the standard error of an estimator because it is a function of parameters that we do not know. Instead, we use the sample data to estimate the standard error. When we hear or read the term “standard error”, we must carefully determine whether the “standard error” presented is the theoretical standard error or the estimated standard error (and it’s nearly always the latter).\nThe mean square error of a point estimator is \\[\nMSE(\\hat{\\theta})=E(\\hat{\\theta}-\\theta)^{2},\n\\] which is equivalent to \\[\nMSE(\\hat{\\theta})=\\mathrm{var}(\\hat{\\theta})+[B(\\hat{\\theta})]^{2}.\n\\]\nThe MSE formula makes it clear that there is a “bias-variance trade off” when choosing between point estimators. Typically, unbiased point estimators will have larger variance (and correspondingly, MSE). Biased estimators will often have smaller MSE, but are (obviously) biased. It’s a trade off we have to balance.\nExample\nLet \\(Y_1, Y_2, \\ldots, Y_n \\stackrel{i.i.d.}{\\sim} \\mathsf{N}(\\mu, \\sigma^2)\\).\nDetermine the mean and variance of the estimator \\(\\hat{\\mu}=\\bar{Y}\\).\nThe mean of the estimator is\n\\[\n\\begin{aligned}\nE(\\bar{Y}) &= E\\biggl(\\frac{1}{n}\\sum_{i=1}^n Y_i \\biggr) & \\tiny\\text{(substitute definition of $\\bar{Y}$)} \\\\\n&= \\frac{1}{n}E\\biggl(\\sum_{i=1}^n Y_i \\biggr) & \\tiny\\text{(factor constant)} \\\\\n&= \\frac{1}{n}\\sum_{i=1}^n E(Y_i) & \\tiny\\text{(linearity of expectation)} \\\\\n&= \\frac{1}{n}\\sum_{i=1}^n \\mu  & \\tiny\\text{(assumption about mean of $Y_i$ for $i=1,2,\\ldots,n$)} \\\\\n&= \\frac{n}{n} \\mu  & \\tiny\\text{(sum $\\mu$ $n$ times)} \\\\\n&= \\mu. &\n\\end{aligned}\n\\]\nThe variance of the estimator is\n\\[\n\\begin{aligned}\n\\mathrm{var}(\\bar{Y}) &= \\mathrm{var}\\biggl(\\frac{1}{n}\\sum_{i=1}^n Y_i \\biggr) & \\tiny\\text{(substitute definition of $\\bar{Y}$)} \\\\\n&= \\frac{1}{n^2}\\mathrm{var}\\biggl(\\sum_{i=1}^n Y_i \\biggr) & \\tiny\\text{(factor constant out of $\\mathrm{var}$)} \\\\\n&= \\frac{1}{n^2}\\sum_{i=1}^n \\mathrm{var}(Y_i) & \\tiny\\text{(linearity of variance for independent random variables)} \\\\\n&= \\frac{1}{n^2}\\sum_{i=1}^n \\sigma^2  & \\tiny\\text{(assumption about variance of $Y_i$ for $i=1,2,\\ldots,n$)} \\\\\n&= \\frac{n}{n^2} \\sigma^2  & \\tiny\\text{(sum $\\sigma^2$ $n$ times)} \\\\\n&= \\frac{\\sigma^2}{n}. &\n\\end{aligned}\n\\] Is the sample median a better or worse estimator of \\(\\mu\\) than the sample mean in this context? To prove this, we would have to consider order statistics, which we will not do. Instead, we will investigate this using a simulation experiment.\nWe will sample 100 values from our population assuming \\(\\mu=0\\) and \\(\\sigma^2 = 100\\) and compare the properties of the sample mean and sample median for 10,000 independent samples. We start by drawing 10,000 samples of size 100 from a \\(\\mathsf{N}(0, 10^2)\\) population, computing the sample mean and median, and then storing these values for further analysis.\nset.seed(1904) # set seed for reproducibility\nnsamples &lt;- 10000 # number of independent samples\nn &lt;- 100 # sample size\n# store sample means and medians\nsample_means &lt;- numeric(nsamples)\nsample_meds &lt;- numeric(nsamples)\n# perform experiment nsamples times\nfor (i in seq_len(nsamples)) {\n  # draw sample\n  y &lt;- rnorm(n, mean = 0, sd = 10)\n  # compute sample mean\n  sample_means[i] &lt;- mean(y)\n  # compute and store sample median\n  sample_meds[i] &lt;- median(y)\n}\nWe now plot the approximate sampling distributions of the sample mean and median using relative frequency histograms of their values from the 10,000 samples. Setting the probability argument of the hist function to TRUE will produce a relative frequency histogram instead of the default frequency histogram. We also set the breaks argument to 100 to ask the hist function to use approximately 100 bins.\npar(mfrow = c(1, 2)) # side-by-side histograms\nhist(sample_means, probability = TRUE, breaks = 100,\n     main = \"sampling distribution of\\n the sample means\")\nhist(sample_meds, probability = TRUE, breaks = 100,\n     main = \"sampling distribution of\\n the sample medians\")\n\n\n\n\n\n\n\npar(mfrow = c(1, 1))\nBoth sampling distributions appear to be approximately normal (and from statistical theory, we know that they are normally distributed). We compute the sample mean and standard deviation of the two sets of statistics to approximate the expected value (mean) and standard error, respectively, of the statistics. The estimated mean of the sampling distribution of the sample mean is 0.01, as shown below. We showed that the theoretical mean is 0, so this is a good approximation.\nmean(sample_means) # estimated mean of statistic\n\n[1] 0.01409124\nThe estimated standard error of the sampling distribution of the sample mean is 1.00, as shown below. Recall that the theoretical standard error is 1 (\\(\\sigma^2/n = 100/100 = 1\\)), so this is a good approximation.\nsd(sample_means) # estimated standard error of statistic\n\n[1] 0.9958627\nThe estimated mean of the sampling distribution of the sample median is 0.01, as shown below. The theoretical mean is 0.\nmean(sample_meds)\n\n[1] 0.01444009\nThe estimated standard error of the sampling distribution of the sample median is 1.24, as shown below. The asymptotic standard error is 1.2533, i.e., the theoretical standard error as \\(n\\rightarrow\\infty\\) (see Wackerly, Mendhall, and Scheaffer 2008, sec. 9.2).\nsd(sample_meds)\n\n[1] 1.235693\nSince both estimators are unbiased and the standard error of the sample mean is smaller than the sample median, this demonstrates that the sample mean is a more efficient estimator than the sample median in this context.\nWe overlay a normal curve (the solid orange lines) on each sampling distribution, using the mean and standard errors computed above for the mean and standard deviation, respectively, of the normal curve. Both curves fit well.\npar(mfrow = c(1, 2)) # side-by-side histograms\ns &lt;- seq(-10, 10, len = 10000) # sequence for plotting\nhist(sample_means, probability = TRUE, breaks = 100, col = \"lightgrey\",\n     main = \"sampling distribution of\\n the sample means\")\n# normal curve with estimated mean and standard error\nlines(s, dnorm(s, mean = mean(sample_means), sd = sd(sample_means)), \n      col = \"orange\", lwd = 2)\nhist(sample_meds, probability = TRUE, breaks = 100, col = \"lightgrey\",\n     main = \"sampling distribution of\\n the sample medians\")\nlines(s, dnorm(s, mean = mean(sample_meds), sd = sd(sample_meds)), \n      col = \"orange\", lwd = 2)\n\n\n\n\n\n\n\npar(mfrow = c(1, 1))",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>F</span>  <span class='chapter-title'>Review of Estimation, Hypothesis Testing, and Confidence Intervals</span>"
    ]
  },
  {
    "objectID": "estimation-inference-review.html#estimation",
    "href": "estimation-inference-review.html#estimation",
    "title": "Appendix F — Review of Estimation, Hypothesis Testing, and Confidence Intervals",
    "section": "",
    "text": "A point estimate is a single number that we hope is close to the true value of the target parameter.\nAn interval estimate is an interval of numbers that we hope will contain the target parameter.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>F</span>  <span class='chapter-title'>Review of Estimation, Hypothesis Testing, and Confidence Intervals</span>"
    ]
  },
  {
    "objectID": "estimation-inference-review.html#hypothesis-testing",
    "href": "estimation-inference-review.html#hypothesis-testing",
    "title": "Appendix F — Review of Estimation, Hypothesis Testing, and Confidence Intervals",
    "section": "F.2 Hypothesis Testing",
    "text": "F.2 Hypothesis Testing\nA statistical test of hypotheses or hypothesis test is a statistical procedure used to decide between a null hypothesis, \\(H_0\\), and an alternative hypothesis, \\(H_a\\) or \\(H_1\\). The null hypothesis is usually a hypothesis that “nothing interesting is going on”. The alternative hypothesis is generally the complement of the null hypothesis and is usually what we want to show is true.\nA test statistic is a number used to decide between \\(H_0\\) and \\(H_a\\). A test statistic is a function of the data, and generally, parameters in the hypotheses. A test statistic measures the compatibility of the observed data with \\(H_0\\). A “small” test statistic suggests the observed data are consistent with \\(H_0\\). An “extreme” test statistic indicates that the observed data are inconsistent with \\(H_0\\), which we take as evidence that \\(H_a\\) is true.\nThe null distribution allows us to identify the values of the test statistic that are typical or unusual when \\(H_0\\) is true. Formally, the null distribution is the distribution of the test statistic under the assumption that \\(H_0\\) is true.\nThere are two types of errors we can make when doing hypothesis testing. A type I error is rejecting \\(H_0\\) when \\(H_0\\) is true. A type II error is failing to reject \\(H_0\\) when \\(H_a\\) is true.\nWe can control the Type I error rate at a specified level, \\(\\alpha\\), called the significance level, since we know the distribution of our test statistic under the assumption that \\(H_0\\) is true. We reject \\(H_0\\) and conclude that \\(H_a\\) is true if the test statistic falls in the rejection region of the null distribution. The rejection region is the set of test statistics that are the \\(100\\alpha\\%\\) most unlikely test statistics if \\(H_0\\) is true.\nInstead of using the test statistic directly to decide between \\(H_0\\) and \\(H_a\\), we generally use the test statistic to compute a p-value. The p-value of a test statistic is the probability of seeing a test statistic at least as supportive of \\(H_a\\) when \\(H_0\\) is true. If we specify the significance level, \\(\\alpha\\), prior to performing our hypothesis test (which is the ethical thing to do), then we reject \\(H_0\\) and conclude \\(H_a\\) is true when the p-value \\(&lt;\\alpha\\). Otherwise, we fail to reject \\(H_0\\).\nResearchers sometimes say that the smaller the p-value, the stronger the evidence that \\(H_a\\) is true and \\(H_0\\) is false. This isn’t definitively true because the p-value doesn’t have the ability to distinguish between the following options: (1) \\(H_0\\) is true but our observed data were very unlikely, (2) \\(H_0\\) is false. When \\(H_0\\) is true, then the test statistic (for simple hypotheses and continuous test statistics) has a uniform distribution over the interval [0, 1]. Conversely, if \\(H_a\\) is true, then the p-value is more likely to be small, which makes us think \\(H_a\\) is true for small p-values. However, unless we know the power of our test, which is the probability that we reject \\(H_0\\) when \\(H_a\\) is true, then it is very difficult to assess how much evidence for \\(H_a\\) a small p-value provides. Gibson (2021) point out the p-values can be interpreted naturally on a \\(\\log_{10}\\) scale. Gibson (2021) states:\n\nThe p-value can be expressed as \\(p=c\\times 10^{-k}\\) so that \\(\\log_{10}(p)=-\\log_{10}(c)+k\\), where \\(c\\) is a constant and \\(k\\) is an integer, which implies that only the magnitude k measures the actual strength of evidence (Boos and Stefanski 2011). \\(\\ldots\\) This would suggest that \\(p=0.01\\) (\\(k=2\\)) could be interpreted as twice the evidence [for \\(H_a\\) as] \\(p=0.10\\) (\\(k=1\\)).\n\nGibson (2021) provides a thorough review of p-value interpretation. Table F.1 summarizes common strength of evidence interpretations for p-values.\n\n\n\n\nTable F.1: A summary of common strength-of-evidence interpretations for p-values.\n\n\n\n\n\n\np-value\nInterpretation\n\n\n\n\n\\(&gt; 0.10\\)\nno evidence for \\(H_a\\)\n\n\n\\(\\leq 0.10\\)\nweak evidence for \\(H_a\\)\n\n\n\\(\\leq 0.05\\)\nmoderate evidence for \\(H_a\\)\n\n\n\\(\\leq 0.01\\)\nstrong evidence for \\(H_a\\)\n\n\n\\(\\leq 0.001\\)\nvery strong evidence for \\(H_a\\)\n\n\n\n\n\n\n\n\nSimulation study\nWe provide a brief simulation study to better understand the null distribution and also how rejection regions are chosen to control the type I error rate.\nAssume that we sample \\(n=10\\) values from a \\(\\mathsf{N}(\\mu, \\sigma^2)\\) population but do not know the mean or standard deviation of the population. We want to test are \\(H_0: \\mu = 2\\) versus \\(H_a: \\mu &gt; 2\\) (implicitly, the null hypothesis is \\(H_0: \\mu \\leq 2\\)). In this context, it is common to use the test statistic \\[\nT^* = \\frac{\\bar{Y} - \\mu}{s/\\sqrt{n}},\n\\] where \\(s\\) is the sample standard deviation of the measurements. If the null hypothesis is true, \\(\\mu=2\\), and statistical theory tells us that \\(T^* \\sim t_{n-1}\\), i.e., the test statistic has a \\(t\\) distribution with \\(n-1\\) degrees of freedom. Since \\(n=10\\), our test statistic has a \\(t\\) distribution with 9 degrees of freedom if the null hypothesis is true.\nWe now describe our simulation experiment. To draw our sample, we must choose a value of \\(\\sigma^2\\). We will use \\(\\sigma^2 = 4^2\\). The exact value isn’t important, but choosing a fixed number for \\(\\sigma^2\\) is critical for the example below. In the code below, we (1) Draw \\(B=1,000\\) samples of size \\(n=10\\) from our \\(\\mathsf{N}(\\mu,4^2)\\) population assuming the null hypothesis is true, i.e., with \\(\\mu = 2\\), and (2) Compute the test statistic for each sample.\n\n# set number seed for reproducible results\nset.seed(12)\n# create vector to store test statistics\ntstats &lt;- numeric(10000)\n# for 10,000 experiments\nfor (i in seq_len(10000)) {\n  # draw a sample of size 10 from a N(2, 4^2)\n  y &lt;- rnorm(n = 10, mean = 2, sd = 4)\n  # compute and store the test statistic for the sample\n  tstats[i] &lt;- (mean(y) - 2) / (sd(y) / sqrt(10))\n}\n\nRecall that the null distribution of a test statistic is its sampling distribution under the assumption that the null hypothesis is true. We now use the code below to compute the empirical density of the computed test statistics (i.e., the estimated sampling distribution) and overlay the density of a \\(t\\) distribution with 9 degrees of freedom. We see that the two distributions match up very well.\n\n# plot empirical null distribution\nplot(density(tstats),\n     xlab = \"test statistic\",\n     ylab = \"density\",\n     main = \"empirical versus true null distribution\")\n# sequence to plot null density over\ns &lt;- seq(-5, 5, len = 1000)\nlines(s, dt(s, df = 9), lty = 2, col = \"blue\")\n\n\n\n\n\n\n\n\nWhat should we takeaway from this example? The null distribution of a hypothesis test is the sampling distribution of the test statistic under the assumption that \\(H_0\\) is true. We approximated the null distribution of our test statistic by drawing 10,000 samples from the population distribution under the assumption that \\(H_0\\) was true.\nHow does the null distribution relate to choosing the rejection region? The type I error rate is the probability of rejecting \\(H_0\\) when \\(H_0\\) is true. Since we know the null distribution, we know what behavior to expect from our test statistic if \\(H_0\\) is true. Specifically, we know what test statistics are most unlikely if \\(H_0\\) is true.\nAssume we want to control the type I error at \\(\\alpha = 0.05\\). For this upper-tailed test, we should reject \\(H_0\\) when the test statistic is greater than \\(t^{0.05}_9\\), i.e., the 0.95 quantile of a \\(t\\) distribution with 9 degrees of freedom.\nWhy do we use this threshold? Because if \\(H_0\\) is true, this will only lead to erroneous rejections of \\(H_0\\) (i.e., a type I error) 5% of the time. Additionally, the test statistics in the rejection are the ones least compatible with the \\(H_0\\) if in fact the \\(H_0\\) is true.\nIn the code below, we compute the sample proportion of test statistics from our null distribution that are more than \\(t^{0.05}_9\\). Our sample proportion is very close to 0.05, and this number will converge to 0.05 as we increase the number samples used in our simulation.\n\nmean(tstats &gt; qt(0.95, df = 9))\n\n[1] 0.0489\n\n\nExample\nSuppose that \\(Y_1,\\ldots,Y_n\\) is a random sample (in other words, an independent and identically distributed sample) from a population having a normal distribution with unknown mean \\(\\mu\\) and variance \\(\\sigma^2=1\\). We would like to decide between the following two hypotheses: \\(H_0:\\mu=0\\) and \\(H_a:\\mu\\neq 0\\).\nIf \\(H_0\\) is true, then the test statistic \\[\nZ^*=\\frac{\\bar{Y}}{1/\\sqrt{n}}=\\sqrt{n}\\bar{Y}\\sim \\mathsf{N}(0, 1),\n\\]\ni.e., the null distribution of \\(Z^*\\) is \\(\\mathsf{N}(0,1)\\). This follows from recognizing that \\(\\bar{Y} \\sim \\mathsf{N}(0,1/n)\\) and the test statistic is simply standardizing \\(\\bar{Y}\\).\nIf \\(\\alpha=0.10\\), then the 10% of test statistics that are most unlikely if \\(H_0\\) is true (i.e., most supportive of \\(H_a\\)) are more extreme than \\(Z^{0.95}\\) and \\(Z^{0.05}\\), the 0.05 and 0.95 quantiles of a standard normal distribution, respectively. The superscript in the quantile notation indicates the area to the right of the quantile in the CDF). The 0.05 quantile of the standard normal distribution is -1.645 and the 0.95 quantile is 1.645. In R, we can find these quantiles using the qnorm function, where the first argument of qnorm is p, which is the vector of probabilities to the left of the quantile. We verify these quantiles using the code below.\n\nqnorm(c(0.05,0.95))\n\n[1] -1.644854  1.644854\n\n\n\\(H_0\\) should be rejected when \\(Z^*\\) is less than -1.645 or more than 1.65, i.e., the rejection region is \\((-\\infty, -1.645) \\cup (1.645,\\infty)\\).\nAlternatively, we can compute the p-value using the formula \\(2P(Z\\geq |Z^*|)\\) in order to make our choice between the hypotheses.\nSuppose \\(z^*=1.74\\) and \\(\\alpha=0.10\\). The test statistic is in the rejection region, so we would conclude that \\(H_a\\) is true. We demonstrate this visually in Figure F.1, shading the rejection region in grey. We represent the observed test statistic with a vertical line.\n\n\n\n\n\n\n\n\nFigure F.1: A plot of the rejection region for a two-tailed alternative hypothesis.\n\n\n\n\n\nThe p-value is \\(2P(Z\\geq 1.74)=0.082\\). In R, we can compute the p-value using the code below.\n\n2 * (1 - pnorm(1.74))\n\n[1] 0.08185902\n\n\nVisually, the p-value is the sum of the area to the left of -1.74 and the area to the right of 1.74 because test statistics with those values are at least as supportive of \\(H_a\\) as the observed statistic. We display this in Figure F.2.\n\n\n\n\n\n\n\n\nFigure F.2: A plot of the area used to compute the p-value based on a two-tailed alternative hypothesis.\n\n\n\n\n\nUsing the p-value approach with \\(\\alpha = 0.10\\), we conclude that \\(H_a\\) is true.\nIn straightforward language, our interpretation could be: there is weak evidence that the population mean differs from 0.\nWe note that the rejection region and p-value approaches to deciding between hypotheses will always agree.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>F</span>  <span class='chapter-title'>Review of Estimation, Hypothesis Testing, and Confidence Intervals</span>"
    ]
  },
  {
    "objectID": "estimation-inference-review.html#confidence-intervals",
    "href": "estimation-inference-review.html#confidence-intervals",
    "title": "Appendix F — Review of Estimation, Hypothesis Testing, and Confidence Intervals",
    "section": "F.3 Confidence Intervals",
    "text": "F.3 Confidence Intervals\nA confidence interval provides us with plausible values of a target parameter. It is the most common type of interval estimator.\nA confidence interval procedure has an associated confidence level. When independent random samples are taken repeatedly from the population, a confidence interval procedure will produce intervals containing the target parameter with probability equal to the confidence level.\nConfidence level is associated with a confidence interval procedure, not a specific interval. A 95% confidence interval procedure will produce intervals that contain the target parameter 95% of the time. A specific interval estimate will either contain the target parameter or it will not.\nThe formulas for confidence intervals are usually derived from a pivotal quantity. A pivotal quantity is a function of the data and the target parameter whose distribution does not depend on the value of the target parameter.\nExample:\nSuppose \\(Y_1,Y_2,\\ldots,Y_n \\stackrel{i.i.d.}{\\sim} \\mathsf{N}(\\mu, 1)\\). The random variable \\(Z=(\\bar{Y}-\\mu)/(1/\\sqrt{n})\\sim \\mathsf{N}(0,1)\\) is a pivotal quantity. Since \\(P(-1.96\\leq Z\\leq 1.96)=0.95\\), we can derive that \\[\nP(\\bar{Y}-1.96\\times 1/\\sqrt{n}\\leq \\mu \\leq \\bar{Y}+1.96\\times 1/\\sqrt{n})=0.95.\n\\]\nOur 95% confidence interval for \\(\\mu\\) in this context is \\[\n[\\bar{Y}-1.96\\times 1/\\sqrt{n}, \\bar{Y}+1.96\\times 1/\\sqrt{n}].\n\\tag{F.1}\\] If \\(\\bar{Y}=0.551\\) and \\(n=10\\), then the associated 95% confidence interval for \\(\\mu\\) is [-0.070,1.171].\nMore discussion of confidence level\nThe CI procedure provided in Equation F.1 is supposed to produce 95% confidence intervals (i.e., the confidence level of the procedure is 0.95). If we produce 100 intervals from independent data sets, then about 95% of them should contain the true mean, but about 5% will not.\nTo illustrate this further, we use a small simulation example to produce 100 95% confidence intervals using samples of size \\(n = 10\\) from a \\(\\mathsf{N(0,1)}\\) population. First, we obtain 100 samples of size 10 from the population \\(\\mathsf{N(0,1)}\\) and then compute the sample mean of each sample. We do this in the code below.\n\nset.seed(33) # set number seed for reproducibility\n# create vector to store sample means\nmeans &lt;- numeric(100)\n# perform experiment 100 times\nfor (i in seq_len(100)) {\n  # draw sample of size 10 from N(0, 1) and compute sample mean\n  means[i] &lt;- mean(rnorm(n = 10, mean = 0, sd = 1))\n}\n\nNext, we use Equation F.1 to determine the lower and upper bound of the confidence interval associated with each interval.\n\n#calculate the lower and upper bounds for the 95% CIs\nlb = means - 1.96 * 1/sqrt(10)\nub = means + 1.96 * 1/sqrt(10)\n\nFigure F.3 displays each interval. An interval is orange if the interval doesn’t contain the true population mean, which is 0. In this example, 94 out of 100 intervals for the population mean contained the true population mean of 0. As we construct more intervals, the proportion of intervals containing the true mean of 0 will typically get closer to 95%.\n\n\n\n\n\n\n\n\nFigure F.3: A plot of 100 95% confidence intervals for a population mean produced from independent samples from a \\(\\mathsf{N}(0,1)\\) population\n\n\n\n\n\nNotice how the intervals move around. This is because each sample provides us with slightly different values, so the intervals move around because of the samples obtained.\nEach interval either contains the true mean of 0 or it does not. But as a whole, the procedure we are using will produce confidence intervals that contain the true mean 95% of the time.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>F</span>  <span class='chapter-title'>Review of Estimation, Hypothesis Testing, and Confidence Intervals</span>"
    ]
  },
  {
    "objectID": "estimation-inference-review.html#linking-hypothesis-tests-and-confidence-intervals",
    "href": "estimation-inference-review.html#linking-hypothesis-tests-and-confidence-intervals",
    "title": "Appendix F — Review of Estimation, Hypothesis Testing, and Confidence Intervals",
    "section": "F.4 Linking Hypothesis Tests and Confidence Intervals",
    "text": "F.4 Linking Hypothesis Tests and Confidence Intervals\nCIs are directly linked to hypothesis tests. A \\(100(1-\\alpha)\\%\\) two-sided confidence interval for target parameter \\(\\theta\\) is linked with a hypothesis test of \\(H_0:\\theta = c\\) versus \\(H_a:\\theta \\neq c\\) tested at level \\(\\alpha\\). Any point that lies within the \\(100(1-\\alpha)\\%\\) confidence interval for \\(\\theta\\) represents a value of \\(c\\) for which the associated null hypothesis would not be rejected at significance level \\(\\theta\\). Any point outside of the confidence interval is a value of \\(c\\) for which the associated null hypothesis would be rejected. Similar relationships hold for one-sided CIs and hypothesis tests.\nA confidence interval provides us with much of the same information as a hypothesis test, but it doesn’t provide the p-value or allow us to do hypothesis tests at different significance levels. Confidence intervals are often preferred over hypothesis tests because they provide additional information in the form of plausible parameters values while giving us enough information to perform a hypothesis test.\nExample:\nConsider the 95% confidence interval for \\(\\mu\\) we previously constructed: [-0.070,1.171]. That interval is conceptually linked to the statistical test of \\(H_0:\\mu = c\\) versus \\(H_a:\\mu \\neq c\\) using \\(\\alpha =0.05\\). We reject \\(H_0\\) for any hypothesized values of \\(c\\) less than -0.070 or more than 1.171. We fail to reject \\(H_0\\) for any values of \\(c\\) between -0.070 and 1.171.\n\n\n\n\nBoos, Dennis D., and Leonard A. Stefanski. 2011. “P-Value Precision and Reproducibility.” The American Statistician 65 (4): 213–21. https://doi.org/10.1198/tas.2011.10129.\n\n\nGibson, Eric W. 2021. “The Role of p-Values in Judging the Strength of Evidence and Realistic Replication Expectations.” Statistics in Biopharmaceutical Research 13 (1): 6–18. https://doi.org/10.1080/19466315.2020.1724560.\n\n\nWackerly, Dennis D, William Mendhall, and Richard L Scheaffer. 2008. “Mathematical Statistics with Applications, 7th Edition.” Belmont, California: Cengage.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>F</span>  <span class='chapter-title'>Review of Estimation, Hypothesis Testing, and Confidence Intervals</span>"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Boos, Dennis D., and Leonard A. Stefanski. 2011. “P-Value\nPrecision and Reproducibility.” The American\nStatistician 65 (4): 213–21. https://doi.org/10.1198/tas.2011.10129.\n\n\nBrewer, Cynthia A. 2022. “ColorBrewer2.org.” https://colorbrewer2.org.\n\n\nFrench, Joshua P. 2023. Api2lm: Functions and Data Sets for the Book\n\"a Progressive Introduction to Linear Models\". https://CRAN.R-project.org/package=api2lm.\n\n\nGibson, Eric W. 2021. “The Role of p-Values in Judging the\nStrength of Evidence and Realistic Replication Expectations.”\nStatistics in Biopharmaceutical Research 13 (1): 6–18. https://doi.org/10.1080/19466315.2020.1724560.\n\n\nGohel, David, and Panagiotis Skintzos. 2024. Ggiraph: Make Ggplot2\nGraphics Interactive. https://davidgohel.github.io/ggiraph/.\n\n\nGorman, Kristen B., Tony D. Williams, and William R. Fraser. 2014.\n“Ecological Sexual Dimorphism and Environmental Variability Within\na Community of Antarctic Penguins (Genus Pygoscelis).” PLOS\nONE 9 (3): 1–14. https://doi.org/10.1371/journal.pone.0090081.\n\n\nHorst, Allison, Alison Hill, and Kristen Gorman. 2022.\nPalmerpenguins: Palmer Archipelago (Antarctica) Penguin Data.\nhttps://allisonhorst.github.io/palmerpenguins/.\n\n\nMüller, Kirill, and Hadley Wickham. 2023. Tibble: Simple Data\nFrames. https://tibble.tidyverse.org/.\n\n\nPearson, Karl, and Alice Lee. 1897. “Mathematical Contributions to\nthe Theory of Evolution. On Telegony in Man. &c.”\nProceedings of the Royal Society of London 60 (359-367):\n273–83. https://doi.org/10.1098/rspl.1896.0048.\n\n\n———. 1903. “On the Laws of Inheritance in Man: I. Inheritance of\nPhysical Characters: I. Inheritance of Physical Characters.”\nBiometrika 2 (4): 357–462. https://doi.org/10.1093/biomet/2.4.357.\n\n\nPlotly Technologies Inc. 2015. “Collaborative Data\nScience.” Montreal, QC: Plotly Technologies Inc. 2015. https://plot.ly.\n\n\nR Core Team. 2024. R: A Language and Environment for Statistical\nComputing. Vienna, Austria: R Foundation for Statistical Computing.\nhttps://www.R-project.org/.\n\n\nSievert, Carson, Chris Parmer, Toby Hocking, Scott Chamberlain, Karthik\nRam, Marianne Corvellec, and Pedro Despouy. 2024. Plotly: Create\nInteractive Web Graphics via Plotly.js. https://plotly-r.com.\n\n\nWackerly, Dennis D, William Mendhall, and Richard L Scheaffer. 2008.\n“Mathematical Statistics with Applications, 7th Edition.”\nBelmont, California: Cengage.\n\n\nWeisberg, Sanford. 2014. Applied Linear Regression. Fourth.\nHoboken NJ: Wiley. http://z.umn.edu/alr4ed.\n\n\nWickham, Hadley. 2019. Advanced R. CRC press. http://adv-r.had.co.nz/.\n\n\n———. 2022. The Tidyverse Style Guide. https://style.tidyverse.org/.\n\n\n———. 2023a. Forcats: Tools for Working with Categorical Variables\n(Factors). https://forcats.tidyverse.org/.\n\n\n———. 2023b. Stringr: Simple, Consistent Wrappers for Common String\nOperations. https://stringr.tidyverse.org.\n\n\n———. 2023c. Tidyverse: Easily Install and Load the Tidyverse.\nhttps://tidyverse.tidyverse.org.\n\n\nWickham, Hadley, and Jennifer Bryan. 2023. Readxl: Read Excel\nFiles. https://readxl.tidyverse.org.\n\n\nWickham, Hadley, Winston Chang, Lionel Henry, Thomas Lin Pedersen,\nKohske Takahashi, Claus Wilke, Kara Woo, Hiroaki Yutani, Dewey\nDunnington, and Teun van den Brand. 2024. Ggplot2: Create Elegant\nData Visualisations Using the Grammar of Graphics. https://ggplot2.tidyverse.org.\n\n\nWickham, Hadley, Romain François, Lionel Henry, Kirill Müller, and Davis\nVaughan. 2023. Dplyr: A Grammar of Data Manipulation. https://dplyr.tidyverse.org.\n\n\nWickham, Hadley, and Lionel Henry. 2023. Purrr: Functional\nProgramming Tools. https://purrr.tidyverse.org/.\n\n\nWickham, Hadley, Jim Hester, and Jennifer Bryan. 2024. Readr: Read\nRectangular Text Data. https://readr.tidyverse.org.\n\n\nWickham, Hadley, Davis Vaughan, and Maximilian Girlich. 2024. Tidyr:\nTidy Messy Data. https://tidyr.tidyverse.org.\n\n\nWilkinson, GN, and CE Rogers. 1973. “Symbolic Description of\nFactorial Models for Analysis of Variance.” Journal of the\nRoyal Statistical Society: Series C (Applied Statistics) 22 (3):\n392–99.\n\n\nXie, Yihui. 2024. Knitr: A General-Purpose Package for Dynamic\nReport Generation in r. https://yihui.org/knitr/.",
    "crumbs": [
      "Appendices",
      "References"
    ]
  }
]