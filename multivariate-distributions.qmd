# Multivariate distributions {#sec-multivariate-distributions}

---

<a rel="license" href="http://creativecommons.org/licenses/by-nc-sa/4.0/"><img alt="Creative Commons License" style="border-width:0" src="https://i.creativecommons.org/l/by-nc-sa/4.0/80x15.png" /></a> This work is licensed under a <a rel="license" href="http://creativecommons.org/licenses/by-nc-sa/4.0/">Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License</a>. 

---

A condensed, interactive version of this content is available as a Colab notebook, which can be accessed by clicking or scanning the QR code below. 

<a href="https://colab.research.google.com/github/jfrench/LinearRegression/blob/master/notebooks/d-multivariate-distributions-notebook.ipynb">
<img src="https://raw.githubusercontent.com/jfrench/LinearRegression/281ae22b4ccc75524058acdc2d41517ac29aaf95/images/qr-multivariate-distributions.svg" width="100">
</a>

---

## Basic properties

Let $Y_1,Y_2,\ldots,Y_n$ denote $n$ random variables with supports $\mathcal{S}_1,\mathcal{S}_2,\ldots,\mathcal{S}_n$, respectively.

If the random variables are **jointly discrete** (i.e., all discrete), then the joint pmf $f(y_1,\ldots,y_n)=P(Y_1=y_1,\ldots,Y_n=y_n)$  satisfies the following properties:

1. $0\leq f(y_1,\ldots,y_n )\leq 1$,
2. $\sum_{y_1\in\mathcal{S}_1}\cdots \sum_{y_n\in\mathcal{S}_n} f(y_1,\ldots,y_n ) = 1$,
3. $P((Y_1,\ldots,Y_n)\in A)=\sum_{(y_1,\ldots,y_n) \in A} f(y_1,\ldots,y_n)$.

In this context,
$$
E(Y_1 \cdots Y_n)=\sum_{y_1\in\mathcal{S}_1} \cdots \sum_{y_n\in\mathcal{S}_n}y_1 \cdots y_n  f(y_1,\ldots,y_n).
$$

In general,
$$
E(g(Y_1,\ldots,Y_n))=\sum_{y_1\in\mathcal{S}_1} \cdots \sum_{y_n\in\mathcal{S}_n} g(y_1, \ldots, y_n) f(y_1,\ldots,y_n),
$$
where $g$ is a function of the random variables.

If the random variables are **jointly continuous**, then $f(y_1,\ldots,y_n)$  is the joint pdf if it satisfies the following properties:

1. $f(y_1,\ldots,y_n ) \geq 0$,
2. $\int_{y_1\in\mathcal{S}_1}\cdots \int_{y_n\in\mathcal{S}_n} f(y_1,\ldots,y_n ) dy_n \cdots dy_1 = 1$,
3. $P((Y_1,\ldots,Y_n)\in A)=\int \cdots \int_{(y_1,\ldots,y_n) \in A} f(y_1,\ldots,y_n) dy_n\ldots dy_1$.

In this context,
$$
E(Y_1 \cdots Y_n)=\int_{y_1\in\mathcal{S}_1} \cdots \int_{y_n\in\mathcal{S}_n} y_1 \cdots y_n  f(y_1,\ldots,y_n) dy_n \ldots dy_1.
$$

In general,
$$
E(g(Y_1,\ldots,Y_n))=\int_{y_1\in\mathcal{S}_1} \cdots \int_{y_n\in\mathcal{S}_n} g(y_1, \ldots, y_n) f(y_1,\ldots,y_n) dy_n \cdots dy_1,
$$
where $g$ is a function of the random variables.

## Marginal distributions

If the random variables are jointly discrete, then the marginal pmf of $Y_1$ is obtained by summing over the other variables $Y_2, ..., Y_n$:
$$
f_{Y_1}(y_1)=\sum_{y_2\in\mathcal{S}_2}\cdots \sum_{y_n\in\mathcal{S}_n} f(y_1,\ldots,y_n).
$$

Similarly, if the  random variables are jointly continuous, then the marginal pdf of $Y_1$ is obtained by integrating over the other variables $Y_2, ..., Y_n$
$$
f_{Y_1}(y_1)=\int_{y_2\in\mathcal{S}_2}\cdots \int_{y_n\in\mathcal{S}_n} f(y_1,\ldots,y_n) dy_n \cdots dy_2.
$$

## Independence of random variables
Random variables $X$ and $Y$ are independent if
$$
F(x, y) = F_X(x) F_Y(y).
$$

Alternatively, $X$ and $Y$ are independent if
$$
f(x, y) = f_X(x)f_Y(y).
$$

## Conditional distributions
Let $X$ and $Y$ be random variables. Then assuming $f_Y(y)>0$, the conditional distribution of $X$ given $Y = y$, denoted $X|Y=y$ comes from Bayes' formula:
$$
f(x\mid y) = \frac{f(x, y)}{f_{Y}(y)}, \quad f_Y(y)>0.
$$

## Covariance 

The covariance between random variables $X$ and $Y$ is 
$$
\mathrm{cov}(X,Y)=E[(X-E(X))(Y-E(Y))]=E(XY)-E(X)E(Y).
$$

## Useful facts for transformations of multiple random variables {#sec-trans-mult-rand-vars}

Let $a$ and $b$ be scalar constants. Let $Y$ and $Z$ be random variables.  Then:

- $E(aY+bZ)=aE(Y)+bE(Z)$.
- $\mathrm{var}(Y+Z)=\mathrm{var}(Y)+\mathrm{var}(Z)+2\mathrm{cov}(Y, Z)$.
- $\mathrm{cov}(a,Y)=0$.
- $\mathrm{cov}(Y,Y)=\mathrm{var}(Y)$.
- $\mathrm{cov}(aY, bZ)=ab\mathrm{cov}(Y, Z)$.
- $\mathrm{cov}(a + Y,b + Z)=\mathrm{cov}(Y, Z)$.

If $Y$ and $Z$ are also independent, then:

- $E(YZ)=E(Y)E(Z)$.
- $\mathrm{cov}(Y, Z)=0$.

In general, if $Y_1, Y_2, \ldots, Y_n$ are a set of random variables, then:

- $E(\sum_{i=1}^n Y_i) = \sum_{i=1}^n E(Y_i)$, i.e., the expectation of the sum of random variables is the sum of the expectation of the random variables.
- $\mathrm{var}(\sum_{i=1}^n Y_i) = \sum_{i=1}^n \mathrm{var}(Y_i) + \sum_{j=1}^n\sum_{1\leq i<j\leq n}2\mathrm{cov}(Y_i, Y_j)$, i.e., the variance of the sum of random variables is the sum fo the variables' variances plus the sum of twice all possible pairwise covariances.

If in addition, $Y_1, Y_2, \ldots, Y_n$ are all independent of each other, then:

- $\mathrm{var}(\sum_{i=1}^n Y_i) = \sum_{i=1}^n \mathrm{var}(Y_i)$ since all pairwise covariances are 0.


## Example (Binomial)

A random variable $Y$ is said to have a Binomial distribution with $n$ trials and probability of success $\theta$, denoted $Y\sim \mathsf{Bin}(n,\theta)$ when $\mathcal{S}=\{0,1,2,\ldots,n\}$ and the pmf is
$$
f(y\mid\theta) = \binom{n}{y} \theta^y (1-\theta)^{(n-y)}.
$$

An alternative explanation of a Binomial random variable is that it is the sum of $n$ independent and identically-distributed Bernoulli random variables. Alternatively, let $Y_1,Y_2,\ldots,Y_n\stackrel{i.i.d.}{\sim} \mathsf{Bernoulli}(\theta)$, where i.i.d. stands for independent and identically distributed, i.e., $Y_1, Y_2, \ldots, Y_n$ are independent random variables with identical distributions. Then $Y=\sum_{i=1}^n Y_i \sim \mathsf{Bin}(n,\theta)$.

A Binomial random variable with $\theta = 0.5$ models the question: what is the probability of flipping $y$ heads in $n$ flips?

Using this information and the facts above, we can easily determine the mean and variance of $Y$.

Using our results from @sec-bernoulli-distribution-example, we can see that $E(Y_i) = \theta$ for $i=1,2,\ldots,n$. Similarly, $\mathrm{var}(Y_i)=\theta(1-\theta)$ for $i=1,2,\ldots,n$. 

We determine that:
$$
E(Y)=E\biggl(\sum_{i=1}^n Y_i\biggr)=\sum_{i=1}^n E(Y_i) = \sum_{i=1}^n \theta = n\theta.
$$

Similarly, since $Y_1, Y_2, \ldots, Y_n$ are i.i.d. and using the facts in @sec-trans-mult-rand-vars, we see that
$$
\mathrm{var}(Y) = \mathrm{var}(\sum_{i=1}^n Y_i) = \sum_{i=1}^n\mathrm{var}(Y_i)=\sum_{i=1}^n \theta(1-\theta) = n\theta(1-\theta).
$$

## Example (Continuous bivariate distribution) {#sec-continuous-bivariate-distribution-example}

Hydration is important for health. Like many people, the author has a water bottle he uses to say hydrated through the day and drinks several liters of water per day. Let's say the author refills his water bottle every 3 hours. Let $Y$ denote the proportion of the water bottle filled with water at the beginning of the 3-hour window. Let $X$ denote the amount of water the author consumes in the 3-hour window (measured in the the proportion of total water bottle capacity).  We know that $0\leq X \leq Y \leq 1$. The joint density of the random variables is
$$
f(x,y)=4y^2,\quad 0 \leq x\leq y\leq 1,
$$
and 0 otherwise.

We answer a series of questions about this distribution.

**Q1: Determine $P(0.5\leq X\leq 1, 0.75\leq Y)$**.

Note that the comma between the two events means "and".

Since $X$ must be no more than $Y$, we can answer this question as 
$$
\int_{3/4}^{1} \int_{1/2}^{y} 4y^2\;dx\;dy=229/768\approx 0.30.
$$

**Q2: Determine the marginal distributions of $X$ and $Y$.**

To find the marginal distribution of $X$, we must integrate the joint pdf with respect to the limits of $Y$. Don't forget to include the support of the pdf of $X$ (which after integrating out $Y$, must be between 0 and 1).
$$
\begin{aligned}
f_X(x) &=\int_{x}^1 4y^2\;dy \\
&=\frac{4}{3}(1-x^3),\quad 0\leq x \leq 1.
\end{aligned}
$$

Similarly,
$$
\begin{aligned}
f_Y(y) &=\int_{0}^y 4y^2\;dx \\
&=4y^3,\quad 0\leq y \leq 1.
\end{aligned}
$$

**Q3: Determine the means of $X$ and $Y$.**

The mean of $X$ is the integral of $x f_X(x)$ over the support of $X$, i.e.,
$$
E(X) =\int_{0}^1 x\biggl(\frac{4}{3}(1-x^3)\biggr)\;dx = \frac{2}{5}
$$

Similarly,
$$
E(Y) =\int_{0}^1 y(4y^3)\;dy = \frac{4}{5}
$$

**Q4: Determine the variances of $X$ and $Y$.**

We use the formula $\mathrm{var}(X)=E(X^2)-[E(X)^2]$ to compute the variances. First, 
$$
E(X^2) =\int_{0}^1 x^2\biggl(\frac{4}{3}(1-x^3)\biggr)\;dx = \frac{2}{9}.
$$
Second, 
$$
E(Y^2) =\int_{0}^1 y^2(4y^3)\;dy = \frac{2}{3}.
$$
Thus, 
$$
\mathrm{var}(X)=\frac{2}{9}-\biggl(\frac{2}{5}\biggr)^2=\frac{14}{225}.
$$
$$
\mathrm{var}(Y)=\frac{2}{3}-\biggl(\frac{4}{5}\biggr)^2=\frac{2}{75}.
$$


**Q5: Determine the mean of $XY$.**

The mean of $XY$ requires us to integrate the product of $xy$ and the joint pdf over the joint support of $X$ and $Y$. Specifically,
$$
E(XY)=\int_{0}^{1}\int_{0}^{y} xy(4y^2)\;dx\;dy= \frac{1}{3}.
$$

**Q6: Determine the covariance of $X$ and $Y$.**

Using our previous work, we see that
$$
\mathrm{cov}(X,Y)=E(XY) - E(X)E(Y)=\frac{1}{3}-\frac{2}{5}\cdot\frac{4}{5}=\frac{1}{75}.
$$

**Q7: Determine the mean and variance of $Y-X$, i.e., the average amount of water remaining after a 3-hour window and the variability of that amount.**

Using the results in @sec-trans-mult-rand-vars, we have that
$$
E(Y-X)=E(Y)-E(X)=4/5-2/5=2/5,
$$
and
$$
\mathrm{var}(Y-X)=\mathrm{var}(Y)+\mathrm{var}(X)-2\mathrm{cov}(Y,X)=
\frac{2}{75}+\frac{14}{225}-2\cdot\frac{1}{75}=\frac{14}{225}.
$$